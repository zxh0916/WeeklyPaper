{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7face075-243f-4600-bce4-b0ece7be0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "import time\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf44275-d8cb-4350-9717-282e66136045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 百度来的，不然下载不动。。\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67e9f94-f455-4f0f-985f-2e8d40ef763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True)\n",
    "print(cifar_train.data.shape) # (50000, 32, 32, 3)\n",
    "cifardata = cifar_train.data / 255\n",
    "mean_pic = torch.tensor(cifardata.mean(axis=(0))).permute(2, 0, 1)\n",
    "print(mean_pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8c5c05-efd1-46ca-a124-db64ca114d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_and_valid = data.random_split(torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True),\n",
    "                                    [45000, 5000],\n",
    "                                    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffe4583-5969-4e5c-bcd8-f16fef6fb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Lambda(lambda pic: pic-mean_pic.to(pic.device)),\n",
    "                                         transforms.RandomCrop(32, padding=4),\n",
    "                                         transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                         transforms.ConvertImageDtype(torch.float)])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.trans(self.dataset[index][0]),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4265e1b-6561-455f-b474-c92d66471063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Lambda(lambda pic: pic-mean_pic.to(pic.device)),\n",
    "                                         transforms.ConvertImageDtype(torch.float)])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.trans(self.dataset[index][0]),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39faf6b-2244-43c9-8e7b-6ecfe7104d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainDataset(train_and_valid[0])\n",
    "valid_dataset = TestDataset(train_and_valid[1])\n",
    "test_dataset = TestDataset(torchvision.datasets.CIFAR10(root=\"../data\", train=False, download=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8829ef2b-0eb9-4639-b6d9-db668cc68fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_acc(net, data_iter, criterion, device=device):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度。\"\"\"\n",
    "    net.eval()  # 设置为评估模式\n",
    "    loss = []\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for input, target in data_iter:\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = net(input)\n",
    "            loss.append(float(criterion(output, target).item()))\n",
    "            metric.add(d2l.accuracy(output, target), target.numel())\n",
    "    return sum(loss) / len(loss), metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8cb7be2-7775-44d7-98b9-18e69cb058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return (optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf54e78-7ba2-4d1d-bd65-c215ee177996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ResNet(net,\n",
    "                 batch_size,\n",
    "                 lr,\n",
    "                 num_epochs,\n",
    "                 weight_decay=1e-4):\n",
    "\n",
    "    train_iter = data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=0)\n",
    "    valid_iter = data.DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                                 shuffle=False, num_workers=0)\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "    net.apply(init_weights)\n",
    "    optimizer = torch.optim.SGD(net.parameters(),\n",
    "                                lr=lr,\n",
    "                                weight_decay=weight_decay,\n",
    "                                momentum=0.9)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, verbose=True)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, threshold=0.0001, verbose=True)\n",
    "    scheduler_name = str(scheduler.__class__).split('.')[-1][:-2]\n",
    "    writer = SummaryWriter(f'runs/ResNet_CIFAR_n={net.n}_{net.option}_bn={net.batch_norm}_{scheduler_name}_weight_decay={weight_decay}')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        tic = time.time()\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (input, target) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(loss * input.shape[0],\n",
    "                           d2l.accuracy(output, target),\n",
    "                           input.shape[0])\n",
    "            timer.stop()\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        valid_loss, valid_acc = evaluate_loss_acc(net, valid_iter, criterion, device)\n",
    "        writer.add_scalar('train/loss', train_loss, global_step=epoch+1)\n",
    "        writer.add_scalar('train/accuracy', train_acc, global_step=epoch+1)\n",
    "        writer.add_scalar('valid/loss', valid_loss, global_step=epoch+1)\n",
    "        writer.add_scalar('valid/accuracy', valid_acc, global_step=epoch+1)\n",
    "        writer.add_scalar('learning rate', get_lr(optimizer), global_step=epoch+1)\n",
    "        # scheduler.step()\n",
    "        scheduler.step(valid_loss)\n",
    "        toc = time.time()\n",
    "        print(f\"epoch {epoch+1:3d}, train loss: {train_loss:.4f}, train accuracy: {train_acc:.4f}, \\\n",
    "valid loss: {valid_loss:.4f}, valid accuracy: {valid_acc:.4f}, time: {toc-tic:.4f}\")\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'valid loss {valid_loss:.3f}, valid acc {valid_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9238c149-a8a1-4eb6-b17d-8a66b55f0a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,727,962 total parameters.\n",
      "1,727,962 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "net = ResNet_CIFAR(n=18, option='A', batch_norm=True, dropout=0.5).to(device)\n",
    "net.print_num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb8cf2-e42c-4854-a5f9-0f55ea30d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1, train loss: 2.8001, train accuracy: 0.1223, valid loss: 2.2500, valid accuracy: 0.1494, time: 61.6174\n",
      "epoch   2, train loss: 2.0884, train accuracy: 0.1987, valid loss: 1.8948, valid accuracy: 0.2668, time: 61.0321\n",
      "epoch   3, train loss: 1.8530, train accuracy: 0.2825, valid loss: 1.7821, valid accuracy: 0.3180, time: 60.9301\n",
      "epoch   4, train loss: 1.7036, train accuracy: 0.3550, valid loss: 1.5742, valid accuracy: 0.4108, time: 61.1335\n",
      "epoch   5, train loss: 1.5688, train accuracy: 0.4148, valid loss: 1.4644, valid accuracy: 0.4670, time: 61.1319\n",
      "epoch   6, train loss: 1.4149, train accuracy: 0.4835, valid loss: 1.3594, valid accuracy: 0.4880, time: 61.2271\n",
      "epoch   7, train loss: 1.2613, train accuracy: 0.5469, valid loss: 1.5640, valid accuracy: 0.4814, time: 61.2310\n",
      "epoch   8, train loss: 1.1194, train accuracy: 0.6059, valid loss: 1.6668, valid accuracy: 0.5138, time: 61.1517\n",
      "epoch   9, train loss: 1.0075, train accuracy: 0.6513, valid loss: 1.3836, valid accuracy: 0.5728, time: 61.2671\n",
      "epoch  10, train loss: 0.9318, train accuracy: 0.6836, valid loss: 1.2233, valid accuracy: 0.6074, time: 61.1991\n",
      "epoch  11, train loss: 0.8645, train accuracy: 0.7094, valid loss: 0.8966, valid accuracy: 0.6926, time: 61.2501\n",
      "epoch  12, train loss: 0.8119, train accuracy: 0.7301, valid loss: 1.1062, valid accuracy: 0.6392, time: 61.3077\n",
      "epoch  13, train loss: 0.7781, train accuracy: 0.7435, valid loss: 1.0430, valid accuracy: 0.6584, time: 61.2673\n",
      "epoch  14, train loss: 0.7419, train accuracy: 0.7583, valid loss: 1.1015, valid accuracy: 0.6598, time: 61.2110\n",
      "epoch  15, train loss: 0.7097, train accuracy: 0.7684, valid loss: 0.8162, valid accuracy: 0.7368, time: 61.2114\n",
      "epoch  16, train loss: 0.6864, train accuracy: 0.7776, valid loss: 0.7876, valid accuracy: 0.7448, time: 61.2779\n",
      "epoch  17, train loss: 0.6702, train accuracy: 0.7834, valid loss: 0.6920, valid accuracy: 0.7686, time: 61.3981\n",
      "epoch  18, train loss: 0.6500, train accuracy: 0.7883, valid loss: 0.9852, valid accuracy: 0.6944, time: 61.2747\n",
      "epoch  19, train loss: 0.6359, train accuracy: 0.7944, valid loss: 0.8716, valid accuracy: 0.7146, time: 61.3104\n",
      "epoch  20, train loss: 0.6101, train accuracy: 0.8018, valid loss: 0.6362, valid accuracy: 0.7842, time: 61.4623\n",
      "epoch  21, train loss: 0.6126, train accuracy: 0.8026, valid loss: 0.8718, valid accuracy: 0.7210, time: 61.2319\n",
      "epoch  22, train loss: 0.5758, train accuracy: 0.8143, valid loss: 0.8229, valid accuracy: 0.7404, time: 61.2289\n",
      "epoch  23, train loss: 0.5703, train accuracy: 0.8176, valid loss: 1.0549, valid accuracy: 0.6592, time: 61.2846\n",
      "epoch  24, train loss: 0.5571, train accuracy: 0.8193, valid loss: 0.8095, valid accuracy: 0.7440, time: 61.2589\n",
      "epoch  25, train loss: 0.5470, train accuracy: 0.8228, valid loss: 0.7313, valid accuracy: 0.7594, time: 61.3108\n",
      "epoch  26, train loss: 0.5328, train accuracy: 0.8270, valid loss: 0.7522, valid accuracy: 0.7542, time: 61.1712\n",
      "epoch  27, train loss: 0.5319, train accuracy: 0.8296, valid loss: 1.0941, valid accuracy: 0.6872, time: 61.1823\n",
      "epoch  28, train loss: 0.5193, train accuracy: 0.8347, valid loss: 0.6096, valid accuracy: 0.8016, time: 61.2597\n",
      "epoch  29, train loss: 0.5155, train accuracy: 0.8336, valid loss: 0.6343, valid accuracy: 0.7906, time: 61.3463\n",
      "epoch  30, train loss: 0.5156, train accuracy: 0.8350, valid loss: 0.7681, valid accuracy: 0.7666, time: 61.3532\n",
      "epoch  31, train loss: 0.4928, train accuracy: 0.8434, valid loss: 0.5665, valid accuracy: 0.8126, time: 61.3367\n",
      "epoch  32, train loss: 0.4824, train accuracy: 0.8463, valid loss: 0.5948, valid accuracy: 0.8032, time: 61.2863\n",
      "epoch  33, train loss: 0.4886, train accuracy: 0.8441, valid loss: 0.7990, valid accuracy: 0.7396, time: 61.3096\n",
      "epoch  34, train loss: 0.4860, train accuracy: 0.8441, valid loss: 0.6476, valid accuracy: 0.8044, time: 61.1330\n",
      "epoch  35, train loss: 0.4714, train accuracy: 0.8488, valid loss: 0.5441, valid accuracy: 0.8196, time: 61.2288\n",
      "epoch  36, train loss: 0.4659, train accuracy: 0.8531, valid loss: 0.6517, valid accuracy: 0.7988, time: 61.4034\n",
      "epoch  37, train loss: 0.4561, train accuracy: 0.8557, valid loss: 0.8564, valid accuracy: 0.7566, time: 61.2478\n",
      "epoch  38, train loss: 0.4597, train accuracy: 0.8548, valid loss: 0.5226, valid accuracy: 0.8334, time: 61.4071\n",
      "epoch  39, train loss: 0.4464, train accuracy: 0.8556, valid loss: 0.7723, valid accuracy: 0.7670, time: 61.3050\n",
      "epoch  40, train loss: 0.4449, train accuracy: 0.8580, valid loss: 0.5739, valid accuracy: 0.8126, time: 61.2116\n",
      "epoch  41, train loss: 0.4390, train accuracy: 0.8592, valid loss: 0.4982, valid accuracy: 0.8382, time: 61.3649\n",
      "epoch  42, train loss: 0.4483, train accuracy: 0.8571, valid loss: 0.6851, valid accuracy: 0.7900, time: 61.2025\n",
      "epoch  43, train loss: 0.4325, train accuracy: 0.8635, valid loss: 0.7271, valid accuracy: 0.7788, time: 61.2780\n",
      "epoch  44, train loss: 0.4280, train accuracy: 0.8636, valid loss: 0.6382, valid accuracy: 0.7920, time: 61.3548\n",
      "epoch  45, train loss: 0.4261, train accuracy: 0.8661, valid loss: 0.5586, valid accuracy: 0.8166, time: 61.2377\n",
      "epoch  46, train loss: 0.4245, train accuracy: 0.8663, valid loss: 0.5078, valid accuracy: 0.8328, time: 61.3589\n",
      "epoch  47, train loss: 0.4206, train accuracy: 0.8666, valid loss: 0.5700, valid accuracy: 0.8224, time: 61.3698\n",
      "epoch  48, train loss: 0.4213, train accuracy: 0.8665, valid loss: 0.6629, valid accuracy: 0.7830, time: 61.2264\n",
      "epoch  49, train loss: 0.4215, train accuracy: 0.8666, valid loss: 0.6474, valid accuracy: 0.7924, time: 61.3073\n",
      "epoch  50, train loss: 0.4114, train accuracy: 0.8707, valid loss: 0.8808, valid accuracy: 0.7578, time: 61.2958\n",
      "epoch  51, train loss: 0.4127, train accuracy: 0.8692, valid loss: 0.4995, valid accuracy: 0.8400, time: 61.3361\n",
      "epoch  52, train loss: 0.4036, train accuracy: 0.8718, valid loss: 0.7202, valid accuracy: 0.7726, time: 61.3054\n",
      "epoch  53, train loss: 0.4005, train accuracy: 0.8732, valid loss: 0.4744, valid accuracy: 0.8484, time: 61.3695\n",
      "epoch  54, train loss: 0.4009, train accuracy: 0.8730, valid loss: 0.4932, valid accuracy: 0.8358, time: 61.3609\n",
      "epoch  55, train loss: 0.4033, train accuracy: 0.8720, valid loss: 0.5985, valid accuracy: 0.8096, time: 61.2850\n",
      "epoch  56, train loss: 0.4058, train accuracy: 0.8715, valid loss: 0.9896, valid accuracy: 0.7396, time: 61.2590\n",
      "epoch  57, train loss: 0.4015, train accuracy: 0.8725, valid loss: 0.6519, valid accuracy: 0.8098, time: 61.3830\n",
      "epoch  58, train loss: 0.4041, train accuracy: 0.8741, valid loss: 0.5023, valid accuracy: 0.8382, time: 61.2432\n",
      "epoch  59, train loss: 0.3988, train accuracy: 0.8739, valid loss: 0.5749, valid accuracy: 0.8192, time: 61.2421\n",
      "epoch  60, train loss: 0.3931, train accuracy: 0.8762, valid loss: 0.5577, valid accuracy: 0.8310, time: 61.2744\n",
      "epoch  61, train loss: 0.3901, train accuracy: 0.8768, valid loss: 0.8368, valid accuracy: 0.7516, time: 61.2601\n",
      "epoch  62, train loss: 0.3853, train accuracy: 0.8772, valid loss: 0.6221, valid accuracy: 0.8108, time: 61.2956\n",
      "epoch  63, train loss: 0.3900, train accuracy: 0.8768, valid loss: 0.4891, valid accuracy: 0.8376, time: 61.2749\n",
      "epoch  64, train loss: 0.3913, train accuracy: 0.8766, valid loss: 0.5546, valid accuracy: 0.8274, time: 61.2250\n",
      "epoch  65, train loss: 0.3895, train accuracy: 0.8777, valid loss: 0.6728, valid accuracy: 0.7844, time: 61.3606\n",
      "epoch  66, train loss: 0.3838, train accuracy: 0.8785, valid loss: 0.9311, valid accuracy: 0.7660, time: 61.2353\n"
     ]
    }
   ],
   "source": [
    "train_ResNet(net,\n",
    "             batch_size=256,\n",
    "             lr=0.1,\n",
    "             num_epochs=300,\n",
    "             weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f82e3d-29c6-425c-84a5-4eecac489be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "test_loss, test_acc = evaluate_loss_acc(net, test_iter, nn.CrossEntropyLoss())\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48121175-10ff-4da8-8b77-747bfcac572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), f'ResNet_CIFAR_n={net.n}_{net.option}_acc={test_acc}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
