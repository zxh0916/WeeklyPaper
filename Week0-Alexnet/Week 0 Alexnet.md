# Week 0 AlexNet论文笔记+复现

论文地址:

-   本地（做了标记）[ImageNet Classification with Deep Convolutional Neural Networks](../papers/Alexnet.pdf)
-   原地址[ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

下期预告：[Week 1 VGG](../Week 1 VGG/Week 1 VGG.md)



Alex Krizhevsky & Ilya Sutskever & **Geoffrey E. Hinton**

University of Toronto

## Abstract

-   在ILSVRC-2010（1.2M张图片，1000类）上达到了SOTA
-   60M参数，65000neurons
-   五个卷积层、若干个池化层、三个全连接层、1000类softmax
-   用了非饱和的神经元和dropout方法
-   参加了ILSVRC-2012 competition中达到了测试集top-5错误率15.3%



## 1 Introduction

**提升图片分类机器学习模型的性能**：

-   收集更大的数据集

-   学习更强大的模型

-   用更好的技巧来预防过拟合



​		作者认为图片识别任务是极为复杂的任务，如此复杂的问题即使使用ImageNet这种体量的数据集也无法完全被模型掌握，所以模型需要一些先验知识，这样的先验知识通过CNN自身的架构来体现。

​		CNN对图片信息做很强的假设：每个像素应当与周围临近的像素有较大的关联，而与距离较远的像素关联性较小。因为卷积核只有有限大小，一次只能看图片中的一小部分，每个特征图中一个值对应到原图中的区域也相对集中。

​		GPU搭配上优化的2d卷积算法，能够训练大尺寸的CNN，ImageNet的数据量也保证了模型不会遭受很严重的过拟合。

​		接下来几节分别讲述了作者使用的新方法来提升性能并加速训练（第三节）、预防过拟合的手段（第四节）。

​		AlexNet包含5个卷积层和3个全连接层，移除任意一个卷积层，都会造成较大的性能损失，即使这个卷积层的参数数量仅占全部参数数量的1%左右。

​		网络大小受到了显卡显存大小的限制，所以作者将整个网路部署在2块GTX580 3GB上，训练了6天。所有的实验都表明，网络性能可以简单地通过使用更强大的GPU和更大的数据集来提升。



## 2 The Dataset

​		简单介绍了ImageNet和ILSVRC——ImageNet的子集，1000个类别，每个类别约1000张图片，共1.2M张训练图片，50000张验证图片，150000张测试图片。ILSVRC-2010是唯一一个测试集公开的版本，大部分的实验在其上进行。

​		ImageNet包含多种分辨率的图片，但网络要求固定输入大小，所以所有样本被通过如下方式缩放至256\*256：将原图短边缩放至256，再在缩放后的图片中心切出256\*256的一块。除了裁剪和将训练图片像素值中心化（均值为0）之外，对图片不做其他处理。



## 3 The Architecture

![LeNet和AlexNet架构对比](..\pictures\alexnet.svg)

​		AlexNet架构如上图（右）。网络有8个可学习的层：5个卷积层，3个全连接层。对比LeNet还是多了不少东西的，尤其是后面那个4096\*4096的全连接层，怕是一层的参数就比LeNet整个网络都多了。（借用沐神一张图~从d2l.ai上扣的）原文那个图上半部分被截掉了，不知道是不是我这显示的问题……反正现在也没人用两张显卡跑AlexNet了，所以干脆就合到一张卡上，用没那么花哨的图片示意一下。



### 3.1 ReLU Nonlinearity

<img src="..\pictures\activation function.png" alt="Activation Function" style="zoom:33%;" />

​		在此之前，激活函数一般是sigmoid函数：
$$
f(x)=\frac{1}{1+e^{-x}}
$$
或双曲正切函数：
$$
f(x)=\tanh{(x)}
$$
它们都具有一个性质：饱和性，即输入较大或较小时导数值趋近于0。这使得在训练时，梯度值较小，导致训练较慢。使用非饱和激活函数：
$$
f(x)=\max{(0,x)}
$$
能够加速网络的训练。（实线ReLU，虚线tanh，达到同样的25%训练集错误率，ReLU快了数倍）

<img src="..\pictures\tanh v.s. ReLU.png" alt="tanh v.s. ReLU" style="zoom:67%;" />

​		在此之前有人用$f(x)=\abs{\tanh{(x)}}$作为激活函数，但那是在较小的数据集上做的实验，主要的目的是避免过拟合，与此处加速收敛的目的不同。

### 3.2 Training on Multiple GPUs

​		AlexNet之大，一块580装不下。但用了NVLink之后，两块580能够绕过内存直接互相读写显存，所以作者将网络砍成两半，每个580放一半神经元。但不是每一层两个gpu之间都交换信息，仅在特定层（第一个3\*3卷积层和所有全连接层）才读取另一块显卡的信息。



### 3.3 Local Response Normalization（局部响应归一化）

​		先吹了一波ReLU的好处：只要有一个预激活值大于0，这个神经元就能学习。但是饱和神经元如果不做归一化，导数值过小，会导致权重更新幅度太小。

​		局部响应归一化公式：
$$
b_{x,y}^{i}=a_{x,y}^{i}/(k+\alpha\sum^{\min{(N-1,i+n/2)}}_{j=\max{(0,i-n/2)}}(a_{x,y}^{j})^2)^\beta
$$


其中$a_{x,y}^{i}$是经过卷积和ReLU的激活值，位于特征图$(x,y)$位置，由第$i$个卷积核产生。局部响应归一化LRN相当于对特征图中的每一个通道的每一个像素，沿着通道方向，考虑自身前后$n/2$个通道的激活值，依照上式做归一化。

​		$k=2,n=5,\alpha=10^{-4},\beta=0.75$都是在验证集上得到的超参数。注意：LRN作用在激活值上，也就是应用ReLU之后才应用LRN！！~~但是后来证明这东西好像没什么屌用~~

​		应用之后top-1和top-5错误率分别降低了1.4%和1.2%。



### 3.4 Overlapping Pooling

​		池化层一个通道内对临近的激活值起到汇聚的作用。通常情况下，池化窗口彼此是不重叠的。例如，使用2\*2的池化窗口时，通常设置stride=2。但作者发现通过设置池化层为3\*3，stride=2而非2\*2，stride=2，可以将top-1和top-5错误率分别降低0.4%和0.3%。经过作者观察，使用重叠的池化窗口能使得网络略微更难过拟合。



### 3.5 Overall Architecture

​		网络的优化目标是最大化多类别Logistic回归，等价于最大化平均log概率。网络在第2、第4和5个卷积层内仅和本GPU内的特征图有连接，而第3个卷积层和两个GPU的特征图均有连接。LRN层作用在前两个卷积层之后。池化层作用在两个LRN层和第5个卷积层之后。ReLU作用在所有卷积层和全连接层之后。



## 4 Reducing Overfitting

​		AlexNet有6000万个参数。将一张图片分成ILSVRC的1000个类别相当于将图片编码成10bits的信息。看起来很少，但事实证明预防过拟合的正则化手段还是有必要的。



### 4.1 Data Augmentation

​		作者使用了两种数据增强方法，这两种方法计算量都较小，增强后的图片由python在CPU上生成，无需存储在硬盘上，直接交给GPU进行计算，这使得数据增强几乎不需要额外的计算量。

​		**第一种**数据增强方法是在256\*256的图片里随机扣出224\*224的图片，然后水平翻转一下（垂直翻转可能改变物体正常的表现形式，例如马:horse:），保留翻转了的和没翻转的图片。这能使得训练集的大小变为原来的2048倍。

​		在测试时，作者将原来的256\*256图片在四个角和中间抠出5张224\*224的图片，然后水平翻转，得到共10张图片，送进网络做预测然后对10张图片输出的结果取平均。

​		**第二种**数据增强方式是对训练集中RGB通道的数值做扰动。作者对ImageNet训练集中的图片的RGB值做主成分分析（PCA），找出三个主成分后对训练集中的每个像素的RGB值$\begin{bmatrix}I_{xy}^R&I_{xy}^G&I_{xy}^B\end{bmatrix}^\mathrm{T}$加上一个量：
$$
\begin{bmatrix}\mathbf{p_1}&\mathbf{p_2}&\mathbf{p_3} \end{bmatrix}\begin{bmatrix}\alpha_1\lambda_1&\alpha_2\lambda_2&\alpha_3\lambda_3\end{bmatrix}^\mathrm{T}
$$
其中，$\mathbf{p_1}$是PCA中3\*3协方差矩阵$\mathrm{X}\mathrm{X}^\mathrm{T}$（看不懂的看一眼西瓜书）的第一个特征向量，$\lambda_1$为对应的特征值，$\alpha_1,\alpha_2,\alpha_3 \sim \mathcal{N}~(0,0.1^2)$为三个随机噪音变量。对某个batch的一张图片，$\alpha$只产生一次，作用于所有像素。对于其他图片，或如果下个batch还遇到了这张图片，则重新生成$\alpha$。

​		这个操作可以让网络抓住图片中的主体，不去关心具体某个或某一片像素的色彩或光照。使用后top-1错误率降低了1%。



### 4.2 Dropout

​		集成学习中的Bagging方法将多个学习期的输出做平均或者投票。但如果一个学习器的训练就要花上好几天，时间成本就太高了。

​		Dropout方法在训练时以0.5的概率随机关闭（置0）一个神经元。每次训练的时候，都可以看做是在训练神经网络的一个“子网络”，最后在测试的时候关掉Dropout，相当于对所有子网络的结果做了平均。原文：“So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights.” 

​		另一种直观的解释认为Dropout能迫使网络不要依赖于某几个神经元的输出，因为他们随时有可能死掉，而是去优化所有神经元，由此避免过拟合。原文：“a neuron cannot rely on the presence of particular other neurons.”

​		AlexNet中，Dropout被应用于前两个全连接层。没有Dropout，网络就会过拟合。但Dropout使得网络收敛所需的迭代次数翻了一倍。



## 5 Details of learning

​		作者优化算法选用SGD，batch_size=128，momentum=0.9，weight_decay=0.0005。作者发现这个权重衰减并不主要是起正则化效果，而是能降低训练集错误率。权重初始化服从$\mathcal{N}(0,0.01^2)$。第2个、第4个和第5个卷积层的bias被初始化为1.0。作者说这能使训练早期预激活值都>0，辅助ReLU加速收敛。其它层的bias都被初始化为0。

​		各层的学习率都一样，初始值为0.01。作者使用了“人工Reduce LR On Plateau”——在验证集误差不再降低时手动将学习率变为原来的0.1倍，在训练过程中一共调了3次。



## 6 Results

<img src="..\pictures\AlexNet ILSVRC-2010 error.png" alt="AlexNet-Table 1" style="zoom:67%;" />

<img src="..\pictures\AlexNet ILSVRC-2012 error.png" alt="AlexNet ILSVRC-2012 error" style="zoom: 67%;" />

​		上来先吹了一波牛逼：我SOTA。长篇大论的不想写了。值得注意的是，作者在AlexNet最后一个卷积层后面加了第六个卷积层，在ImageNet上训练完之后又在ILSVRC-2012上微调，得到了16.6%的top-5错误率，就是1 CNN\*那行。然后他又把两个这样微调过的AlexNet Pro和5个标准的AlexNet做平均，拿到了15.3%的top-5错误率，就是7 CNNs\*那行。第二张图前两行分别是1个和5个标准AlexNet做平均的结果。骚操作还挺多



### 6.1 Qualitative Evaluations

<img src="..\pictures\AlexNet-Figure3.png" alt="AlexNet-Figure3" style="zoom:50%;" />

​		这张图应该是CNN可视化的鼻祖了。这是AlexNet的第一个卷积层的可视化，学到的大多是边缘、纹理和色彩信息。网上各种讲“卷积操作到底在卷什么”的视频和博客应该都放了这张图。上面3行是GPU1学的，下面3行是GPU2学的，看起来分工还挺明确的。注意：这个卷积层的后面是ReLU和LRN，一般情况下两个GPU应该没有这么默契，这里分工能这么明确，所以我猜GPU1学亮度，GPU2学彩色这个结果可能跟LRN有关系。

![image-20220128143947023](..\pictures\AlexNet-Figure4.png)

​		想了想，这张图还是有必要放的。右半边的第一列是ILSVRC-2010测试集里面的5张图，后面的各列是6张测试集中经过AlexNet编码之后（倒数第二个全连接层的输出，长为4096的那个向量）和第一列的图片的编码欧氏距离（向量相减之后的L2norm）最短的图片。

​		后面作者又说：总是用4096维的编码向量判断两张图片语义信息的相似度，计算量有点大，可以训练一个自编码器auto-encoder来将这个编码向量压缩到一个短一点的向量上。这个方法绝对比直接对原图应用自编码器效果要好很多，因为自编码器审视图片的方式和CNN不一样，没有例如局部性的先验知识。但其实这个方法就相当于把CNN做成了自编码器的一部分。



## 7 Discussion

​		这篇文章主要说明了用一个大而深的卷积神经网络和纯监督学习方法在挑战性极强的数据集上达到创纪录的成绩是有可能的。值得说明的是，移除任何一个卷积层都会对网络的性能带来不小的伤害，所以保证网络的深度对取得良好的结果有重要的意义。

​		文章结尾还特意强调了一下为了简化实验，他们没有用任何无监督学习预训练方法来改善网络的性能，言外之意就是这么好的成绩纯纯是CNN的优点带来的。作者还展望了一下未来大而深的卷积神经网络很有可能会被用来处理包含事件信息的图像序列（视频） ，因为在视频的时间维度包含了许许多多静态图像无法展现出来的信息。



## 总结

### 	贡献

​		赢得了ILSVRC2012的冠军，展现出了深度CNN在图像处理领域的潜力，掀起CNN研究的热潮，是如今深度学习和AI迅猛发展的重要原因。

### 	动机

​		图片分类是复杂的任务，需要先验知识，CNN自身的平移不变性和特征检测的局部性就包含了处理图像所需的先验信息。

### 	结构

​		5个卷积层，3个全连接层，相互重叠的池化层，LRN，多GPU训练

### 	预处理

​		训练：从256\*256随机裁剪出224\*224，随机水平翻转，结合PCA随机对RGB值进行扰动

​		预测：从256\*256四个角和中心裁剪出5张224\*224，水平翻转，产生十张图片，对结果做平均

### 	最后

​		本文发表于2012年，作者读这篇论文是2022年。LeNet把CNN推向了历史的舞台，可惜由于包括计算量不足的种种原因，没能成为历史的主角。但AlexNet一举在图像分类中打败了所有机器学习算法，真正使得CNN站到了舞台中央。这十年间，CNN蓬勃发展，各种奇异的架构令人眼花缭乱。作者直到写下这篇笔记为止开始学习深度学习也才一年。若有解释不当之处恳请读者指正，若有文章之精髓被笔者忽略则恳请读者加以补充。祝各位都能在理论学习和代码实操的过程中享受到学习的乐趣！

​		强烈建议各位在看完之后去看李沐老师（B站：[跟李沐学AI](https://space.bilibili.com/1567748478)）的：

-   [9年后重读深度学习奠基作之一：AlexNet【论文精读】](https://www.bilibili.com/video/BV1ih411J7Kz)

-   [AlexNet论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1hq4y157t1)

    看完绝对收获满满~

