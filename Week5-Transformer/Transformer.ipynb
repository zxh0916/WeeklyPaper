{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0646d5-6c5a-41a2-b31e-ed9744e1ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from d2l import torch as d2l\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.functional import generate_sp_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff007c0-918f-4701-aaa4-117f517864f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def read_data_nmt():    \n",
    "    with open('cmn-simplified.txt', 'r',\n",
    "             encoding='utf-8') as f:\n",
    "        raw_text = f.readlines()\n",
    "    text = []\n",
    "    def preprocess_nmt(text):\n",
    "        def no_space(char, prev_char):\n",
    "            return char in set(',.!?') and prev_char != ' '\n",
    "        # 使用空格替换不间断空格\n",
    "        # 使用小写字母替换大写字母\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "        # 在单词和标点符号之间插入空格\n",
    "        # out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "        #        for i, char in enumerate(text)]\n",
    "        out = re.sub('[,.?:!，。？：！]', '', text)\n",
    "        return ''.join(out)\n",
    "    \n",
    "    for line in raw_text:\n",
    "        line = line.split('CC-BY 2.0')\n",
    "        text.append(preprocess_nmt(line[0]))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e02dfd-6a17-4d8f-bd49-49eed60a7422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28447\n",
      "hi\t嗨\t\n",
      "hi\t你好\t\n",
      "run\t你用跑的\t\n",
      "stop\t住手\t\n",
      "wait\t等等\t\n"
     ]
    }
   ],
   "source": [
    "text = read_data_nmt()\n",
    "print(len(text))\n",
    "for i in range(5):\n",
    "    print(text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd892bc-e666-4b20-86f2-c0403e04c6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'run', 'stop', 'wait'] ['hi', 'hi', 'run', 'stop', 'wait']\n"
     ]
    }
   ],
   "source": [
    "def en_zh_split(text):\n",
    "    en, zh = [], []\n",
    "    for line in text:\n",
    "        line = line.split('\\t')\n",
    "        en.append(line[0])\n",
    "        zh.append(line[1])\n",
    "    return en, zh\n",
    "\n",
    "text_en, text_zh = en_zh_split(text)\n",
    "print(text_en[:5], text_en[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6eecdd1-6b8a-4dd6-a798-914193ec78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, mode='en'):\n",
    "    if mode == 'en':\n",
    "        return [line.split(' ') for line in lines]\n",
    "    elif mode == 'zh':\n",
    "        return [list(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c210ba-a727-466a-820e-d5da1996c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi'], ['hi'], ['run']] [['i', 'got', 'fired', 'from', 'the', 'company', 'but', 'since', 'i', 'have', 'a', 'little', 'money', 'saved', 'up', 'for', 'the', 'time', 'being', 'i', \"won't\", 'have', 'trouble', 'with', 'living', 'expenses'], ['if', 'a', 'person', 'has', 'not', 'had', 'a', 'chance', 'to', 'acquire', 'his', 'target', 'language', 'by', 'the', 'time', \"he's\", 'an', 'adult', \"he's\", 'unlikely', 'to', 'be', 'able', 'to', 'reach', 'native', 'speaker', 'level', 'in', 'that', 'language']]\n",
      "[['嗨'], ['你', '好'], ['你', '用', '跑', '的']] [['虽', '然', '我', '被', '公', '司', '解', '雇', '了', '但', '是', '我', '还', '有', '点', '存', '款', '所', '以', '目', '前', '不', '用', '担', '心', '生', '计', '问', '题'], ['如', '果', '一', '个', '人', '在', '成', '人', '前', '没', '有', '机', '会', '习', '得', '目', '标', '语', '言', '他', '对', '该', '语', '言', '的', '认', '识', '达', '到', '母', '语', '者', '程', '度', '的', '机', '会', '是', '相', '当', '小', '的']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_en = tokenize(text_en, mode='en')\n",
    "print(tokenized_en[:3], tokenized_en[-2:])\n",
    "tokenized_zh = tokenize(text_zh, mode='zh')\n",
    "print(tokenized_zh[:3], tokenized_zh[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679e593c-0375-4457-9f6b-cd6b3e75a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokens, min_freq):\n",
    "    def count_corpus(tokens):\n",
    "        # 这里的 `tokens` 是 1D 列表或 2D 列表\n",
    "        if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "            # 将词元列表展平成一个列表\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        return Counter(tokens)\n",
    "    \n",
    "    counter = count_corpus(tokens)\n",
    "    token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                               reverse=True)\n",
    "    ordered_dict = OrderedDict(token_freqs)\n",
    "    Vocab = vocab(ordered_dict, min_freq, specials=['<unk>', '<pad>', '<bos>', '<eos>'], special_first=True)\n",
    "    Vocab.set_default_index(Vocab['<unk>'])\n",
    "    return Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab41361-9a0f-4297-b0d8-17eefe54e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = get_vocab(tokens=tokenized_en, min_freq=2)\n",
    "vocab_zh = get_vocab(tokens=tokenized_zh, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877e093c-17e9-4f0e-a9a6-ad4f7f113a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4545\n",
      "2398\n",
      "['<unk>', '<pad>', '<bos>', '<eos>', 'the', 'i', 'to', 'you', 'a', 'is']\n",
      "['<unk>', '<pad>', '<bos>', '<eos>', '我', '的', '了', '你', '他', '不']\n",
      "[406, 559, 1148, 96, 263, 5, 1255, 843, 283, 40, 7, 1722, 0, 0, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_en))\n",
    "print(len(vocab_zh))\n",
    "print(vocab_en.lookup_tokens([i for i in range (10)]))\n",
    "print(vocab_zh.lookup_tokens([i for i in range (10)]))\n",
    "print(vocab_zh(list('愿指引明路的苍蓝星为你闪耀！')+['<unk>', '<pad>', '<bos>', '<eos>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4d1c01-8904-487f-ac81-21b855732a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # 截断\n",
    "    return line + [padding_token] * (num_steps - len(line))  # 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972847b4-c002-42fd-b87f-5c2fdc8958c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 20, 30, 7, 89, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_pad(vocab_en(tokenized_en[5000]), 10, vocab_en['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63eac2f0-85ac-453d-ad9d-0e3f66f1a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array_nmt(lines, vocab, num_steps, label=False):\n",
    "    \"\"\"将机器翻译的文本序列转换成小批量\"\"\"\n",
    "    lines = [vocab(l) for l in lines]\n",
    "    if not label:\n",
    "        lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    else:\n",
    "        lines = [[vocab['<bos>']] + l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b86cd9a3-c489-47e1-a195-e80b4aef9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 1501,    3,    1,    1],\n",
      "        [   2, 1501,    3,    1,    1],\n",
      "        [   2,  547,    3,    1,    1],\n",
      "        [   2,  258,    3,    1,    1],\n",
      "        [   2,  213,    3,    1,    1]]) tensor([3, 3, 3, 3, 3])\n",
      "[['hi'], ['hi'], ['run'], ['stop'], ['wait']]\n"
     ]
    }
   ],
   "source": [
    "array, valid_len = build_array_nmt(tokenized_en[:5], vocab_en, 5, True)\n",
    "print(array, valid_len)\n",
    "print(tokenized_en[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e8e6b16-bafc-4724-aadf-5c0559c846ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, max_len_en, max_len_zh, source='en', min_freq=0):\n",
    "        super().__init__()\n",
    "        self.data = self.raw_dataset(max_len_en, max_len_zh, source, min_freq)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data[0].shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return [self.data[i][index] for i in range(len(self.data))]\n",
    "    \n",
    "    def raw_dataset(self, max_len_en, max_len_zh, source, min_freq):\n",
    "        text = read_data_nmt()\n",
    "        text_en, text_zh = en_zh_split(text)\n",
    "        tokenized_en = tokenize(text_en, mode='en')\n",
    "        tokenized_zh = tokenize(text_zh, mode='zh')\n",
    "        self.vocab_en = get_vocab(tokens=tokenized_en, min_freq=min_freq)\n",
    "        self.vocab_zh = get_vocab(tokens=tokenized_zh, min_freq=min_freq)\n",
    "        if source == 'en':\n",
    "            enc_input, enc_valid_len = build_array_nmt(tokenized_en, self.vocab_en, max_len_en, False)\n",
    "            target, dec_valid_len = build_array_nmt(tokenized_zh, self.vocab_zh, max_len_zh, False)\n",
    "            dec_input, _ = build_array_nmt(tokenized_zh, self.vocab_zh, max_len_zh, True)\n",
    "        elif source == 'zh':\n",
    "            enc_input, enc_valid_len = build_array_nmt(tokenized_zh, self.vocab_zh, max_len_zh, False)\n",
    "            target, dec_valid_len = build_array_nmt(tokenized_en, self.vocab_en, max_len_en, False)\n",
    "            dec_input, _ = build_array_nmt(tokenized_en, self.vocab_en, max_len_en, True)\n",
    "        return enc_input, enc_valid_len, dec_input, target, dec_valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b23ed73-f475-49b8-85e4-b9cb5ff5e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 75,   8, 308,  44,  36,  65,   8]), tensor(7), tensor([  2, 160, 145,  11,  18,  24,  12]), tensor([160, 145,  11,  18,  24,  12, 105]), tensor(7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28447"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = NMTDataset(7, 7, 'en')\n",
    "print(dataset_test[28446])\n",
    "len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025ad5e5-c339-4586-aa46-df1bb20263a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(1), tensor(2), tensor(3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "print(type(a))\n",
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a297b8e8-94cd-40e6-a4b1-9be600f95f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2sentense(indices, vocab):\n",
    "    \n",
    "    if isinstance(indices[0], torch.Tensor) and indices[0].dim() >= 1:\n",
    "        return [idx2sentense(sentense, vocab) for sentense in indices]\n",
    "    elif isinstance(indices, torch.Tensor):\n",
    "        \n",
    "        special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "        # special_tokens = []\n",
    "        sentense = []\n",
    "        for index in indices:\n",
    "            if vocab.lookup_token(int(index)) not in special_tokens:\n",
    "                sentense.append(vocab.lookup_token(int(index)))\n",
    "            if vocab.lookup_token(int(index)) == '<eos>':\n",
    "                break\n",
    "        return sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de57daa-272b-40d2-a805-53c1b6cc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi'], ['hi'], ['run'], ['stop'], ['wait'], ['wait'], ['begin'], ['hello'], ['i', 'try'], ['i', 'won']]\n",
      "['is', 'tom', 'hurt']\n",
      "[['嗨'], ['你', '好'], ['你', '用', '跑', '的'], ['住', '手'], ['等', '等'], ['等', '一', '下'], ['开', '始'], ['你', '好'], ['我', '试', '试'], ['我', '赢', '了']]\n",
      "['汤', '姆', '受', '伤', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "print(idx2sentense([dataset_test[i][0] for i in range(10)], vocab_en))\n",
    "print(idx2sentense(dataset_test[500][0], vocab_en))\n",
    "print(idx2sentense([dataset_test[i][2] for i in range(10)], vocab_zh))\n",
    "print(idx2sentense(dataset_test[500][2], vocab_zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dcd73d3-66b8-4d30-900c-c04eeeb27a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU(pred_seq, label_seq, k=4):\n",
    "    if pred_seq == []:\n",
    "        return 0\n",
    "    if isinstance(pred_seq[0], list):\n",
    "        assert(len(pred_seq) == len(label_seq))\n",
    "        scores = [BLEU(pred, label) for (pred, label) in zip(pred_seq, label_seq)]\n",
    "        return sum(scores) / len(scores)\n",
    "    len_pred, len_label = len(pred_seq), len(label_seq)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        if n > len_pred:\n",
    "            break\n",
    "        num_matches, label_subs = 0, defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[''.join(label_seq[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[''.join(pred_seq[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[''.join(pred_seq[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "073e6ffd-8df0-4765-b3b7-ee160373310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2BLEU(output, target, vocab):\n",
    "    output = idx2sentense(output, vocab)\n",
    "    target = idx2sentense(target, vocab)\n",
    "    # print(output[0], target[0])\n",
    "    return BLEU(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d9fb31a-925f-4e32-a8d2-0689280564db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenses = [dataset_test[i][0] for i in range(10)]\n",
    "idx2BLEU(sentenses, sentenses, vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf1482f6-435d-4042-9de8-d3380a596fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74aee134-d1f3-4eae-9ca9-d00292a418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"实现多头点积注意力\"\"\"\n",
    "    def __init__(self, d_model, num_head=8):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.d = d_model // num_head\n",
    "        self.scale = self.d ** -0.5\n",
    "        if d_model % num_head != 0:\n",
    "            print('!!!!Warning: d_model % num_head != 0!!!!')\n",
    "        self.linear_q = nn.Linear(d_model, self.d * num_head, bias=False)\n",
    "        self.linear_k = nn.Linear(d_model, self.d * num_head, bias=False)\n",
    "        self.linear_v = nn.Linear(d_model, self.d * num_head, bias=False)\n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "        self.output_layer = nn.Linear(self.d * num_head, d_model, bias=False)\n",
    "        initialize_weight(self.output_layer)\n",
    "    \n",
    "    def sequence_mask(self, X, valid_len, value):\n",
    "        \"\"\"根据有效长度将注意力分数矩阵每个query的末尾元素用掩码覆盖\"\"\"\n",
    "        maxlen = X.shape[3]\n",
    "        # [1, 1, d] + [batch_size, num_query, 1] -> [batch_size, num_query, d]\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, None, :] >= valid_len[:, :, None]\n",
    "        # shape of mask: [batch_size, num_heads, num_query, d]\n",
    "        mask = mask.unsqueeze(1).repeat(1, X.shape[1], 1, 1)\n",
    "        X[mask] = value\n",
    "        return X\n",
    "        \n",
    "    def masked_softmax(self, X, valid_len):\n",
    "        \"\"\"带掩码的softmax，有效长度可以是每个batch一个（适用于编码器），\n",
    "        也可以是每个query一个（适用于训练时的解码器自注意力）\"\"\"\n",
    "        if valid_len is None:\n",
    "            return nn.functional.softmax(X, dim=-1)\n",
    "        else:\n",
    "            # 有效长度是一维向量:对批量中的每个样本指定一个有效长度\n",
    "            # 有效长度是二维张量:对批量中每个样本的每个query都指定一个有效长度\n",
    "            assert(valid_len.dim() in [1, 2])\n",
    "            if valid_len.dim() == 1:\n",
    "                valid_len = valid_len.reshape(-1, 1).repeat(1, X.shape[2])\n",
    "            X = self.sequence_mask(X, valid_len, value=-1e9)\n",
    "            return nn.functional.softmax(X, dim=-1)\n",
    "        \n",
    "    def forward(self, q, k, v, valid_len):\n",
    "        d = self.d\n",
    "        batch_size = q.shape[0]\n",
    "        assert(k.shape[1] == v.shape[1])\n",
    "        if valid_len is not None:\n",
    "            assert(valid_len.shape[0] == batch_size)\n",
    "        \n",
    "        q = self.linear_q(q).reshape(batch_size, -1, self.num_head, d)\n",
    "        k = self.linear_k(k).reshape(batch_size, -1, self.num_head, d)\n",
    "        v = self.linear_v(v).reshape(batch_size, -1, self.num_head, d)\n",
    "        \n",
    "        # [batch_size, #q/#k/#v, num_heads, d] -> [batch_size, num_heads, #q/#k/#v, d]\n",
    "        q, v, k = q.transpose(1, 2), v.transpose(1, 2), k.transpose(1, 2)\n",
    "        \n",
    "        # [batch_size, num_heads, #q, #k/#v]\n",
    "        x = torch.matmul(q, (k.transpose(2, 3))) * self.scale\n",
    "        x = self.masked_softmax(x, valid_len)\n",
    "        x = torch.matmul(x, v)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.reshape(batch_size, -1, self.num_head * self.d)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4586b080-4639-4880-98ab-2b84ff510f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 10])\n",
      "torch.Size([3, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "atten = MultiHeadAttention(10, 2)\n",
    "x = torch.randn(3, 5, 10)\n",
    "valid_len = torch.tensor([[2,3,1,2,3],\n",
    "                          [1,3,2,1,3],\n",
    "                          [3,2,1,3,2]])\n",
    "valid_len2 = torch.tensor([2,3,1])\n",
    "print(atten(x,x,x, valid_len).shape)\n",
    "print(atten(x,x,x, valid_len2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "111ead41-9a58-43ba-8733-8208e01f3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(d_model, hidden_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer2 = nn.Linear(hidden_size, d_model)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e59bdf11-2e87-4c91-9fbb-00abb8b63f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_Norm(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        \n",
    "    def forward(self, res, x):\n",
    "        return self.norm(res + self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "927a4c84-1d79-4484-a238-47ccc190f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden_size, dropout, num_head):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_head)\n",
    "        self.add_norm1 = Add_Norm(d_model, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, ffn_hidden_size)\n",
    "        self.add_norm2 = Add_Norm(d_model, dropout)\n",
    "    \n",
    "    def forward(self, x, enc_valid_len):\n",
    "        y = self.self_attention(x, x, x, enc_valid_len)\n",
    "        y = self.add_norm1(x, y)\n",
    "        z = self.ffn(y)\n",
    "        z = self.add_norm2(y, z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d716326-62d8-4613-bbc9-70fd0eea953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden_size, dropout, num_head, i):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(d_model, num_head)\n",
    "        self.add_norm1 = Add_Norm(d_model, dropout)\n",
    "        \n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model, num_head)\n",
    "        self.add_norm2 = Add_Norm(d_model, dropout)\n",
    "        \n",
    "        self.ffn = FeedForwardNetwork(d_model, ffn_hidden_size)\n",
    "        self.add_norm3 = Add_Norm(d_model, dropout)\n",
    "        \n",
    "    def forward(self, x, state):\n",
    "        \n",
    "        enc_output, enc_valid_len = state[0], state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            self_kv = x\n",
    "        else:\n",
    "            self_kv = torch.concat([state[2][self.i], x], dim=1)\n",
    "        state[2][self.i] = self_kv\n",
    "        \n",
    "        if self.training:\n",
    "            batch_size, num_steps, d = x.shape\n",
    "            # 训练时，一个样本中有效长度应该与query在序列中的位置相等\n",
    "            # shape of `dec_valid_len`: [batch_size, num_steps]\n",
    "            dec_valid_len = torch.arange(1, num_steps+1, device=x.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_len = None\n",
    "\n",
    "        y = self.self_attention(x, self_kv, self_kv, dec_valid_len)\n",
    "        y = self.add_norm1(x, y)\n",
    "        z = self.enc_dec_attention(y, enc_output, enc_output, enc_valid_len)\n",
    "        z = self.add_norm2(y, z)\n",
    "        out = self.ffn(z)\n",
    "        out = self.add_norm3(z, out)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b435df8-cb88-4b0c-bf17-88132d713230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.P = torch.zeros((1, max_len, d_model))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, d_model, 2, dtype=torch.float32) / d_model)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # print(X.shape, self.P.shape)\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "280889ed-d05e-4317-8502-f64160724184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, d_model, ffn_hidden_size, dropout, num_head, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb_scale = d_model ** 0.5\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # nn.init.normal_(self.embedding.weight, mean=0, std=d_model**-0.5)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, ffn_hidden_size, dropout, num_head) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x, enc_valid_len):\n",
    "        out = self.positional_encoding(self.embedding(x) * self.emb_scale)\n",
    "        for m in self.layers:\n",
    "            out = m(out, enc_valid_len)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "492fe0ee-dbc9-4588-b2fe-ecc6e6f963f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, d_model, ffn_hidden_size, dropout, num_head, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb_scale = d_model ** 0.5\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # nn.init.normal_(self.embedding.weight, mean=0, std=d_model**-0.5)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, ffn_hidden_size, dropout, num_head, i) for i in range(N)])\n",
    "        \n",
    "    def forward(self, x, state):\n",
    "        out = self.positional_encoding(self.embedding(x) * self.emb_scale)\n",
    "        for m in self.layers:\n",
    "            out, state = m(out, state)\n",
    "        # shape of `self.embedding.weight`: [vocab_size, d_model]\n",
    "        out = torch.matmul(out, self.embedding.weight.T)\n",
    "        return out, state\n",
    "    \n",
    "    def init_state(self, enc_output, enc_valid_len):\n",
    "        N = len(self.layers)\n",
    "        return [enc_output, enc_valid_len, [None for _ in range(N)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c5f4fb-7c60-4376-b86d-5f80af2fc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 N,\n",
    "                 d_model,\n",
    "                 ffn_hidden_size,\n",
    "                 dropout,\n",
    "                 num_head):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.d_model = d_model\n",
    "        self.ffn_hidden_size = ffn_hidden_size\n",
    "        self.num_head = num_head\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = Encoder(N, d_model, ffn_hidden_size, dropout, num_head, src_vocab_size)\n",
    "        self.decoder = Decoder(N, d_model, ffn_hidden_size, dropout, num_head, trg_vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, enc_valid_len, targets):\n",
    "        enc_outputs = self.encoder(inputs, enc_valid_len)\n",
    "        state = self.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "        out, state = self.decoder(targets, state)\n",
    "        return out, state\n",
    "    \n",
    "    def print_num_params(self):\n",
    "        total_trainable_params = sum(\n",
    "            p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f'{total_trainable_params:,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a7ea6f0-ea61-46a0-bf38-e1d3ca6b16c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 120, 10])\n",
      "torch.Size([3, 50, 3])\n"
     ]
    }
   ],
   "source": [
    "net = Transformer(3, 3, 3, 10, 20, 0, 2)\n",
    "# print(net.encoder)\n",
    "# print(net.decoder)\n",
    "test_input = torch.zeros(3, 120, dtype=torch.long)\n",
    "valid_len = torch.tensor([30, 5, 70])\n",
    "print(net.encoder(test_input, valid_len).shape)\n",
    "test_target = torch.zeros(3, 50, dtype=torch.long)\n",
    "print(net(test_input, valid_len, test_target)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a26053a-7f71-4817-8756-aeb20c8b7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带掩码的softmax交叉熵损失函数\"\"\"\n",
    "    # `pred` 的形状：(`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` 的形状：(`batch_size`, `num_steps`)\n",
    "    # `valid_len` 的形状：(`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = self.sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss / pred.shape[0]\n",
    "    \n",
    "    def sequence_mask(self, X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f6a8758-9d72-4986-a6ca-815f4f617c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  11, 1611,   15,   29,   38,   23,    8],\n",
      "        [1523,    4,  462,    3,    1,    1,    1],\n",
      "        [  49,   60,  241, 1085,   18,  134,    3],\n",
      "        [  85,   81, 3055,  176,    3,    1,    1],\n",
      "        [  10,    9, 1548, 3744,    3,    1,    1]])\n",
      "tensor([7, 4, 7, 5, 5])\n",
      "tensor([[   2,    8,  344,  275,   62,   57,   11],\n",
      "        [   2,   13,   10,  222, 1107,    3,    1],\n",
      "        [   2,   65,    4,   39,   34,   20,   54],\n",
      "        [   2,    4,   55,  109,  117,  936,   64],\n",
      "        [   2,   14,   15,  178,   88,    5,  925]])\n",
      "tensor([[   8,  344,  275,   62,   57,   11,   67],\n",
      "        [  13,   10,  222, 1107,    3,    1,    1],\n",
      "        [  65,    4,   39,   34,   20,   54,   10],\n",
      "        [   4,   55,  109,  117,  936,   64,    3],\n",
      "        [  14,   15,  178,   88,    5,  925, 1184]])\n",
      "tensor([7, 5, 7, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "dataloader_test = DataLoader(dataset_test, batch_size=5, shuffle=True)\n",
    "for batch in dataloader_test:\n",
    "    for thing in batch:\n",
    "        print(thing)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c450ce-88c1-4372-b309-e96a28887b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_LR(d_model, warmup_steps, cur_step):\n",
    "    if cur_step == 0:\n",
    "        return 0\n",
    "    lr = d_model ** -0.5\n",
    "    lr *= min(cur_step ** -0.5, (cur_step * warmup_steps ** -1.5))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bb26c94-0cbc-46c5-8430-16fd90ebe82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(net, data_iter, base_lr, warmup_steps, num_iters, source_vocab, target_vocab, device):\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=base_lr, betas=[0.9, 0.98], eps=1e-9)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=base_lr, momentum=0.9)\n",
    "    get_lr = lambda cur_step: warmup_LR(net.d_model, warmup_steps, cur_step)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr)\n",
    "    criterion = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    iter_counter = 1\n",
    "    \n",
    "    # writer = SummaryWriter(f'runs/Transformer_N={net.N}_head={net.num_head}_d={net.d_model}_ffn={net.ffn_hidden_size}_dropout={net.dropout}')\n",
    "    \n",
    "    timer = d2l.Timer()\n",
    "    metric = d2l.Accumulator(3)  # 训练损失总和，词元数量\n",
    "    while True:\n",
    "        for batch in data_iter:\n",
    "            train = iter_counter < num_iters\n",
    "            optimizer.zero_grad()\n",
    "            enc_input, enc_valid_len, dec_input, target, dec_valid_len = [x.to(device) for x in batch]\n",
    "            output, _ = net(enc_input, enc_valid_len, dec_input)\n",
    "            l = criterion(output, target, dec_valid_len)\n",
    "            l.sum().backward() # 损失函数的标量进行“反向传播”\n",
    "            # d2l.grad_clipping(net, 1)\n",
    "            num_tokens = dec_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            iter_counter += 1\n",
    "            scheduler.step()\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(output, dim=-1)\n",
    "                # print(pred[0])\n",
    "                bleu = idx2BLEU(pred, target, target_vocab)\n",
    "                metric.add(l.sum(), num_tokens, bleu)\n",
    "            # writer.add_scalar(float(l.sum()), 'train/loss', global_step=iter_counter)\n",
    "            # writer.add_scalar(bleu, 'train/BLEU', global_step=iter_counter)\n",
    "            # writer.add_scalar(optimizer.state_dict()['param_groups'][0]['lr'], \n",
    "            #                   'learning rate', global_step=iter_counter)\n",
    "            if iter_counter % 100 == 0:\n",
    "                print(f'iter {iter_counter:6d}, loss = {l.sum().item():8.4f}, bleu = {bleu:.8f}')\n",
    "                print(idx2sentense(pred[0], target_vocab), '\\n',\n",
    "                      idx2sentense(dec_input[0], target_vocab), '\\n',\n",
    "                      idx2sentense(enc_input[0], source_vocab))\n",
    "            if not train:\n",
    "                break\n",
    "        if not train:\n",
    "            break\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, BLEU {metric[2] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42a0a128-e380-4f90-adc9-9ba49d1dcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, src_sentense, src_vocab, trg_vocab, num_steps, source='en'):\n",
    "    if isinstance(src_sentense, list):\n",
    "        return [predict(net, sentense, src_vocab, trg_vocab, num_steps, source) for sentense in src_sentense]\n",
    "    net.eval()\n",
    "    if source == 'en':\n",
    "        src_tokens = src_vocab(src_sentense.lower().split(' ') + ['<eos>'])\n",
    "    else:\n",
    "        src_tokens = src_vocab(list(src_sentense) + ['<eos>'])\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [trg_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq = []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == trg_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(trg_vocab.lookup_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc3c3d80-0850-4f00-8c2a-ba293440da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zh2en = NMTDataset(max_len_en=20, max_len_zh=20, min_freq=1, source='zh')\n",
    "dataloader_zh2en = DataLoader(dataset_zh2en, batch_size=256, shuffle=True, num_workers=0)\n",
    "# dataset_en2zh = NMTDataset(max_len_en=20, max_len_zh=20, min_freq=1, source='en')\n",
    "# dataloader_en2zh = DataLoader(dataset_en2zh, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aabc9fe4-1ad8-47a9-9c08-698d03502a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,632,576 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "net_zh2en = Transformer(src_vocab_size=len(dataset_zh2en.vocab_zh),\n",
    "                        trg_vocab_size=len(dataset_zh2en.vocab_en),\n",
    "                        N=4,\n",
    "                        d_model=128,\n",
    "                        ffn_hidden_size=256,\n",
    "                        dropout=0.1,\n",
    "                        num_head=4).to(device)\n",
    "net_zh2en.print_num_params()\n",
    "# net_en2zh = Transformer(src_vocab_size=len(dataset_en2zh.vocab_en),\n",
    "#                         trg_vocab_size=len(dataset_en2zh.vocab_zh),\n",
    "#                         N=4,\n",
    "#                         d_model=128,\n",
    "#                         ffn_hidden_size=256,\n",
    "#                         dropout=0,\n",
    "#                         num_head=4).to(device)\n",
    "# net_en2zh.print_num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc1029-e94c-401b-ae99-5071ef72de0d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    100, loss =   7.8633, bleu = 0.00009918\n",
      "['carved', 'carved', 'the', 'your'] \n",
      " ['she', 'poured', 'brandy', 'into', 'the', 'glasses'] \n",
      " ['她', '把', '白', '兰', '地', '倒', '进', '玻', '璃', '杯', '里']\n",
      "iter    200, loss =   6.0218, bleu = 0.00007155\n",
      "['intelligent', 'the', 'equals', 'handle', 'laughed', 'yawn', 'unforgettable'] \n",
      " ['tom', 'told', 'me', 'that', \"he'd\", 'like', 'to', 'become', 'a', 'doctor'] \n",
      " ['汤', '姆', '告', '诉', '我', '他', '想', '当', '医', '生']\n",
      "iter    300, loss =   2.9847, bleu = 0.00022436\n",
      "['the'] \n",
      " ['please', 'remind', 'me', 'to', 'mail', 'the', 'report', 'tomorrow'] \n",
      " ['请', '提', '醒', '我', '明', '天', '把', '报', '告', '寄', '了']\n",
      "iter    400, loss =   2.8959, bleu = 0.00000969\n",
      "[] \n",
      " ['he', 'said', '\"i\\'m', 'from', 'canada\"'] \n",
      " ['他', '说', '“', '我', '是', '加', '拿', '大', '来', '的', '”']\n",
      "iter    500, loss =   2.8679, bleu = 0.00037357\n",
      "[] \n",
      " ['he', 'is', 'good', 'at', 'dealing', 'with', 'children'] \n",
      " ['他', '擅', '长', '应', '付', '小', '孩', '子']\n",
      "iter    600, loss =   2.7850, bleu = 0.00052746\n",
      "[] \n",
      " ['tom', 'has', 'a', 'veterinary', 'background'] \n",
      " ['t', 'o', 'm', '有', '兽', '医', '背', '景']\n",
      "iter    700, loss =   2.8705, bleu = 0.00123936\n",
      "['certain'] \n",
      " ['he', 'cut', 'the', 'envelope', 'open'] \n",
      " ['他', '裁', '开', '了', '那', '个', '信', '封']\n",
      "iter    800, loss =   2.7241, bleu = 0.00104427\n",
      "['i'] \n",
      " ['tom', 'was', 'a', 'little', 'drunk'] \n",
      " ['汤', '姆', '有', '点', '醉']\n",
      "iter    900, loss =   2.5883, bleu = 0.00130960\n",
      "['i'] \n",
      " ['i', 'do', 'my', 'homework', 'after', 'school'] \n",
      " ['我', '在', '放', '学', '后', '做', '作', '业']\n",
      "iter   1000, loss =   2.6059, bleu = 0.00166019\n",
      "['he', 'the', 'the', 'the'] \n",
      " ['tom', 'has', 'lost', 'interest', 'in', 'studying', 'french'] \n",
      " ['汤', '姆', '已', '经', '失', '去', '学', '习', '法', '语', '的', '兴', '趣']\n",
      "iter   1100, loss =   2.5981, bleu = 0.00058486\n",
      "['i', 'the'] \n",
      " ['visiting', 'a', 'foreign', 'country', 'must', 'be', 'expensive'] \n",
      " ['到', '外', '国', '一', '定', '很', '贵']\n",
      "iter   1200, loss =   2.5320, bleu = 0.00343291\n",
      "['you', 'you'] \n",
      " [\"aren't\", 'you', 'cold'] \n",
      " ['你', '不', '冷', '吗']\n",
      "iter   1300, loss =   2.7054, bleu = 0.00007511\n",
      "['you', 'you'] \n",
      " ['what', 'are', 'your', 'strengths', 'and', 'weaknesses'] \n",
      " ['你', '们', '的', '优', '点', '和', '缺', '点', '是', '什', '么']\n",
      "iter   1400, loss =   2.5025, bleu = 0.00240084\n",
      "['the', 'the'] \n",
      " ['how', 'about', 'another', 'cup', 'of', 'coffee'] \n",
      " ['要', '不', '要', '再', '来', '一', '杯', '咖', '啡']\n",
      "iter   1500, loss =   2.4139, bleu = 0.00019448\n",
      "['you', 'you', 'you', 'you'] \n",
      " ['the', 'less', 'you', 'know', 'the', 'better', 'ok'] \n",
      " ['你', '知', '道', '的', '越', '少', '越', '好', '好', '吗']\n",
      "iter   1600, loss =   2.5668, bleu = 0.00000000\n",
      "['i', 'i', 'to', 'to', 'to'] \n",
      " ['let', 'me', 'introduce', 'my', 'friends', 'to', 'you'] \n",
      " ['我', '给', '你', '介', '绍', '介', '绍', '我', '的', '朋', '友']\n",
      "iter   1700, loss =   2.4729, bleu = 0.00147180\n",
      "['i', 'you'] \n",
      " ['you', 'could', 'be', 'right', 'i', 'suppose'] \n",
      " ['我', '猜', '想', '你', '可', '能', '是', '对', '的']\n",
      "iter   1800, loss =   2.6554, bleu = 0.00087160\n",
      "['you', 'you', 'you', 'you', 'you', 'to', 'you'] \n",
      " ['i', 'know', 'everything', 'that', \"you've\", 'done'] \n",
      " ['我', '知', '道', '你', '做', '的', '所', '有', '事']\n",
      "iter   1900, loss =   2.4255, bleu = 0.00163151\n",
      "['the', 'the'] \n",
      " ['it', 'really', 'works'] \n",
      " ['真', '的', '有', '用']\n",
      "iter   2000, loss =   2.4424, bleu = 0.00144671\n",
      "['you', 'you'] \n",
      " [\"it's\", 'very', 'big', 'of', 'you', 'to', 'admit', \"you're\", 'wrong'] \n",
      " ['你', '能', '承', '认', '你', '错', '了', '是', '很', '伟', '大', '的']\n",
      "iter   2100, loss =   2.3931, bleu = 0.00228633\n",
      "['i', 'to', 'to', 'to', 'to', 'to'] \n",
      " ['we', 'hardly', 'have', 'time', 'to', 'eat', 'breakfast'] \n",
      " ['我', '们', '几', '乎', '没', '空', '吃', '早', '饭']\n",
      "iter   2200, loss =   2.3319, bleu = 0.00743379\n",
      "['you', 'you', 'you'] \n",
      " [\"you'll\", 'be', 'answering', 'the', 'phones'] \n",
      " ['你', '要', '接', '电', '话']\n",
      "iter   2300, loss =   2.3746, bleu = 0.00232618\n",
      "['you', 'have', 'have', 'to', 'you'] \n",
      " ['answer', 'the', 'telephone', 'will', 'you'] \n",
      " ['请', '你', '接', '听', '一', '下', '电', '话', '好', '吗']\n",
      "iter   2400, loss =   2.3884, bleu = 0.00019448\n",
      "['i', 'is', 'a', 'my', 'a'] \n",
      " [\"i've\", 'got', 'a', 'bike'] \n",
      " ['我', '有', '一', '辆', '自', '行', '车']\n",
      "iter   2500, loss =   2.2451, bleu = 0.00300965\n",
      "['this', 'the', 'this'] \n",
      " ['smoking', 'is', 'not', 'allowed', 'here'] \n",
      " ['这', '里', '不', '允', '许', '抽', '烟']\n",
      "iter   2600, loss =   2.2498, bleu = 0.00319778\n",
      "['the', 'the', 'the'] \n",
      " ['finishing', 'the', 'report', 'by', 'tomorrow', 'is', 'next', 'to', 'impossible'] \n",
      " ['在', '明', '天', '前', '完', '成', '报', '告', '几', '乎', '是', '不', '可', '能', '的']\n",
      "iter   2700, loss =   2.1523, bleu = 0.00052865\n",
      "['you', 'have', 'you', 'you', 'you'] \n",
      " ['you', 'should', 'do', 'that', 'soon'] \n",
      " ['你', '应', '该', '很', '快', '那', '样', '做']\n",
      "iter   2800, loss =   2.3226, bleu = 0.00143703\n",
      "['the', 'is', 'is', 'a'] \n",
      " ['a', 'capital', 'letter', 'is', 'used', 'at', 'the', 'beginning', 'of', 'a', 'sentence'] \n",
      " ['一', '个', '句', '子', '以', '一', '个', '大', '写', '字', '母', '开', '始']\n",
      "iter   2900, loss =   2.1883, bleu = 0.00586749\n",
      "['i', 'i', 'me', 'have'] \n",
      " [\"there's\", 'nothing', 'i', 'can', 'do'] \n",
      " ['没', '有', '我', '可', '以', '做', '的', '事']\n",
      "iter   3000, loss =   2.1863, bleu = 0.00234744\n",
      "['you', 'is', 'you'] \n",
      " ['this', 'letter', 'is', 'addressed', 'to', 'you'] \n",
      " ['这', '封', '信', '是', '寄', '给', '你']\n",
      "iter   3100, loss =   2.2256, bleu = 0.00493593\n",
      "['do', 'you', 'know'] \n",
      " ['do', 'you', 'know', 'whose', 'handwriting', 'this', 'is'] \n",
      " ['你', '知', '道', '这', '是', '谁', '的', '字', '吗']\n",
      "iter   3200, loss =   2.2704, bleu = 0.00191650\n",
      "['the', 'is', 'the'] \n",
      " ['two', 'times', 'two', 'is', 'four'] \n",
      " ['二', '乘', '以', '二', '等', '于', '四']\n",
      "iter   3300, loss =   2.3110, bleu = 0.00834256\n",
      "['i', 'have', 'the', 'the', 'the'] \n",
      " ['i', 'am', 'able', 'to', 'drive', 'a', 'car'] \n",
      " ['我', '会', '开', '车']\n",
      "iter   3400, loss =   2.1977, bleu = 0.00591151\n",
      "['the', 'have', 'the', 'a', 'the', 'a'] \n",
      " ['you', \"can't\", 'fight', 'a', 'good', 'fight', 'with', 'such', 'a', 'defeatist', 'attitude'] \n",
      " ['抱', '著', '失', '败', '主', '义', '的', '态', '度', '你', '无', '法', '打', '一', '场', '漂', '亮', '的', '仗']\n",
      "iter   3500, loss =   2.1463, bleu = 0.00287714\n",
      "['tom', 'tom', 'tom'] \n",
      " ['i', 'made', 'tom', 'eat', 'it'] \n",
      " ['我', '让', '汤', '姆', '吃', '它']\n",
      "iter   3600, loss =   2.2009, bleu = 0.00597085\n",
      "['he', 'was', 'to', 'to'] \n",
      " ['he', 'was', 'scared', 'you', 'would', 'shoot', 'him'] \n",
      " ['他', '害', '怕', '你', '会', '开', '枪', '打', '他']\n",
      "iter   3700, loss =   2.2308, bleu = 0.00414927\n",
      "['i'] \n",
      " [\"i'm\", 'job', 'hunting'] \n",
      " ['我', '在', '找', '工', '作']\n",
      "iter   3800, loss =   2.0950, bleu = 0.00203433\n",
      "['the', 'the'] \n",
      " ['please', 'close', 'the', 'door'] \n",
      " ['请', '关', '门']\n",
      "iter   3900, loss =   2.1705, bleu = 0.02005087\n",
      "['i', \"don't\", 'a', 'to', 'the'] \n",
      " ['i', 'prefer', 'walking', 'to', 'cycling'] \n",
      " ['我', '更', '愿', '意', '走', '路', '而', '不', '是', '骑', '自', '行', '车']\n",
      "iter   4000, loss =   2.1609, bleu = 0.00236926\n",
      "['i', 'have', 'to', 'to', 'the', 'conservationists'] \n",
      " ['i', 'made', 'him', 'carry', 'the', 'suitcase'] \n",
      " ['我', '要', '他', '提', '行', '李', '箱']\n",
      "iter   4100, loss =   2.0692, bleu = 0.00504773\n",
      "['this', 'is', 'is', 'the'] \n",
      " ['this', 'room', 'is', 'air-conditioned'] \n",
      " ['这', '个', '房', '间', '有', '空', '调']\n",
      "iter   4200, loss =   2.0429, bleu = 0.01127474\n",
      "['tom', 'is', 'a', 'the'] \n",
      " ['tom', 'went', 'to', 'sleep'] \n",
      " ['汤', '姆', '睡', '着', '了']\n",
      "iter   4300, loss =   2.1048, bleu = 0.01224646\n",
      "['i', 'not'] \n",
      " [\"i'm\", 'feeling', 'blue', 'today'] \n",
      " ['我', '今', '天', '的', '心', '情', '不', '好']\n",
      "iter   4400, loss =   2.0593, bleu = 0.00971134\n",
      "['this', 'is', 'is'] \n",
      " ['everybody', 'thinks', 'so'] \n",
      " ['大', '家', '都', '是', '这', '样', '想', '的']\n",
      "iter   4500, loss =   2.1206, bleu = 0.00652630\n",
      "['i', \"can't\", 'is', 'i', 'to', 'i', 'my'] \n",
      " ['this', 'tie', \"doesn't\", 'go', 'with', 'my', 'suit'] \n",
      " ['这', '条', '领', '带', '跟', '我', '的', '西', '装', '不', '配']\n",
      "iter   4600, loss =   2.0391, bleu = 0.00236926\n",
      "['i', 'your', 'your', 'you', 'you'] \n",
      " ['look', 'at', 'me', 'with', 'your', 'books', 'closed'] \n",
      " ['把', '你', '的', '书', '合', '起', '来', '看', '著', '我']\n",
      "iter   4700, loss =   1.9435, bleu = 0.01477854\n",
      "['i', \"don't\", 'that', 'that'] \n",
      " ['i', 'think', 'about', 'it', 'often'] \n",
      " ['我', '经', '常', '想', '着', '它']\n",
      "iter   4800, loss =   1.9837, bleu = 0.00306284\n",
      "['do', 'you', 'have', 'to', 'the'] \n",
      " ['can', 'you', 'think', 'of', 'something', 'better'] \n",
      " ['你', '能', '想', '到', '更', '好', '的', '东', '西', '吗']\n",
      "iter   4900, loss =   2.1144, bleu = 0.00476856\n",
      "['have', 'a', 'a', 'a'] \n",
      " ['is', 'there', 'someone', 'that', 'can', 'drive'] \n",
      " ['有', '人', '会', '开', '车', '吗']\n",
      "iter   5000, loss =   2.0744, bleu = 0.00289791\n",
      "[\"i'm\", 'was', '\"where\\'s', 'to'] \n",
      " ['my', 'little', 'brother', 'ran', 'through', 'the', 'living', 'room', 'stark', 'naked'] \n",
      " ['我', '弟', '弟', '全', '身', '光', '溜', '溜', '地', '跑', '过', '客', '厅']\n",
      "iter   5100, loss =   1.9642, bleu = 0.01214224\n",
      "['we', 'have', 'a'] \n",
      " ['we', 'are', 'against', 'working', 'on', 'sundays'] \n",
      " ['我', '们', '反', '对', '星', '期', '日', '工', '作']\n",
      "iter   5200, loss =   1.8694, bleu = 0.02282608\n",
      "['he', 'has', 'a', 'a', 'the'] \n",
      " ['he', 'barely', 'passed', 'the', 'examination'] \n",
      " ['他', '勉', '强', '地', '通', '过', '了', '考', '试']\n",
      "iter   5300, loss =   1.9946, bleu = 0.01831203\n",
      "['tom', 'has', 'a'] \n",
      " ['tom', 'was', 'selected'] \n",
      " ['汤', '姆', '被', '选', '中', '了']\n",
      "iter   5400, loss =   2.0653, bleu = 0.01084885\n",
      "['that', 'have', 'the', 'a', 'the'] \n",
      " ['you', 'cannot', 'purchase', 'this', 'medicine', 'without', 'a', 'prescription'] \n",
      " ['那', '个', '药', '的', '话', '没', '有', '处', '方', '是', '买', '不', '到', '的']\n",
      "iter   5500, loss =   1.9609, bleu = 0.00480449\n",
      "['she', 'is', 'a'] \n",
      " ['she', 'is', 'dieting'] \n",
      " ['她', '在', '节', '食', '中']\n",
      "iter   5600, loss =   2.0587, bleu = 0.01220957\n",
      "['i', 'was', 'that', 'that', 'the', 'to', 'i'] \n",
      " ['i', \"can't\", 'forget', 'what', 'happened', 'that', 'day'] \n",
      " ['我', '无', '法', '忘', '记', '那', '天', '发', '生', '了', '什', '么']\n",
      "iter   5700, loss =   1.9783, bleu = 0.01324420\n",
      "['the', 'is', 'the', 'of'] \n",
      " ['china', 'is', 'about', 'twenty-five', 'times', 'as', 'large', 'as', 'japan'] \n",
      " ['中', '国', '的', '版', '图', '大', '概', '是', '日', '本', '的', '二', '十', '五', '倍']\n",
      "iter   5800, loss =   1.9672, bleu = 0.01894988\n",
      "['i', 'have', 'to', 'you', 'to'] \n",
      " ['i', 'need', 'to', 'ask', 'you', 'a', 'silly', 'question'] \n",
      " ['我', '必', '须', '问', '你', '一', '个', '蠢', '问', '题']\n",
      "iter   5900, loss =   1.9908, bleu = 0.00304219\n",
      "['tom', 'is', 'to'] \n",
      " ['tom', 'looks', 'pale'] \n",
      " ['汤', '姆', '看', '起', '来', '很', '苍', '白']\n",
      "iter   6000, loss =   1.9281, bleu = 0.00687130\n",
      "['we', 'would', 'have'] \n",
      " [\"haven't\", 'we', 'met', 'before'] \n",
      " ['我', '们', '以', '前', '没', '见', '过', '吗']\n",
      "iter   6100, loss =   1.9101, bleu = 0.00655455\n",
      "['this', 'tom', 'is', 'the'] \n",
      " ['the', 'house', 'is', 'haunted'] \n",
      " ['这', '房', '子', '闹', '鬼']\n",
      "iter   6200, loss =   1.8853, bleu = 0.01901541\n",
      "['i', 'do', 'you', 'do', 'you'] \n",
      " ['i', 'came', 'to', 'warn', 'you', 'not', 'to', 'do', 'that'] \n",
      " ['我', '来', '警', '告', '你', '别', '那', '样', '做']\n",
      "iter   6300, loss =   1.9257, bleu = 0.00000000\n",
      "[\"i'm\", 'me'] \n",
      " ['let', 'me', 'have', 'a', 'try'] \n",
      " ['让', '我', '试', '试']\n",
      "iter   6400, loss =   1.8569, bleu = 0.01608399\n",
      "[\"i'm\", 'father', 'is', 'is'] \n",
      " ['my', 'father', 'walks'] \n",
      " ['我', '爸', '爸', '走', '路']\n",
      "iter   6500, loss =   1.8515, bleu = 0.01670477\n",
      "['tom', 'votes', 'not', 'mary', 'to'] \n",
      " ['tom', 'was', 'advised', 'by', 'mary', 'not', 'to', 'go', 'there', 'by', 'himself'] \n",
      " ['玛', '丽', '建', '议', '汤', '姆', '不', '要', '独', '自', '去']\n",
      "iter   6600, loss =   1.9232, bleu = 0.01462373\n",
      "['we', 'the', 'semester', 'television', 'on', 'the'] \n",
      " ['at', 'last', 'the', 'truth', 'was', 'revealed', 'to', 'us'] \n",
      " ['真', '相', '总', '算', '在', '我', '们', '面', '前', '揭', '晓', '了']\n",
      "iter   6700, loss =   1.8214, bleu = 0.00546798\n",
      "['the', 'always', 'is', 'is', 'a', 'in', 'in'] \n",
      " ['the', 'shopping', 'center', 'is', 'one', 'mile', 'ahead'] \n",
      " ['这', '个', '购', '物', '中', '心', '在', '前', '面', '一', '英', '里', '远', '的', '地', '方']\n",
      "iter   6800, loss =   1.9215, bleu = 0.01760003\n",
      "['you', 'side', 'you'] \n",
      " ['you', 'like', 'it', \"don't\", 'you'] \n",
      " ['你', '喜', '欢', '它', '不', '是', '吗']\n",
      "iter   6900, loss =   1.9762, bleu = 0.00657495\n",
      "['i', 'have', 'to', 'for', 'the', 'for'] \n",
      " ['i', 'asked', 'him', 'to', 'wait', 'here'] \n",
      " ['我', '请', '他', '在', '这', '里', '等']\n",
      "iter   7000, loss =   1.9106, bleu = 0.01030421\n",
      "['i', \"don't\", 'my', 'my', 'my', 'my', 'my'] \n",
      " ['i', 'was', 'unable', 'to', 'finish', 'my', 'homework'] \n",
      " ['我', '无', '法', '完', '成', '我', '的', '作', '业']\n",
      "iter   7100, loss =   1.8468, bleu = 0.00428888\n",
      "['you', 'is', 'you'] \n",
      " ['what', 'about', 'you'] \n",
      " ['您', '呢']\n",
      "iter   7200, loss =   1.9059, bleu = 0.01282687\n",
      "['you', 'taller', 'the', 'good'] \n",
      " [\"you've\", 'set', 'a', 'bad', 'example'] \n",
      " ['你', '做', '了', '个', '坏', '榜', '样']\n",
      "iter   7300, loss =   1.9261, bleu = 0.00279895\n",
      "['the', 'the', 'good', 'good', 'a'] \n",
      " ['half', 'a', 'loaf', 'is', 'better', 'than', 'none'] \n",
      " ['聊', '胜', '于', '无', '、', '有', '比', '没', '有', '好']\n",
      "iter   7400, loss =   1.8112, bleu = 0.01051878\n",
      "['are', 'you', 'have'] \n",
      " [\"don't\", 'you', 'feel', 'better'] \n",
      " ['你', '感', '觉', '好', '些', '了', '吗']\n",
      "iter   7500, loss =   1.9596, bleu = 0.02073677\n",
      "['i', 'know', 'what', 'get', 'good', 'of', 'that'] \n",
      " ['you', 'know', 'quite', 'a', 'lot', 'about', 'sumo'] \n",
      " ['你', '对', '相', '扑', '知', '道', '得', '很', '多']\n",
      "iter   7600, loss =   1.9206, bleu = 0.01184178\n",
      "['i', 'be', 'me', 'me'] \n",
      " [\"don't\", 'throw', 'out', 'this', 'magazine', 'i', \"haven't\", 'read', 'it', 'yet'] \n",
      " ['这', '本', '杂', '志', '不', '要', '扔', '我', '还', '没', '看', '呢']\n",
      "iter   7700, loss =   1.7297, bleu = 0.01083229\n",
      "['tom', 'is', 'the'] \n",
      " ['everyone', 'except', 'tom', 'ate', 'pizza'] \n",
      " ['所', '有', '人', '除', '了', '汤', '姆', '都', '吃', '了', '比', '萨']\n",
      "iter   7800, loss =   1.7818, bleu = 0.01136175\n",
      "['i', \"can't\", 'my', 'a', 'the', 'my'] \n",
      " ['i', \"can't\", 'afford', 'to', 'play', 'tennis'] \n",
      " ['我', '负', '担', '不', '起', '打', '网', '球', '的', '费', '用']\n",
      "iter   7900, loss =   1.8690, bleu = 0.00000000\n",
      "['i', 'me', 'be', 'be'] \n",
      " ['coming', \"i'll\", 'be', 'there', 'in', 'a', 'minute'] \n",
      " ['马', '上', '我', '就', '来']\n",
      "iter   8000, loss =   1.7988, bleu = 0.01458213\n",
      "['do', 'you', 'have', 'a', 'this'] \n",
      " ['do', 'you', 'have', 'any', 'free', 'time', 'this', 'weekend'] \n",
      " ['这', '个', '周', '末', '你', '有', '时', '间', '吗']\n",
      "iter   8100, loss =   1.9592, bleu = 0.00703151\n",
      "['she', 'is', 'a', 'good', 'of', 'good'] \n",
      " ['she', 'wrote', 'a', 'lot', 'of', 'poems'] \n",
      " ['她', '写', '了', '好', '多', '诗']\n",
      "iter   8200, loss =   1.8632, bleu = 0.00443760\n",
      "['i', 'have', 'the', 'tom', 'the'] \n",
      " ['i', 'found', 'the', 'new', 'magazine', 'very', 'interesting'] \n",
      " ['我', '发', '现', '这', '本', '新', '杂', '志', '非', '常', '有', '趣']\n",
      "iter   8300, loss =   1.8743, bleu = 0.01066272\n",
      "['tom', 'is', 'to', 'to', 'adjust'] \n",
      " ['tom', 'showed', 'us', 'his', 'new', 'car'] \n",
      " ['汤', '姆', '给', '我', '们', '看', '他', '的', '新', '车']\n",
      "iter   8400, loss =   1.9216, bleu = 0.01592801\n",
      "['the', 'the', 'the', 'the'] \n",
      " ['these', 'scissors', 'are', 'suitable', 'for', 'left', 'and', 'right-handed', 'people'] \n",
      " ['这', '把', '剪', '刀', '左', '右', '撇', '子', '都', '适', '用']\n",
      "iter   8500, loss =   1.7604, bleu = 0.00687688\n",
      "['i', 'reread', 'to', 'me', 'to', 'me', 'me', 'me'] \n",
      " ['you', 'ought', 'to', 'have', 'come', 'to', 'see', 'me', 'yesterday'] \n",
      " ['你', '昨', '天', '应', '该', '来', '看', '我', '的']\n",
      "iter   8600, loss =   1.8088, bleu = 0.00374417\n",
      "['tom', 'have', 'tom', 'tom', 'tom'] \n",
      " [\"you'll\", 'never', 'guess', 'what', 'tom', 'brought', 'with', 'him'] \n",
      " ['你', '永', '远', '猜', '不', '到', '汤', '姆', '带', '来', '了', '什', '么']\n",
      "iter   8700, loss =   1.8688, bleu = 0.00944403\n",
      "['tom', \"doesn't\", 'the', 'e-mail'] \n",
      " ['tom', \"wasn't\", 'too', 'happy', 'in', 'those', 'days'] \n",
      " ['汤', '姆', '这', '些', '天', '里', '不', '太', '高', '兴']\n",
      "iter   8800, loss =   1.8595, bleu = 0.01579967\n",
      "['i', 'was', 'the', 'the'] \n",
      " ['i', 'ate', 'the', 'apple'] \n",
      " ['我', '吃', '了', '这', '个', '苹', '果']\n",
      "iter   8900, loss =   1.7296, bleu = 0.01230453\n",
      "['we', 'have', 'to', 'he', 'to', 'a'] \n",
      " ['we', 'tried', 'to', 'compromise', 'with', 'them'] \n",
      " ['我', '们', '试', '著', '和', '他', '们', '妥', '协']\n",
      "iter   9000, loss =   1.7811, bleu = 0.01700041\n",
      "['i', 'is', 'me', 'not', 'i', 'me', 'me'] \n",
      " ['this', 'coffee', 'is', 'too', 'hot', 'for', 'me', 'to', 'drink'] \n",
      " ['这', '咖', '啡', '热', '得', '我', '没', '办', '法', '喝']\n",
      "iter   9100, loss =   1.8684, bleu = 0.01514378\n",
      "[\"let's\", 'years'] \n",
      " [\"let's\", 'play', 'tennis', 'in', 'the', 'afternoon'] \n",
      " ['今', '天', '下', '午', '让', '我', '们', '打', '网', '球', '吧']\n",
      "iter   9200, loss =   1.8304, bleu = 0.01107001\n",
      "['do', 'know', 'want', 'to', 'know'] \n",
      " [\"don't\", 'you', 'want', 'to', 'know', 'the', 'reason'] \n",
      " ['你', '不', '想', '知', '道', '原', '因', '吗']\n",
      "iter   9300, loss =   1.8643, bleu = 0.02164894\n",
      "['we', 'have', 'a', 'go', 'to'] \n",
      " ['we', 'had', 'to', 'go', 'there', 'together'] \n",
      " ['我', '们', '必', '须', '一', '起', '去']\n",
      "iter   9400, loss =   1.6969, bleu = 0.01187955\n",
      "['where', 'is', 'the', 'boils'] \n",
      " ['where', 'is', 'the', 'bus', 'stop', 'for', 'downtown'] \n",
      " ['到', '市', '区', '的', '公', '车', '站', '牌', '在', '哪', '里']\n",
      "iter   9500, loss =   1.8195, bleu = 0.00769671\n",
      "['i', 'tell', 'me', 'to', 'i', 'do', 'you'] \n",
      " ['they', 'told', 'me', 'that', 'i', 'would', 'feel', 'a', 'little', 'better', 'if', 'i', 'took', 'this', 'medicine'] \n",
      " ['他', '们', '告', '诉', '我', '吃', '完', '这', '个', '药', '我', '就', '会', '觉', '得', '舒', '服', '一', '点']\n",
      "iter   9600, loss =   1.7974, bleu = 0.00910699\n",
      "['i', \"can't\", 'is', 'have'] \n",
      " ['your', 'problems', \"don't\", 'concern', 'me'] \n",
      " ['你', '的', '问', '题', '和', '我', '没', '有', '关', '系']\n",
      "iter   9700, loss =   1.8444, bleu = 0.01670600\n",
      "['where', 'did', 'you', 'get', 'my', 'my', 'car'] \n",
      " ['what', 'did', 'you', 'do', 'with', 'my', 'luggage'] \n",
      " ['你', '把', '我', '的', '行', '李', '放', '到', '哪', '里', '去', '了']\n",
      "iter   9800, loss =   1.8040, bleu = 0.02194079\n",
      "['that', 'not'] \n",
      " [\"that's\", 'none', 'of', 'your', 'business'] \n",
      " ['那', '不', '关', '你', '的', '事']\n",
      "iter   9900, loss =   1.7126, bleu = 0.02316992\n",
      "['please', 'a', 'to'] \n",
      " [\"you're\", 'invited', 'too'] \n",
      " ['你', '也', '被', '邀', '请', '了']\n",
      "iter  10000, loss =   1.7439, bleu = 0.02587990\n",
      "['the', 'the', 'is', 'the'] \n",
      " ['the', 'warnings', 'were', 'ignored'] \n",
      " ['警', '告', '被', '忽', '视']\n",
      "iter  10100, loss =   1.8190, bleu = 0.01828650\n",
      "['how', 'the', 'the', 'speech'] \n",
      " ['what', 'is', 'the', 'price', 'of', 'this', 'cap'] \n",
      " ['这', '顶', '帽', '子', '多', '少', '钱']\n",
      "iter  10200, loss =   1.7033, bleu = 0.02421618\n",
      "['when', 'when', 'when', 'me', 'the'] \n",
      " ['you', 'can', 'call', 'me', 'anytime', 'you', 'like'] \n",
      " ['你', '喜', '欢', '什', '么', '时', '候', '打', '给', '我', '就', '什', '么', '时', '候', '打']\n",
      "iter  10300, loss =   1.7551, bleu = 0.01033507\n",
      "['you', \"can't\", 'be', 'you', 'car'] \n",
      " ['you', 'can', 'drive', 'a', 'car', \"can't\", 'you'] \n",
      " ['你', '会', '开', '车', '不', '是', '吗']\n",
      "iter  10400, loss =   1.8270, bleu = 0.01727151\n",
      "['this', 'a', 'people'] \n",
      " [\"there's\", 'a', 'hole', 'in', 'this', 'bucket'] \n",
      " ['这', '个', '桶', '子', '上', '有', '个', '洞']\n",
      "iter  10500, loss =   1.8056, bleu = 0.01023064\n",
      "['the', 'is', 'no', 'no'] \n",
      " ['there', 'is', 'no', 'point', 'in', 'pretending', 'to', 'be', 'sick'] \n",
      " ['装', '病', '是', '没', '用', '的']\n",
      "iter  10600, loss =   1.7365, bleu = 0.02183374\n",
      "['the', 'have', 'is'] \n",
      " ['the', 'rain', 'washed', 'away', 'the', 'soil'] \n",
      " ['土', '壤', '被', '雨', '水', '冲', '走', '了']\n",
      "iter  10700, loss =   1.8301, bleu = 0.01308094\n",
      "['my', 'the', 'is', 'my', 'my', 'of'] \n",
      " ['that', 'house', 'with', 'a', 'red', 'roof', 'is', 'my', \"uncle's\"] \n",
      " ['红', '色', '屋', '顶', '的', '房', '子', '是', '我', '叔', '叔', '的', '屋', '子']\n",
      "iter  10800, loss =   1.8332, bleu = 0.00676597\n",
      "['we', 'need', 'a', 'that', 'the'] \n",
      " ['we', 'have', 'lots', 'of', 'catching', 'up', 'to', 'do'] \n",
      " ['我', '们', '有', '很', '多', '事', '情', '要', '做']\n",
      "iter  10900, loss =   1.8093, bleu = 0.01646775\n",
      "['he', 'is', 'the', 'in', 'the', 'the'] \n",
      " ['he', 'was', 'born', 'in', 'the', '19th', 'century'] \n",
      " ['他', '生', '于', '1', '9', '世', '纪']\n",
      "iter  11000, loss =   1.7339, bleu = 0.01690017\n",
      "['what', 'do', 'you', 'think', 'of', 'it'] \n",
      " ['what', 'do', 'you', 'think', 'about', 'this', 'plan'] \n",
      " ['你', '觉', '得', '这', '个', '计', '划', '怎', '么', '样']\n",
      "iter  11100, loss =   1.7672, bleu = 0.01881577\n",
      "['we', \"can't\", 'nothing', 'to'] \n",
      " ['his', 'sudden', 'appearance', 'surprised', 'us', 'all'] \n",
      " ['他', '的', '突', '然', '出', '现', '让', '我', '们', '都', '感', '到', '惊', '讶']\n",
      "iter  11200, loss =   1.6963, bleu = 0.00878050\n",
      "['i', 'have', 'my', \"can't\"] \n",
      " ['i', 'returned', 'the', 'knife', 'which', 'i', 'had', 'borrowed'] \n",
      " ['我', '把', '我', '借', '来', '的', '刀', '还', '了']\n",
      "iter  11300, loss =   1.6665, bleu = 0.00862894\n",
      "['that', 'that', 'that', 'that', 'that', 'that'] \n",
      " ['i', 'got', 'scolded', 'severely', 'by', 'that', 'teacher'] \n",
      " ['我', '被', '那', '位', '老', '师', '严', '厉', '斥', '责', '了']\n",
      "iter  11400, loss =   1.8253, bleu = 0.02839339\n",
      "['the', 'a', 'to', 'to', 'the', 'the'] \n",
      " [\"it's\", 'quite', 'difficult', 'to', 'master', 'french', 'in', '2', 'or', '3', 'years'] \n",
      " ['两', '三', '年', '就', '想', '掌', '握', '法', '语', '是', '很', '困', '难', '的']\n",
      "iter  11500, loss =   1.7718, bleu = 0.01550698\n",
      "[\"i'm\", 'your', 'to'] \n",
      " [\"i'm\", 'glad', 'you', 'were', 'right'] \n",
      " ['很', '高', '兴', '你', '是', '对', '的']\n",
      "iter  11600, loss =   1.7889, bleu = 0.01256478\n",
      "['i', \"can't\", 'have', 'to', 'that', 'i', 'i', 'a', 'to', 'to', 'the'] \n",
      " ['i', 'can', 'usually', 'tell', 'when', 'someone', 'is', 'hiding', 'something', 'from', 'me'] \n",
      " ['我', '常', '常', '能', '分', '辨', '出', '来', '一', '个', '人', '是', '不', '是', '瞒', '着', '我', '什', '么', '事']\n",
      "iter  11700, loss =   1.7694, bleu = 0.01041088\n",
      "['i', \"can't\", 'see', 'him'] \n",
      " ['i', \"don't\", 'see', 'much', 'of', 'him'] \n",
      " ['我', '不', '常', '见', '到', '他']\n",
      "iter  11800, loss =   1.7417, bleu = 0.01725968\n",
      "['we', 'had', 'to', 'the'] \n",
      " ['we', 'talked', 'about', 'it', 'all', 'night'] \n",
      " ['我', '们', '谈', '这', '事', '谈', '了', '一', '整', '夜']\n",
      "iter  11900, loss =   1.7706, bleu = 0.01535959\n",
      "['this', 'not', 'to', 'not'] \n",
      " ['the', 'picture', 'is', 'nice'] \n",
      " ['这', '画', '不', '错']\n",
      "iter  12000, loss =   1.6474, bleu = 0.02339032\n",
      "['he', 'is', 'a', 'to'] \n",
      " ['he', 'carried', 'six', 'boxes', 'at', 'a', 'time'] \n",
      " ['他', '一', '次', '搬', '6', '个', '箱', '子']\n",
      "iter  12100, loss =   1.6484, bleu = 0.01785884\n",
      "['i', 'not', 'grapefruit', 'to', 'i', 'she', 'was', 'i', 'to'] \n",
      " [\"i'm\", 'not', 'saying', 'that', 'what', 'she', 'did', 'was', 'right'] \n",
      " ['我', '的', '意', '思', '不', '是', '她', '做', '得', '对']\n"
     ]
    }
   ],
   "source": [
    "train_transformer(net_zh2en,\n",
    "                  data_iter=dataloader_zh2en,\n",
    "                  base_lr=1e1,\n",
    "                  warmup_steps=3000,\n",
    "                  num_iters=100000,\n",
    "                  source_vocab=dataset_zh2en.vocab_zh,\n",
    "                  target_vocab=dataset_zh2en.vocab_en,\n",
    "                  device=device)\n",
    "# train_transformer(net_en2zh,\n",
    "#                   data_iter=dataloader_en2zh,\n",
    "#                   base_lr=1e1,\n",
    "#                   warmup_steps=1000,\n",
    "#                   num_iters=10000,\n",
    "#                   source_vocab=dataset_en2zh.vocab_en,\n",
    "#                   target_vocab=dataset_en2zh.vocab_zh,\n",
    "#                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a86a7-ce83-4a51-911d-88c42250ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_zh2en, f'Transformer_N={net_zh2en.N}_head={net_zh2en.num_head}_d={net_zh2en.d_model}_ffn={net_zh2en.ffn_hidden_size}_dropout={net_zh2en.dropout}_zh2en.pth')\n",
    "# torch.save(net_en2zh.state_dict(), f'Transformer_N={net.N}_head={net.num_head}_d={net.d_model}_ffn={net.ffn_hidden_size}_dropout={net.dropout}_en2zh.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2b0a3-d878-407f-9e4d-57bf548a5d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_zh2en = torch.load(f'Transformer_N={net_zh2en.N}_head={net_zh2en.num_head}_d={net_zh2en.d_model}_ffn={net_zh2en.ffn_hidden_size}_dropout={net_zh2en.dropout}_zh2en.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c136c8-a790-4e96-80dc-4859a9fc68dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_input = ['我可以去', \n",
    "              '早上好', \n",
    "              '你今天过得怎么样', \n",
    "              '什么时候去吃晚饭',\n",
    "              '不错',\n",
    "              '学英语',\n",
    "              '汤姆很喜欢吃东西',\n",
    "              '现在几点了',\n",
    "              '今天下雨',\n",
    "              '明天下雨',\n",
    "              '你打算怎么去学校']\n",
    "predict(net_zh2en,\n",
    "        src_sentense=test_input,\n",
    "        src_vocab=dataset_zh2en.vocab_zh,\n",
    "        trg_vocab=dataset_zh2en.vocab_en,\n",
    "        num_steps=20,\n",
    "        source='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f97b88-1f8c-45b6-bad2-d778a8736ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
