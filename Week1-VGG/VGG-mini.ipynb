{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983dc964-6365-4677-8197-806fb71fe83f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VGG简单复现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb4357-613c-4687-b266-2d3dfcf88088",
   "metadata": {},
   "source": [
    "## 导入所需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18877a22-edf4-4bbe-9152-2bb4123fdd1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ebaee-b88b-4c45-85bb-8f5c359839a1",
   "metadata": {},
   "source": [
    "### 在命令行输入`tensorboard --logdir=runs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4983f08c-f416-4ac9-b564-333de7fd94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 百度来的，不然下载不动。。\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e06568-7add-4518-a28e-6d175169d103",
   "metadata": {},
   "source": [
    "## 计算CIFAR10数据集RGB三个通道的均值和方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088d1795-2f42-4dc0-90c4-a1b22ff42c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n",
      "[0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True)\n",
    "print(cifar_train.data.shape) # (50000, 32, 32, 3)\n",
    "cifardata = cifar_train.data / 255\n",
    "mean = cifardata.mean(axis=(0, 1, 2))\n",
    "std = cifardata.std(axis=(0, 1, 2))\n",
    "print(mean, std) # [0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74791bf-2fb3-497a-81e6-1533a77e32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.491, 0.482, 0.446], [0.247, 0.243, 0.261]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b77ff-6799-45bc-a503-d9e58d78e9e1",
   "metadata": {},
   "source": [
    "## 定义训练数据集\n",
    "\n",
    "#### 预处理顺序：将正方形图片裁剪成长宽都为$S$，再从中裁剪出64\\*64的一块（CIFAR10的图片大小为32\\*32，适度缩小网络以适应数据集），随机水平翻转，随机扰动色彩，转化为张量，归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f78c5ac-c033-4389-9b71-606cab7cd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, S):\n",
    "        super().__init__()\n",
    "        self.dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"../data\", train=True, download=True)\n",
    "        \n",
    "        self.S = S\n",
    "        \n",
    "        self.trans = [transforms.RandomCrop(64),\n",
    "                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                      transforms.ColorJitter(brightness=0.1,\n",
    "                                             contrast=0.1,\n",
    "                                             saturation=0.1,\n",
    "                                             hue=0),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize(mean, std, inplace=True)]\n",
    "        self.trans = transforms.Compose(self.trans)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(self.S, int):\n",
    "            resize = transforms.Resize(max(64, self.S))\n",
    "        elif isinstance(self.S, list):\n",
    "            assert(len(self.S) == 2)\n",
    "            resize = transforms.Resize(\n",
    "                random.randint(self.S[0], self.S[1]))\n",
    "\n",
    "        return (self.trans(resize(self.dataset[index][0])),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10ce3e-696a-4dfe-ada8-80ca6f13ec10",
   "metadata": {},
   "source": [
    "## 定义测试数据集\n",
    "\n",
    "#### 预处理顺序：将图片尺寸缩放为$Q$，水平翻转（可选），转化为张量，归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba014a6-ec42-4bde-bd30-2076667af194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, Q, horizontal_flip=False):\n",
    "        super().__init__()\n",
    "        self.dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"../data\", train=False, download=True)\n",
    "        \n",
    "        self.Q = Q\n",
    "        \n",
    "        self.trans = [transforms.ToTensor(),\n",
    "                      transforms.Normalize(mean, std, inplace=True)]\n",
    "        if horizontal_flip:\n",
    "            self.trans.insert(0, transforms.RandomHorizontalFlip(p=1))\n",
    "        self.trans = transforms.Compose(self.trans)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert(isinstance(self.Q, int))\n",
    "        resize = transforms.Resize(max(64, self.Q))\n",
    "            \n",
    "        return (self.trans(resize(self.dataset[index][0])),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6ba03-2dc8-4456-94cb-e33a713b8003",
   "metadata": {},
   "source": [
    "## 定义VGG块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c237404-4b93-4f82-b1c7-634c69255afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_3x3, conv_1x1=False, batch_norm=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(in_channels, out_channels,\n",
    "                             kernel_size=3, stride=1, padding=1),\n",
    "                   nn.BatchNorm2d(out_channels),\n",
    "                   nn.ReLU(inplace=True)] if batch_norm else \\\n",
    "                  [nn.Conv2d(in_channels, out_channels,\n",
    "                             kernel_size=3, stride=1, padding=1),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "        if num_3x3 > 1:\n",
    "            for i in range(1, num_3x3):\n",
    "                layers += [nn.Conv2d(out_channels, out_channels,\n",
    "                                     kernel_size=3, stride=1, padding=1),\n",
    "                           nn.BatchNorm2d(out_channels),\n",
    "                           nn.ReLU(inplace=True)] if batch_norm else \\\n",
    "                          [nn.Conv2d(out_channels, out_channels,\n",
    "                                     kernel_size=3, stride=1, padding=1),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "        if conv_1x1:\n",
    "            layers += [nn.Conv2d(out_channels, out_channels,\n",
    "                                 kernel_size=1, stride=1, padding=0),\n",
    "                       nn.BatchNorm2d(out_channels),\n",
    "                       nn.ReLU(inplace=True)] if batch_norm else \\\n",
    "                      [nn.Conv2d(out_channels, out_channels,\n",
    "                                 kernel_size=1, stride=1, padding=0),\n",
    "                       nn.ReLU(inplace=True)]\n",
    "        layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.block(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da19a2-e7f2-4ddf-9973-b2576d04f983",
   "metadata": {},
   "source": [
    "## 定义VGG网络（缩小版）\n",
    "\n",
    "#### 输入尺寸为64\\*64，输出尺寸为10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f85038-4d69-4c07-aecb-3f5a49f9a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_mini(nn.Module):\n",
    "    def __init__(self, configuration, batch_norm=False, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.configurations = {\n",
    "            'A': [[3,   8,  1, False],\n",
    "                  [8,   16, 1, False],\n",
    "                  [16,  32, 2, False],\n",
    "                  [32,  64, 2, False],\n",
    "                  [64,  64, 2, False]],\n",
    "            \n",
    "            'B': [[3,   8,  2, False],\n",
    "                  [8,   16, 2, False],\n",
    "                  [16,  32, 2, False],\n",
    "                  [32,  64, 2, False],\n",
    "                  [64,  64, 2, False]],\n",
    "            \n",
    "            'C': [[3,   8,  2, False],\n",
    "                  [8,   16, 2, False],\n",
    "                  [16,  32, 2, True],\n",
    "                  [32,  64, 2, True],\n",
    "                  [64,  64, 2, True]],\n",
    "            \n",
    "            'D': [[3,   8,  2, False],\n",
    "                  [8,   16, 2, False],\n",
    "                  [16,  32, 3, False],\n",
    "                  [32,  64, 3, False],\n",
    "                  [64,  64, 3, False]],\n",
    "            \n",
    "            'E': [[3,   8,  2, False],\n",
    "                  [8,   16, 2, False],\n",
    "                  [16,  32, 4, False],\n",
    "                  [32,  64, 4, False],\n",
    "                  [64,  64, 4, False]]\n",
    "        }\n",
    "        self.configuration = configuration\n",
    "        self.batch_norm = batch_norm\n",
    "        self.blocks = []\n",
    "        for arg_list in self.configurations[self.configuration]:\n",
    "            self.blocks.append(VGG_block(*arg_list, self.batch_norm))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "        \n",
    "        # 用全卷积代替全连接\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Conv2d(64, 512, kernel_size=2),\n",
    "            nn.Dropout2d(p=dropout, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=1),\n",
    "            nn.Dropout2d(p=dropout, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 10, kernel_size=1),\n",
    "            # 把后两个空间维度（H、W）合并成一个\n",
    "            nn.Flatten(start_dim=2, end_dim=-1)\n",
    "        )\n",
    "    \n",
    "    def print_num_params(self):\n",
    "        \"\"\"打印网络参数数量\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'{total_params:,} total parameters.')\n",
    "        total_trainable_params = sum(\n",
    "            p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f'{total_trainable_params:,} trainable parameters.')\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = self.blocks(X)\n",
    "        output = self.FC(output)\n",
    "        # 这里为了实现简单，把空间平均的操作放在softmax前面了，原文是先softmax再空间平均\n",
    "        # 输出维度为 (`batch_size`, 10)\n",
    "        return output.mean(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97d1cb-7b55-4529-8cf7-0a5fdd2d3e6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 根据给定的图片缩放策略和$S$，计算$Q$。\n",
    "\n",
    "#### 4.1 Single Scale Evaluation\n",
    "-   $S$值固定时，$Q=S$。\n",
    "-   $S$值可变（$S\\in[S_{min},S_{max}]$）时，$Q=0.5(S_{min}+S_{max})$。\n",
    "\n",
    "#### 4.2 Multi-Scale Evaluation\n",
    "-   $S$值固定时，$Q=\\{S-32,\\ S,\\ S+32\\}$\n",
    "-   $S$值可变时，$Q=\\{S_{min}, \\ 0.5(S_{min}+S_{max}), \\ S_{max}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b442c89c-17ec-45b2-9c39-63419f2dfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_scale_eval_SQ(S):\n",
    "    if isinstance(S, int):\n",
    "        Q = S\n",
    "    elif isinstance(S, list):\n",
    "        assert(len(S) == 2)\n",
    "        Q = int(0.5*(S[0] + S[1]))\n",
    "    return S, Q\n",
    "\n",
    "def multi_scale_eval_SQ(S):\n",
    "    if isinstance(S, int):\n",
    "        # 因为输入图片本来就不大，改成 +-8 了\n",
    "        Q = [S-8, S, S+8]\n",
    "    elif isinstance(S, list):\n",
    "        assert(len(S) == 2)\n",
    "        Q = [S[0], int(0.5*(S[0] + S[1])), S[1]]\n",
    "    return S, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a93a60-0a99-4fef-9795-98aa162e76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_S_and_Q(S, single_scale_eval):\n",
    "    if single_scale_eval:\n",
    "        return single_scale_eval_SQ(S)\n",
    "    else:\n",
    "        return multi_scale_eval_SQ(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05842afc-9f8c-4a99-a46d-2329aaa00947",
   "metadata": {},
   "source": [
    "## 定义性能评估类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2c21ea-7af0-439b-9857-eab6db51cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluater:\n",
    "    def __init__(self, S, batch_size, mode='single'):\n",
    "        self.single_scale = mode=='single'\n",
    "        self.S, self.Q = get_S_and_Q(S, self.single_scale)\n",
    "        if self.single_scale:\n",
    "            # 未开启水平翻转和开启水平翻转两个数据集\n",
    "            self.datasets = [TestDataset(self.Q, False), TestDataset(self.Q, True)]\n",
    "        else:\n",
    "            self.datasets = []\n",
    "            # 对Q中的每个尺寸都使用未开启水平翻转和开启水平翻转两个数据集，共6个数据集\n",
    "            for q in self.Q:\n",
    "                self.datasets += [TestDataset(q, False), TestDataset(q, True)]\n",
    "        # 每个数据集创建一个dataloader\n",
    "        self.dataloaders = [data.DataLoader(dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=8) for dataset in self.datasets]\n",
    "    def evaluate(self, net, criterion):\n",
    "        net.eval()\n",
    "        loss, accuracy = [], []\n",
    "        outputs = {}\n",
    "        with torch.no_grad():\n",
    "            # 对每个dataloader都过一遍\n",
    "            for dataloader in self.dataloaders:\n",
    "                for i, (input, _) in enumerate(dataloader):\n",
    "                    input = input.to(device)\n",
    "                    output = net(input)\n",
    "                    # 把网络的输出存储起来\n",
    "                    try:\n",
    "                        outputs[i] += F.softmax(output, dim=1)\n",
    "                    except KeyError:\n",
    "                        outputs[i] = F.softmax(output, dim=1)\n",
    "            # 网络的输出收集完毕后，用第一个dataloader的target计算精度和loss\n",
    "            for i, (_, target) in enumerate(self.dataloaders[0]):\n",
    "                target = target.to(device)\n",
    "                loss.append(criterion(outputs[i] / len(self.datasets), target))\n",
    "                accuracy.append((outputs[i].argmax(dim=1)==target).sum() / target.shape[0])\n",
    "        # 计算在所有batch上loss和accuracy的均值\n",
    "        loss = torch.tensor(loss).mean().item()\n",
    "        accuracy = torch.tensor(accuracy).mean().item()\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91b095-dba0-4cf0-aab7-31da4dcff81a",
   "metadata": {},
   "source": [
    "## 访问优化器的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c1e35f-5a9c-4a87-83e6-52d600ea556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return (optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f2946-cdb8-4828-87ab-c9d3ba1a1e1f",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd7d25b-fd3f-49c9-aac9-c60813245a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VGG(net,\n",
    "              batch_size,\n",
    "              num_epochs,\n",
    "              lr,\n",
    "              evaluater,\n",
    "              S=[64, 128],\n",
    "              weight_decay=5e-4):\n",
    "\n",
    "    writer = SummaryWriter(f'runs/VGG-mini-{net.configuration}'+('-batchnorm' if net.batch_norm else ''))\n",
    "    cifar_train = TrainDataset(S)\n",
    "    train_iter = data.DataLoader(cifar_train, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=8)\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "    net.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           factor=0.1,\n",
    "                                                           patience=3,\n",
    "                                                           threshold=1e-3,\n",
    "                                                           verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        tic = time.time()\n",
    "        # 训练损失之和，训练准确率之和，范例数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (input, target) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(loss * input.shape[0],\n",
    "                           d2l.accuracy(output, target),\n",
    "                           input.shape[0])\n",
    "            timer.stop()\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            \n",
    "            # if (i+1) % (num_batches//20) == 0:\n",
    "            #     print(f\"loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n",
    "        valid_loss, valid_acc = evaluater.evaluate(net, criterion)\n",
    "        writer.add_scalar('train/loss', train_loss, global_step=epoch+1)\n",
    "        writer.add_scalar('train/accuracy', train_acc, global_step=epoch+1)\n",
    "        writer.add_scalar('valid/loss', valid_loss, global_step=epoch+1)\n",
    "        writer.add_scalar('valid/accuracy', valid_acc, global_step=epoch+1)\n",
    "        writer.add_scalar('learning rate', get_lr(optimizer), global_step=epoch+1)\n",
    "        scheduler.step(valid_loss)\n",
    "        toc = time.time()\n",
    "        print(f\"epoch {epoch+1:2d}, train loss: {train_loss:.4f}, train accuracy: {train_acc:.4f}, \\\n",
    "valid loss: {valid_loss:.4f}, valid accuracy: {valid_acc:.4f}, time: {toc-tic:.4f}\")\n",
    "    valid_loss, valid_acc = evaluater.evaluate(net, criterion)\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'valid loss {valid_loss:.3f}, valid acc {valid_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2aa608-2744-4202-a779-69fcf9e6290e",
   "metadata": {},
   "source": [
    "## 创建VGG网络实例，查看网络参数数量，创建evaluater实例，查看网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3985d5-9f22-4d65-aa40-97e7c38ae9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543,930 total parameters.\n",
      "543,930 trainable parameters.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG_mini(\n",
       "  (blocks): Sequential(\n",
       "    (0): VGG_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (1): VGG_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (2): VGG_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (3): VGG_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (4): VGG_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (FC): Sequential(\n",
       "    (0): Conv2d(64, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.5, inplace=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): Dropout2d(p=0.5, inplace=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): Flatten(start_dim=2, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VGG_mini(configuration='A',\n",
    "               batch_norm=False,\n",
    "               dropout=0.5).to(device)\n",
    "net.print_num_params()\n",
    "S = [64, 128]\n",
    "mode = 'multi'\n",
    "evaluater = Evaluater(S, mode=mode, batch_size=256)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8f098-a327-4d3c-8e85-4c8f71352fee",
   "metadata": {},
   "source": [
    "### 在训练之前测试一下，精度应该在10%左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512f6226-c6d3-481c-be69-03a63a735622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3025765419006348, 0.09912109375)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluater.evaluate(net, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab290d0a-8b16-4e40-9a9d-cc0da6deb154",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 训练VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b518b37b-081b-47f0-92f5-112d3d34ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "epoch  1, train loss: 2.3268, train accuracy: 0.1846, valid loss: 2.2290, valid accuracy: 0.3321, time: 12.4156\n",
      "epoch  2, train loss: 1.8299, train accuracy: 0.3136, valid loss: 2.1952, valid accuracy: 0.4158, time: 12.7650\n",
      "epoch  3, train loss: 1.7002, train accuracy: 0.3664, valid loss: 2.1736, valid accuracy: 0.4367, time: 12.5378\n",
      "epoch  4, train loss: 1.6095, train accuracy: 0.4079, valid loss: 2.1324, valid accuracy: 0.4966, time: 12.7743\n",
      "epoch  5, train loss: 1.5532, train accuracy: 0.4349, valid loss: 2.1350, valid accuracy: 0.5127, time: 12.7556\n",
      "epoch  6, train loss: 1.4987, train accuracy: 0.4547, valid loss: 2.0994, valid accuracy: 0.5097, time: 12.9219\n",
      "epoch  7, train loss: 1.4517, train accuracy: 0.4731, valid loss: 2.1000, valid accuracy: 0.5331, time: 12.9392\n",
      "epoch  8, train loss: 1.4067, train accuracy: 0.4964, valid loss: 2.0801, valid accuracy: 0.5525, time: 12.8054\n",
      "epoch  9, train loss: 1.3816, train accuracy: 0.5025, valid loss: 2.0594, valid accuracy: 0.5840, time: 12.8715\n",
      "epoch 10, train loss: 1.3314, train accuracy: 0.5237, valid loss: 2.0592, valid accuracy: 0.5981, time: 12.9957\n",
      "epoch 11, train loss: 1.2955, train accuracy: 0.5410, valid loss: 2.0365, valid accuracy: 0.6124, time: 12.9510\n",
      "epoch 12, train loss: 1.2655, train accuracy: 0.5503, valid loss: 2.0335, valid accuracy: 0.6242, time: 13.0397\n",
      "epoch 13, train loss: 1.2362, train accuracy: 0.5621, valid loss: 1.9965, valid accuracy: 0.6590, time: 13.2563\n",
      "epoch 14, train loss: 1.2084, train accuracy: 0.5727, valid loss: 1.9819, valid accuracy: 0.6428, time: 12.9247\n",
      "epoch 15, train loss: 1.1836, train accuracy: 0.5831, valid loss: 1.9829, valid accuracy: 0.6826, time: 12.9215\n",
      "epoch 16, train loss: 1.1546, train accuracy: 0.5944, valid loss: 1.9670, valid accuracy: 0.6919, time: 12.5273\n",
      "epoch 17, train loss: 1.1487, train accuracy: 0.6004, valid loss: 1.9770, valid accuracy: 0.6918, time: 12.6167\n",
      "epoch 18, train loss: 1.1049, train accuracy: 0.6119, valid loss: 1.9537, valid accuracy: 0.7015, time: 13.1441\n",
      "epoch 19, train loss: 1.0986, train accuracy: 0.6175, valid loss: 1.9481, valid accuracy: 0.7079, time: 12.8500\n",
      "epoch 20, train loss: 1.0812, train accuracy: 0.6247, valid loss: 1.9707, valid accuracy: 0.6750, time: 12.6785\n",
      "epoch 21, train loss: 1.0535, train accuracy: 0.6338, valid loss: 1.9350, valid accuracy: 0.7164, time: 12.8207\n",
      "epoch 22, train loss: 1.0355, train accuracy: 0.6408, valid loss: 1.9340, valid accuracy: 0.7152, time: 12.8491\n",
      "epoch 23, train loss: 1.0273, train accuracy: 0.6416, valid loss: 1.9145, valid accuracy: 0.7237, time: 12.7739\n",
      "epoch 24, train loss: 1.0130, train accuracy: 0.6489, valid loss: 1.9022, valid accuracy: 0.7378, time: 12.6823\n",
      "epoch 25, train loss: 1.0082, train accuracy: 0.6515, valid loss: 1.9301, valid accuracy: 0.7062, time: 13.1335\n",
      "epoch 26, train loss: 0.9747, train accuracy: 0.6644, valid loss: 1.9011, valid accuracy: 0.7374, time: 12.8046\n",
      "epoch 27, train loss: 0.9718, train accuracy: 0.6658, valid loss: 1.8933, valid accuracy: 0.7592, time: 12.8000\n",
      "epoch 28, train loss: 0.9667, train accuracy: 0.6670, valid loss: 1.8941, valid accuracy: 0.7583, time: 12.6042\n",
      "epoch 29, train loss: 0.9408, train accuracy: 0.6767, valid loss: 1.8800, valid accuracy: 0.7581, time: 12.7910\n",
      "epoch 30, train loss: 0.9326, train accuracy: 0.6792, valid loss: 1.8862, valid accuracy: 0.7416, time: 13.0031\n",
      "epoch 31, train loss: 0.9328, train accuracy: 0.6797, valid loss: 1.8813, valid accuracy: 0.7537, time: 12.8931\n",
      "epoch 32, train loss: 0.9234, train accuracy: 0.6827, valid loss: 1.8727, valid accuracy: 0.7703, time: 13.1617\n",
      "epoch 33, train loss: 0.9258, train accuracy: 0.6824, valid loss: 1.8571, valid accuracy: 0.7809, time: 12.5929\n",
      "epoch 34, train loss: 0.9070, train accuracy: 0.6875, valid loss: 1.8667, valid accuracy: 0.7677, time: 12.6765\n",
      "epoch 35, train loss: 0.8923, train accuracy: 0.6943, valid loss: 1.8640, valid accuracy: 0.7543, time: 12.6874\n",
      "epoch 36, train loss: 0.8989, train accuracy: 0.6930, valid loss: 1.8673, valid accuracy: 0.7533, time: 13.3287\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "epoch 37, train loss: 0.8839, train accuracy: 0.6969, valid loss: 1.8588, valid accuracy: 0.7709, time: 13.1960\n",
      "epoch 38, train loss: 0.8091, train accuracy: 0.7230, valid loss: 1.8221, valid accuracy: 0.8040, time: 13.2163\n",
      "epoch 39, train loss: 0.7885, train accuracy: 0.7310, valid loss: 1.8161, valid accuracy: 0.8089, time: 13.0318\n",
      "epoch 40, train loss: 0.7825, train accuracy: 0.7319, valid loss: 1.8073, valid accuracy: 0.8116, time: 12.6186\n",
      "epoch 41, train loss: 0.7764, train accuracy: 0.7319, valid loss: 1.8090, valid accuracy: 0.8160, time: 12.6354\n",
      "epoch 42, train loss: 0.7713, train accuracy: 0.7365, valid loss: 1.8066, valid accuracy: 0.8170, time: 13.0018\n",
      "epoch 43, train loss: 0.7684, train accuracy: 0.7374, valid loss: 1.8061, valid accuracy: 0.8183, time: 12.4939\n",
      "epoch 44, train loss: 0.7659, train accuracy: 0.7356, valid loss: 1.8036, valid accuracy: 0.8170, time: 12.7258\n",
      "epoch 45, train loss: 0.7614, train accuracy: 0.7384, valid loss: 1.8086, valid accuracy: 0.8164, time: 12.7603\n",
      "epoch 46, train loss: 0.7617, train accuracy: 0.7368, valid loss: 1.8003, valid accuracy: 0.8178, time: 12.5228\n",
      "epoch 47, train loss: 0.7583, train accuracy: 0.7408, valid loss: 1.7988, valid accuracy: 0.8237, time: 12.6682\n",
      "epoch 48, train loss: 0.7495, train accuracy: 0.7439, valid loss: 1.7974, valid accuracy: 0.8191, time: 12.9722\n",
      "epoch 49, train loss: 0.7538, train accuracy: 0.7421, valid loss: 1.7947, valid accuracy: 0.8236, time: 12.9438\n",
      "epoch 50, train loss: 0.7490, train accuracy: 0.7450, valid loss: 1.8028, valid accuracy: 0.8155, time: 12.7167\n",
      "epoch 51, train loss: 0.7523, train accuracy: 0.7407, valid loss: 1.7966, valid accuracy: 0.8207, time: 12.7412\n",
      "epoch 52, train loss: 0.7427, train accuracy: 0.7472, valid loss: 1.7948, valid accuracy: 0.8184, time: 13.1001\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch 53, train loss: 0.7468, train accuracy: 0.7437, valid loss: 1.7944, valid accuracy: 0.8211, time: 12.6582\n",
      "epoch 54, train loss: 0.7304, train accuracy: 0.7481, valid loss: 1.7913, valid accuracy: 0.8269, time: 12.7301\n",
      "epoch 55, train loss: 0.7304, train accuracy: 0.7496, valid loss: 1.7912, valid accuracy: 0.8275, time: 12.8311\n",
      "epoch 56, train loss: 0.7264, train accuracy: 0.7492, valid loss: 1.7905, valid accuracy: 0.8251, time: 12.5682\n",
      "epoch 57, train loss: 0.7327, train accuracy: 0.7466, valid loss: 1.7912, valid accuracy: 0.8259, time: 12.8563\n",
      "epoch 58, train loss: 0.7317, train accuracy: 0.7507, valid loss: 1.7888, valid accuracy: 0.8272, time: 12.7621\n",
      "epoch 59, train loss: 0.7272, train accuracy: 0.7505, valid loss: 1.7888, valid accuracy: 0.8258, time: 12.7092\n",
      "epoch 60, train loss: 0.7276, train accuracy: 0.7520, valid loss: 1.7877, valid accuracy: 0.8262, time: 12.9197\n",
      "epoch 61, train loss: 0.7297, train accuracy: 0.7489, valid loss: 1.7891, valid accuracy: 0.8259, time: 13.0329\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 62, train loss: 0.7290, train accuracy: 0.7492, valid loss: 1.7885, valid accuracy: 0.8260, time: 13.3065\n",
      "epoch 63, train loss: 0.7266, train accuracy: 0.7520, valid loss: 1.7891, valid accuracy: 0.8274, time: 12.8343\n",
      "epoch 64, train loss: 0.7258, train accuracy: 0.7524, valid loss: 1.7891, valid accuracy: 0.8278, time: 12.6346\n",
      "epoch 65, train loss: 0.7257, train accuracy: 0.7499, valid loss: 1.7891, valid accuracy: 0.8272, time: 12.6171\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "epoch 66, train loss: 0.7194, train accuracy: 0.7526, valid loss: 1.7889, valid accuracy: 0.8278, time: 12.6353\n",
      "epoch 67, train loss: 0.7330, train accuracy: 0.7475, valid loss: 1.7889, valid accuracy: 0.8276, time: 12.8861\n",
      "epoch 68, train loss: 0.7309, train accuracy: 0.7491, valid loss: 1.7889, valid accuracy: 0.8276, time: 12.6597\n",
      "epoch 69, train loss: 0.7222, train accuracy: 0.7515, valid loss: 1.7889, valid accuracy: 0.8276, time: 12.8857\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "epoch 70, train loss: 0.7301, train accuracy: 0.7490, valid loss: 1.7889, valid accuracy: 0.8278, time: 12.4829\n",
      "train loss 0.730, train acc 0.749, valid loss 1.789, valid acc 0.828\n",
      "22518.9 examples/sec on cuda\n"
     ]
    }
   ],
   "source": [
    "train_VGG(net,\n",
    "          batch_size=256,\n",
    "          num_epochs=70,\n",
    "          lr=1e-3,\n",
    "          evaluater=evaluater,\n",
    "          S=[64, 128],\n",
    "          weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b99ec-ada4-41e5-bd75-0b4e44c8b831",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 训练完再评估一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4de949-60f9-41f2-af06-4cc0879a0abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7888615131378174, 0.827832043170929)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluater.evaluate(net, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109ed09-75ef-42c9-a84b-16d1ebe8f2b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47073b8f-777c-4293-bdd9-0f06b3ddcbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, f'VGG-mini-{net.configuration}' + ('-batchnorm.pth' if net.batch_norm else '.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
