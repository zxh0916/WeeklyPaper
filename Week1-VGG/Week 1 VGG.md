# Week 1 VGG论文笔记+复现

论文地址：

-   本地（做了标记）[Very Deep Convolutional Networks For Large-Scale Image Recognition](../papers/VGG.pdf)
-   原地址[Very Deep Convolutional Networks For Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf)

上期回顾：[Week 0 AlexNet](../Week0-Alexnet\Week 0 Alexnet.md)



ILCR 2015

Karen Simonyan & Andrew Zisserman

**V**isual **G**eometry **G**roup, Department of Engineering Science, University of Oxford

## Abstract

​		主要介绍了图片分类任务中深度对卷积神经网络精度的影响。**核心思想：深度！！**把已有结构加深到16-19层之后能获得显著的性能提升。

​		不仅在ImageNet Challenge 2014分别在图像定位和分类任务中获得了第一名和第二名，还能在其他数据集中取得SOTA的效果。



## 1 Introduction

​		ILSVRC是几代图片分类系统的测试平台。

​		当时在ILSVRC上效果最好的解决方案：

-   把第一个卷积层的卷积核变小

-   对整个图像多个尺度做密集的训练和预测

​		本文将目光转向CNN设计的另外一个重要的方面：深度。为了测试深度对性能的影响，作者固定其他超参数，通过添加卷积层来逐渐增加网络的深度。由于使用的卷积核窗口较小（3\*3），添加卷积层仅对特征图的大小有少许缩减，所以叠加大量卷积层来构建较深的CNN得以成为一种可能。

​		结果是，CNN得到了大幅度的性能提升，不仅在ILSVRC 2014中达到了图片分类和定位的SOTA，这种CNN架构还能**在不需要微调的情况下作为一个模块来使用**（将CNN抽取出来的特征作为输入，送进如SVM的线性分类器）从而应用于其他图片分类的数据集！

​		2、3、4、5节分别是CNN配置、训练和验证、几种配置的横向比较和总结。



## 2 ConvNet Configurations

​		为了确保性能测试的公平性，所有的配置都是由同样的原则设计的。2.1节先介绍通用结构，2.2节再介绍各个配置的细节设计，最后2.3节比较了本文和先前的CNN设计。

### 2.1 Architecture

​		介绍了构成本文不同配置的基本模块：

#### 	输入

​		224\*224 RGB图片。

#### 	预处理

​		在训练集上分别计算RGB三个通道的均值，对所有图片的三个通道分别中心化。

#### 	卷积层

​		3\*3卷积层（能捕捉图片中上下左右和中心特征的最小尺寸），步长为1，padding为1（same）。

​		1\*1卷积层（可看做对特征图中通道维度的线性变换），步长为1，padding为0（same）。

#### 	池化层

​		2\*2最大池化层，步长为2（将特征图高宽减半）。

#### 	全连接层

​		三个全连接层：flattened卷积层输出->4096，4096->4096，4096->1000（1000类softmax）。

​		全连接层的设置对所有配置均相同。

#### Dropout

​		p=0.5，应用于前两个全连接层。

#### 	激活函数

​		ReLU，每个卷积层和全连接层后面都跟一个。

#### 	LRN？~~狗都不用！~~

​		不仅无法提升性能，还会增加显存占用，延长计算时间。



### 2.2 Configurations

![VGG-Table 1](..\pictures\VGG-Table 1.png)

![VGG-Table 2](..\pictures\VGG-Table 2.png)

![vgg](..\pictures\vgg.svg)



### 2.3 Discussion

​		VGG全部使用3\*3卷积层，而不是其他ILSVRC winner使用的11\*11或7\*7卷积层。两个3\*3卷积层的感受野和一个5\*5卷积层的感受野相同，三个3\*3卷积层的感受野和一个7\*7卷积层的感受野相同。但相比之下，首先，前者包含了2/3个激活函数，而不是一个，这使得网络的表达能力更加强大；其次，使用3\*3卷积层能使得网络参数大幅降低。1\*1卷积层（2014年在NiN中被用到）是用来在不改变卷积层感受野的情况下增加非线性性的。

​		相比之下，GoogLeNet的设计结构更加复杂，并且为了减少计算量，其第一个卷积层从原图到特征图的尺寸缩减过于迅速了。



## 3 Classification Framework

### 3.1 Training

​		损失函数：交叉熵

​		优化算法：SGD

​		lr_scheduler：Reduce LR On Plateau

#### 	超参数

​		momentum=0.9

​		weight_decay=5e-4

​		learning_rate=1e-2

​		dropout=0.5（前两个全连接层）

​		batch_size=256

​		num_epochs=74

​		尽管参数较多，深度较大，但网络收敛还是较快，是由于：1. 较深的网络搭配上较小的卷积核，对网络有隐式的正则化效果；2. 特定层做了权重初始化。

#### 	权重初始化

​		为了应对网络过深之后，随机初始化会导致模型不稳定的问题，作者先用随机初始化训练较浅的模型A，而后在训练其他配置的模型时，以预训练的网络A的参数作为对应层的初始参数，并用同样的学习率进行训练。对于其他配置较配置A新增的层，使用高斯分布$\mathcal{N}\sim (0, 0.1^2)$来初始化权重。偏置值全部初始化为0。

#### 	数据增强

​		在训练时，从缩放过后的图片上随机裁剪出224\*224的一块，随机水平翻转，并利用和AlexNet一样的方法（PCA）随机扰动图像的RGB值。

#### 	图片缩放策略

​		网络输入为固定的224\*224，是在缩放后的图片中随机裁剪而来。设缩放后的图片短边长为$S$，有如下两种策略确定$S$：

​		**固定$S$的值：**将$S$固定为两个值：$S=256$和$S=384$。为了加速$S=384$时网络的训练速度，首先在$S=256$下训练网络，然后以此模型初始化$S=384$的模型，并调整学习率为$10^{-3}$。

​		**可变$S$的值：**对每张训练图片，在$[S_{min},S_{max}]$中随机取值，其中$S_{min}=256$，$S_{max}=512$。由于物体在图片中大小不一，将这个操作引入训练通常是有利的。可变$S$值也可看成是随机扰动训练图片尺寸的数据增强，使得一个模型能够检测不同尺寸的物体。为了加速训练，对于可变$S$值的模型，所有层的参数都使用固定$S=384$的模型参数来初始化。



### 3.2 Testing

​		设缩放后的图片短边长为$Q$。紧接着，应用与[(Sermanet et al., 2014)](https://www.cnblogs.com/liaohuiqiang/p/9348276.html)相同的操作：将全连接层转化为全卷积层。网络卷积层的输出特征图大小为7\*7，将第一个全连接层转化为卷积核大小为7\*7的全卷积层，后两个全连接层转化为卷积核大小为1\*1的全卷积层，参数不变。则第一个全连接层转化的全卷积层的输出为$(batch \ size \cross4096 \cross 1\cross1)$，后两个全连接层转化的全卷积层的输出为$(batch \ size \cross4096 \cross 1\cross1)$和$(batch \ size \cross1000 \cross 1\cross1)$。

​		这样做的好处是：当输入图像**大于**224\*224时，则最后一个卷积层的输出特征图大小**大于**7\*7，无法进入全连接层中，只能在图像中裁剪出224\*224大小的子图，对这些子图做多次前向运算，其中有相当一部分（裁剪重叠的部分）计算都是重复进行的。但如果将全连接层转化为全卷积，则相当于**对特征图的不同部分分别应用全连接层**，能够使网络适配不同的输入大小，同时节省了计算量。

![fully-convolution](..\pictures\fully-convolution.png)

​		例如：若输入图片的尺寸是256\*256，则最后一个卷积层的输出特征图大小为8\*8，经过三个全卷积层后，得到大小为$(batch \ size\cross1000\cross2\cross2)$的输出，左上角的值即相当于对原256\*256的图片右上角224\*224的子图送进网络得到的输出（并不相等，因为padding的缘故，见下面对比的第三条。如果不做padding，全卷积和裁剪之后分别计算完全等价）。（没明白可以看一眼[吴恩达的课](https://www.bilibili.com/video/BV1F4411y7o7?p=26)）

​		如此转换之后，将短边为$Q$的测试图片直接输入进网络，输出一个张量，通道维长度为1000，高宽维长度不定，取决于输入图片的高宽。最终，将这个张量在高宽维做平均，得到一个$(batch \ size \cross1000 \cross 1\cross1)$的张量，再送进softmax层做归一化，得到各类概率。

​		除此之外，测试图片的水平翻转版本也被送进网络，输出和未翻转的版本平均之后作为最终的输出。

**全卷积和直接裁剪原图并分别计算的对比：**

-   直接裁剪原图，能得到更多裁剪得更精细的子图，从而提升精度；但全卷积只能等效地以卷积层输出特征图的感受野为步长来对原图进行“裁剪分块”。

-   直接裁剪原图，需要对每一张裁剪出来的图片都计算一次前向传播；但全卷积只需要对大图进行一次前向传播即可。

-   直接裁剪原图，在3\*3卷积时，卷积核在特征图边缘运算时padding填充为0；但对整张大图进行运算时，全卷积相当于用特征图周围的值作为padding（这里需要仔细思考一下），间接扩大了感受野，包含更多的“上下文”信息。



​		实际使用中，裁剪原图后分别计算的方式多出的计算量并没有带来相应的性能提升。作者将图片缩放为3个尺寸，每个尺寸用5\*5的网格将图片裁剪成25张子图，并水平翻转，得到共3\*5\*5\*2=150张子图作为测试，得到了上述结论。



### 3.3 Implementation Details

​		C++，Caffe，多GPU，数据并行（看[李沐](https://www.bilibili.com/video/BV1vU4y1V7rd)），一个batch拆成几份分给各个GPU，并行地前向传播、反向传播，梯度汇总后平均，最后更新模型。4块GPU，比一张卡快了3.75倍，但还是要花上2到3周。



## 4 Classification Experiments

​		数据集：ILSVRC-2012数据集，是ILSVRC 2012-2014比赛用数据集，1000类，训练集1.3M张，验证集50K张，测试集100K张（不公开，所以实验的时候用验证集当测试集）。网络性能主要由top-1错误率和top-5错误率评估。



### 4.1 Single Scale Evaluation

​		设训练时缩放后的图片短边长为$S$，测试时缩放后的图片短边长为$Q$。

-   $S$值固定时，$Q=S$。
-   $S$值可变（$S\in[S_{min},S_{max}]$）时，$Q=0.5(S_{min}+S_{max})$。

​		在Table2中的配置A-LRN和配置A（没有任何归一化层）相比，没有性能提升，所以配置B~E不加LRN。

![VGG-Table 3](..\pictures\VGG-Table 3.png)

​		从表格中可以看出，深度越大精度越好。值得注意的是，D和C深度相同，仅将1\*1卷积全部替换成了3\*3卷积，就获得了精度提升，说明精度的提升不仅和额外引入的非线性性有关，3\*3卷积核带来的空间信息融合同样重要。

​		上文提到两个3\*3卷积层的感受野和一个5\*5卷积层的感受野相同，作者在配置B中还测试了用一个5\*5卷积层替换一对3\*3卷积层，结果top-1错误率增加了7%，这证实了较深的网络搭配上较小的卷积核的性能要强过较浅的网络搭配较大的卷积核。

​		从表格中还可得知，可变$S$的模型的性能完全碾压了固定$S$的模型，这证实了在训练时对图片尺寸的随机扰动确实有助于模型捕捉多尺度物体的统计信息。



### 4.2 Multi-Scale Evaluation

​		在单一测试图片尺寸上测试过模型的精度后，作者评估了在测试时扰动图片尺寸带来的结果，这是通过对同一图片的不同尺寸版本的输出做平均来实现的。由于$S$和$Q$差别过大会导致性能下降，对于固定$S$值，选取$Q=\{S-32,\ S,\ S+32\}$；对于可变$S$值$S\in[S_{min},S_{max}]$，由于这种情况下模型的鲁棒性更强，对尺寸的敏感度更低，适用于更大范围的尺寸，选取$Q=\{S_{min}, \ 0.5(S_{min}+S_{max}), \ S_{max}\}$。

![VGG-Table 4](..\pictures\VGG-Table 4.png)

​		可以看到，多$Q$模型的性能普遍好于单$Q$模型。



### 4.3 Multi-Crop Evaluation

![VGG-Table 5](..\pictures\VGG-Table 5.png)

​		可见，直接裁剪原图，分别计算并平均（计算softmax之后）得到的结果要好于直接全卷积法，但两者结合起来性能更佳，体现出互补的关系。作者假设这是由于两种方法在卷积是对边缘处padding的策略不同导致的。



### 4.4 ConvNet Fusion

​		正如李沐老师所说，搞计算机视觉的都是刷榜大师。作者在提交ILSVRC结果的时候把所有单尺寸的模型配置（A，A-LRN，B，C，D，E）和一个多尺寸的配置D在softmax之后做了平均，一共7个模型，达到了7.3%的top-5**测试集**错误率。之后，作者又把表现最好的两个模型（配置D和E）的多尺寸（多$Q$多$S$）版本做了平均，又把这两个模型的全卷积和裁剪原图版本做了平均，得到了6.8%的top-5错误率。

​		注意：VGG7模型ensemble拿7.3%，2模型ensemble拿6.8%。

​		真绝了



### 4.5 Comparison With The State-Of-The-Art

​		冠军是GoogLeNet，7模型ensemble拿6.7%，VGG只有俩模型拿了6.8%，作者还特意强调了他们遵循的是LeNet的传统CNN架构，只增加了深度。

![VGG-Table 6](..\pictures\VGG-Table 6.png)



![VGG-Table 7](..\pictures\VGG-Table 7.png)



## 5 Conclusion

​		评估了超级深CNN在图片分类任务上的表现。深度越大，精度越好，并且用传统CNN架构（不是GoogLeNet那种花里胡哨的分支结构）也能取得SOTA的性能。



## 总结

### 		贡献

​		展现了在传统CNN架构的基础上仅仅增加深度就能取得SOTA的性能，摒弃了LRN，详细地对比、评估了dense evaluation和multi-crop，详细地对比、评估了训练和测试图片的尺寸选取策略对网络性能的影响，用训练好的浅模型初始化深模型，增加训练稳定性。

### 		动机

​		看看传统CNN能做多深

### 		结构

​		VGG块+3个全连接

​		VGG块：3x3conv -> ... -> 1x1 / 3x3conv -> 2x2maxpool

### 		预处理

​		训练：和AlexNet一样，随机裁剪，随机翻转，对RGB用PCA提取主成分后随机扰动。

​		预测：dense evaluation或者multi-crop，然后随机翻转。
