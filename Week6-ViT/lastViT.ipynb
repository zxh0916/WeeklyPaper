{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61acc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066a27ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(hidden_dim, emb_dim), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0f3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadDotProductSelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.scale = emb_dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(emb_dim, 3 * emb_dim)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x, mask=None):\n",
    "        # shape of x: [batch_size, sequence_length, embedding_size]\n",
    "        b, n, _, h = *x.shape, self.num_heads\n",
    "        # shape of qkv: [batch_size, sequence_length, 3*embedding_size]\n",
    "        qkv = self.qkv(x)\n",
    "        # shape of q, k, v: [batch_size, sequence_length, embedding_size]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        # shape of q, k, v: [batch_size, num_heads, sequence_length, embedding_size / num_heads]\n",
    "        q, k, v = map(lambda x:x.reshape(b, n, h, -1).transpose(2, 1), (q, k, v))\n",
    "        # shape of attention_score: [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        attention_score = torch.matmul(q, k.transpose(2, 3)) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            # 最前面的[class]token永远得是有效的，所以在第一列pad一列True\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == attention_score.shape[-1], 'mask has incorrect dimensions'\n",
    "            # Mask中同一行里的False只会出现在True后面!!\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            attention_score.masked_fill_(~mask, float('-inf'))\n",
    "            del mask\n",
    "        \n",
    "        attention_weight = F.softmax(attention_score, dim=-1)\n",
    "        # shape of out: [batch_size, num_heads, sequence_length, embedding_size / num_heads]\n",
    "        out = torch.matmul(attention_weight, v)\n",
    "        out = out.transpose(2, 1).reshape(b, n, -1)\n",
    "        # shape of out: [batch_size, sequence_length, embedding_size]\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c374a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa = MultiHeadDotProductSelfAttention(emb_dim=10, num_heads=2, dropout=0.)\n",
    "a = torch.zeros(2, 5, 10)\n",
    "mask = torch.tensor([[True, True, False, False],\n",
    "                     [True, False, False, False]])\n",
    "msa(a, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964ecb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_norm_input = nn.LayerNorm(emb_dim)\n",
    "        self.layer_norm_out = nn.LayerNorm(emb_dim)\n",
    "        self.attention = MultiHeadDotProductSelfAttention(emb_dim, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(emb_dim, hidden_dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.layer_norm_input(x)\n",
    "        y = self.attention(y)\n",
    "        y = y + x\n",
    "        z = self.layer_norm_out(y)\n",
    "        z = self.ffn(z)\n",
    "        return z + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3aac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, emb_dim, num_heads, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderBlock(emb_dim, num_heads, hidden_dim, dropout) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058e2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.N = cfg.N\n",
    "        self.patch_size = cfg.patch_size\n",
    "        self.emb_dim = cfg.emb_dim\n",
    "        self.hidden_dim = cfg.hidden_dim\n",
    "        self.num_heads = cfg.num_heads\n",
    "        self.dropout = cfg.dropout\n",
    "        self.image_size = cfg.image_size\n",
    "        self.num_channels = cfg.num_channels\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_classes = cfg.num_classes\n",
    "        assert self.image_size % self.patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, self.emb_dim))\n",
    "        self.embedding = nn.Linear(self.num_channels * self.patch_size ** 2, self.emb_dim)\n",
    "        self.PE = nn.Parameter(torch.randn(1, self.num_patches+1, self.emb_dim))\n",
    "        self.emb_dropout = nn.Dropout(self.dropout)\n",
    "        self.transformer = Encoder(self.N, self.emb_dim, self.num_heads, self.hidden_dim, self.dropout)\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, 2*self.emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*self.emb_dim, self.num_classes))\n",
    "        \n",
    "    def forward(self, x, tokenize=True):\n",
    "        if tokenize:\n",
    "            x = self.tokenize(x)\n",
    "        b = x.shape[0]\n",
    "        x = self.embedding(x)\n",
    "        cls_tokens = self.class_token.repeat(b, 1, 1)\n",
    "        x = torch.concat((cls_tokens, x), dim=1)\n",
    "        x += self.PE\n",
    "        x = self.emb_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.MLP_head(x[:, 0, :])\n",
    "        return x\n",
    "    \n",
    "    def tokenize(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        assert h == self.image_size and w == self.image_size, 'the size of the input image is incorrect'\n",
    "        x = x.chunk(self.image_size // self.patch_size, dim=2)\n",
    "        patches = []\n",
    "        for patch in x:\n",
    "            patches += patch.chunk(self.image_size // self.patch_size, dim=3)\n",
    "        # shape of x: [batch_size, num_patches, num_channels * patch_size ** 2]\n",
    "        x = torch.stack(patches, dim=1).reshape(b, self.num_patches, -1)\n",
    "        return x\n",
    "    \n",
    "    def print_num_params(self):\n",
    "        print(sum(p.numel() for p in self.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07aa5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79547d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "['__add__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_integrity', '_format_transform_repr', '_is_protocol', '_load_meta', '_repr_indent', 'base_folder', 'class_to_idx', 'classes', 'data', 'download', 'extra_repr', 'filename', 'meta', 'root', 'target_transform', 'targets', 'test_list', 'tgz_md5', 'train', 'train_list', 'transform', 'transforms', 'url']\n",
      "(50000, 32, 32, 3)\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True)\n",
    "print(dir(cifar_train))\n",
    "print(cifar_train.data.shape) # (50000, 32, 32, 3)\n",
    "cifardata = cifar_train.data / 255\n",
    "mean = torch.tensor(cifardata.mean(axis=(0, 1, 2)))\n",
    "std = torch.tensor(cifardata.std(axis=(0, 1, 2)))\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e57c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_and_valid = data.random_split(torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True),\n",
    "                                    [45000, 5000],\n",
    "                                    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9933b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.RandomCrop(32, padding=4),\n",
    "                                         transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                         transforms.ConvertImageDtype(torch.float),\n",
    "                                         transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                                              [0.2470, 0.2435, 0.2616],\n",
    "                                                              inplace=True)])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.trans(self.dataset[index][0]),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb51215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.ConvertImageDtype(torch.float),\n",
    "                                         transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                                              [0.2470, 0.2435, 0.2616],\n",
    "                                                              inplace=True)])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.trans(self.dataset[index][0]),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53427b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainDataset(train_and_valid[0])\n",
    "valid_dataset = TestDataset(train_and_valid[1])\n",
    "test_dataset = TestDataset(torchvision.datasets.CIFAR10(root=\"../data\", train=False, download=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b183bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_acc(output, target, criterion):\n",
    "    pred = output.argmax(dim=1)\n",
    "    acc = ((pred == target).sum() / target.numel()).item()\n",
    "    loss = criterion(output, target)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a812534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_ratio(emb_dim, warmup_steps, cur_step):\n",
    "    if cur_step == 0:\n",
    "        return 0\n",
    "    lr = emb_dim ** -0.5\n",
    "    lr *= min(cur_step ** -0.5, (cur_step * warmup_steps ** -1.5))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c814c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return (optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af348b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ViT(net, train_dataset, valid_dataset):\n",
    "    cfg = net.cfg\n",
    "    with open('configs.txt', 'a') as f:\n",
    "        f.write('{\\n')\n",
    "        for k, v in net.cfg.__dict__.items():\n",
    "            f.write(k + ': ' + str(v) + \"\\n\")\n",
    "        f.write('}\\n')\n",
    "    train_dataloader = data.DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    valid_dataloader = data.DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    writer = SummaryWriter(f\"runs/ViT_CIFAR_{cfg.version}\")\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    warmup_lr = lambda cur_step: lr_ratio(net.emb_dim, cfg.warmup_steps, cur_step)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    global_step = 0\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        net.train()\n",
    "        train_loss, train_acc = [], []\n",
    "        for input, target in train_dataloader:\n",
    "            global_step += 1\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(input)\n",
    "            loss, acc = loss_acc(output, target, criterion)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            loss.backward()\n",
    "            writer.add_scalar('learning rate', get_lr(optimizer), global_step=global_step)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            writer.add_scalar('train/loss', loss.item(), global_step=global_step)\n",
    "            writer.add_scalar('train/accuracy', acc, global_step=global_step)\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            valid_loss, valid_acc = [], []\n",
    "            for input, target in valid_dataloader:\n",
    "                input, target = input.to(device), target.to(device)\n",
    "                output = net(input)\n",
    "                loss, acc = loss_acc(output, target, criterion)\n",
    "                valid_loss.append(loss.item())\n",
    "                valid_acc.append(acc)\n",
    "            writer.add_scalar('valid/loss', sum(valid_loss) / len(valid_loss), global_step=global_step)\n",
    "            writer.add_scalar('valid/accuracy', sum(valid_acc) / len(valid_acc), global_step=global_step)\n",
    "        message = list(map(lambda x:sum(x) / len(x), (train_loss, train_acc, valid_loss, valid_acc)))\n",
    "        print(f'epoch {epoch+1:3d}, train loss: {message[0]:8.4f}, train accuracy: {message[1]:8.4f}, valid loss: {message[2]:8.4f}, valid accuracy: {message[3]:8.4f}')\n",
    "        torch.save(net.state_dict(), f'ViT_CIFAR_{cfg.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c9e1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, version):\n",
    "        ############### model hyperparameters ###############\n",
    "        self.version = version\n",
    "        self.N = 8\n",
    "        self.patch_size = 4\n",
    "        self.emb_dim = 128\n",
    "        self.hidden_dim = 256\n",
    "        self.num_heads = 4\n",
    "        self.dropout = 0.05\n",
    "        self.image_size = 32\n",
    "        self.num_channels = 3\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_classes = 10\n",
    "        ############### train hyperparameters ###############\n",
    "        self.batch_size = 256\n",
    "        self.lr = 1e-1\n",
    "        self.weight_decay = 5e-4\n",
    "        self.warmup_steps = 1000\n",
    "        self.num_epochs = 75\n",
    "        self.num_workers = 0\n",
    "        #####################################################\n",
    "    def __str__(self):\n",
    "        return self.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09741d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110154\n",
      "epoch   1, train loss:   2.2402, train accuracy:   0.1540, valid loss:   2.0744, valid accuracy:   0.2178\n",
      "epoch   2, train loss:   2.0106, train accuracy:   0.2448, valid loss:   1.9771, valid accuracy:   0.2638\n",
      "epoch   3, train loss:   1.9153, train accuracy:   0.2878, valid loss:   1.9100, valid accuracy:   0.2823\n",
      "epoch   4, train loss:   1.8166, train accuracy:   0.3250, valid loss:   1.7713, valid accuracy:   0.3560\n",
      "epoch   5, train loss:   1.7142, train accuracy:   0.3654, valid loss:   1.7098, valid accuracy:   0.3781\n"
     ]
    }
   ],
   "source": [
    "    cfg = Configuration('Version4')\n",
    "    net = ViT(cfg).to(device)\n",
    "    net.print_num_params()\n",
    "    train_ViT(net, train_dataset, valid_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
