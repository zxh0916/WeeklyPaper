{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231228cd-c3eb-42ec-bba2-6fd2a8bbf3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c54a95-b6c4-4184-8781-a2b4151693c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Dropout(dropout, inplace=True), \n",
    "            nn.Linear(hidden_dim, emb_dim), \n",
    "            nn.Dropout(dropout, inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab3867-39d6-4ecb-88b7-009680c4c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadDotProductSelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.scale = emb_dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(emb_dim, 3 * emb_dim)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout, inplace=True)\n",
    "        )\n",
    "    def forward(self, x, mask=None):\n",
    "        # shape of x: [batch_size, sequence_length, embedding_size]\n",
    "        b, n, _, h = *x.shape, self.num_heads\n",
    "        # shape of qkv: [batch_size, sequence_length, 3*embedding_size]\n",
    "        qkv = self.qkv(x)\n",
    "        # shape of q, k, v: [batch_size, sequence_length, embedding_size]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        # shape of q, k, v: [batch_size, num_heads, sequence_length, embedding_size / num_heads]\n",
    "        q, k, v = map(lambda x:x.reshape(b, n, h, -1).transpose(2, 1), (q, k, v))\n",
    "        # shape of attention_score: [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        attention_score = torch.matmul(q, k.transpose(2, 3)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            # 最前面的[class]token永远得是有效的，所以在第一列pad一列True\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == attention_score.shape[-1], 'mask has incorrect dimensions'\n",
    "            # Mask中同一行里的False只会出现在True后面!!\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            attention_score.masked_fill_(~mask, float('-inf'))\n",
    "            del mask\n",
    "        \n",
    "        attention_weight = F.softmax(attention_score, dim=-1)\n",
    "        # shape of out: [batch_size, num_heads, sequence_length, embedding_size / num_heads]\n",
    "        out = torch.matmul(attention_weight, v)\n",
    "        out = out.transpose(2, 1).reshape(b, n, -1)\n",
    "        # shape of out: [batch_size, sequence_length, embedding_size]\n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c2fce-ad93-49a3-8cc1-a3683db3d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = MultiHeadDotProductSelfAttention(emb_dim=10, num_heads=2, dropout=0.)\n",
    "a = torch.zeros(2, 5, 10)\n",
    "mask = torch.tensor([[True, True, False, False],\n",
    "                     [True, False, False, False]])\n",
    "msa(a, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a83cf54-2acc-48b9-972a-c1b25dad8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_norm_input = nn.LayerNorm(emb_dim)\n",
    "        self.layer_norm_out = nn.LayerNorm(emb_dim)\n",
    "        self.attention = MultiHeadDotProductSelfAttention(emb_dim, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(emb_dim, hidden_dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.layer_norm_input(x)\n",
    "        y = self.attention(y)\n",
    "        y = y + x\n",
    "        z = self.layer_norm_out(y)\n",
    "        z = self.ffn(z)\n",
    "        return z + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acffc5-1e6d-4843-88e0-bb712c0cba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, emb_dim, num_heads, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderBlock(emb_dim, num_heads, hidden_dim, dropout) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b7958-8866-4e88-8b38-598a19dae702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.N = cfg.N\n",
    "        self.patch_size = cfg.patch_size\n",
    "        self.emb_dim = cfg.emb_dim\n",
    "        self.hidden_dim = cfg.hidden_dim\n",
    "        self.num_heads = cfg.num_heads\n",
    "        self.dropout = cfg.dropout\n",
    "        self.max_length = cfg.max_length\n",
    "        self.image_size = cfg.image_size\n",
    "        self.num_channels = cfg.num_channels\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_classes = cfg.num_classes\n",
    "        assert self.image_size % self.patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        assert self.max_length >= self.num_patches, 'max length of sequence must greater or equal than the number of patches'\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, self.emb_dim))\n",
    "        self.embedding = nn.Linear(self.num_channels * self.patch_size ** 2, self.emb_dim)\n",
    "        self.PE = nn.Parameter(torch.randn(1, self.max_length, self.emb_dim))\n",
    "        self.emb_dropout = nn.Dropout(self.dropout, inplace=True)\n",
    "        self.transformer = Encoder(self.N, self.emb_dim, self.num_heads, self.hidden_dim, dropout=0.)\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, 2*self.emb_dim),\n",
    "            nn.Linear(2*self.emb_dim, self.num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        assert h == self.image_size and w == self.image_size, 'the size of the input image is incorrect'\n",
    "        x = x.chunk(self.image_size // self.patch_size, dim=2)\n",
    "        patches = []\n",
    "        for patch in x:\n",
    "            patches += patch.chunk(self.image_size // self.patch_size, dim=3)\n",
    "        # shape of x: [batch_size, num_patches, num_channels * patch_size ** 2]\n",
    "        x = torch.stack(patches, dim=1).reshape(b, self.num_patches, -1)\n",
    "        x = self.embedding(x)\n",
    "        cls_tokens = self.class_token.repeat(b, 1, 1)\n",
    "        x = torch.concat((cls_tokens, x), dim=1)\n",
    "        x += self.PE[:, :self.num_patches+1]\n",
    "        x = self.emb_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.MLP_head(x[:, 0, :])\n",
    "        return x\n",
    "    \n",
    "    def print_num_params(self):\n",
    "        \"\"\"打印网络参数数量\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'{total_params:,} total parameters.')\n",
    "        total_trainable_params = sum(\n",
    "            p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f'{total_trainable_params:,} trainable parameters.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4bbda-a795-4924-9c61-65584844e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa04e1-d95f-42b6-a7d7-20ac614e2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True)\n",
    "print(cifar_train.data.shape) # (50000, 32, 32, 3)\n",
    "cifardata = cifar_train.data / 255\n",
    "mean = torch.tensor(cifardata.mean(axis=(0, 1, 2)))\n",
    "std = torch.tensor(cifardata.std(axis=(0, 1, 2)))\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df29123-01c7-458f-bfc5-7be6c425d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_valid = data.random_split(torchvision.datasets.CIFAR10(root=\"../data\", train=True, download=True),\n",
    "                                    [45000, 5000],\n",
    "                                    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b9170-340a-4480-b736-fceef9a98da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.RandomCrop(32, padding=4),\n",
    "                                         transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                         transforms.ConvertImageDtype(torch.float),\n",
    "                                         transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                                              [0.2470, 0.2435, 0.2616],\n",
    "                                                              inplace=True)])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.trans(self.dataset[index][0]),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c27d7-0908-4413-a4f9-65a16d3898c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.ConvertImageDtype(torch.float),\n",
    "                                         transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                                              [0.2470, 0.2435, 0.2616],\n",
    "                                                              inplace=True)])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.trans(self.dataset[index][0]),\n",
    "                self.dataset[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc5853-c1f4-4289-a4c6-fcc2eca9f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(train_and_valid[0])\n",
    "valid_dataset = TestDataset(train_and_valid[1])\n",
    "test_dataset = TestDataset(torchvision.datasets.CIFAR10(root=\"../data\", train=False, download=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83540544-d22b-4995-9fd2-c3becb6793b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_acc(output, target, criterion):\n",
    "    pred = output.argmax(dim=1)\n",
    "    acc = ((pred == target).sum() / target.numel()).item()\n",
    "    loss = criterion(output, target)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e5cfb-7687-4662-ac1d-9920585106a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_ratio(emb_dim, warmup_steps, cur_step):\n",
    "    if cur_step == 0:\n",
    "        return 0\n",
    "    lr = emb_dim ** -0.5\n",
    "    lr *= min(cur_step ** -0.5, (cur_step * warmup_steps ** -1.5))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2873f-3716-4f8f-ba72-01a997f655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return (optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63477d-5ef6-4338-ab5c-c8a65f1eab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ViT(net, cfg):\n",
    "    # with open('configs.txt', 'a') as f:\n",
    "    #     f.write('{\\n')\n",
    "    #     for k, v in net.cfg.__dict__.items():\n",
    "    #         f.write(k + ': ' + str(v) + \"\\n\")\n",
    "    #     f.write('}\\n')\n",
    "    train_dataloader = data.DataLoader(train_dataset,\n",
    "                                       batch_size=cfg.batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=cfg.num_workers)\n",
    "    valid_dataloader = data.DataLoader(valid_dataset,\n",
    "                                       batch_size=cfg.batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=cfg.num_workers)\n",
    "    writer = SummaryWriter(f\"runs/ViT_CIFAR_{cfg.version}\")\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    warmup_lr = lambda cur_step: lr_ratio(net.emb_dim, cfg.warmup_steps, cur_step)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    global_step = 0\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        net.train()\n",
    "        for input, target in train_dataloader:\n",
    "            global_step += 1\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(input)\n",
    "            loss, acc = loss_acc(output, target, criterion)\n",
    "            loss.backward()\n",
    "            writer.add_scalar('learning rate', get_lr(optimizer), global_step=global_step)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            writer.add_scalar('train/loss', loss.item(), global_step=global_step)\n",
    "            writer.add_scalar('train/accuracy', acc, global_step=global_step)\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            valid_loss, valid_acc = [], []\n",
    "            for input, target in valid_dataloader:\n",
    "                input, target = input.to(device), target.to(device)\n",
    "                output = net(input)\n",
    "                loss, acc = loss_acc(output, target, criterion)\n",
    "                valid_loss.append(loss.item())\n",
    "                valid_acc.append(acc)\n",
    "            writer.add_scalar('valid/loss', sum(valid_loss) / len(valid_loss), global_step=global_step)\n",
    "            writer.add_scalar('valid/accuracy', sum(valid_acc) / len(valid_acc), global_step=global_step)\n",
    "        message = list(map(lambda x:sum(x) / len(x), (train_loss, train_acc, valid_loss, valid_acc)))\n",
    "        print(f'epoch {epoch+1:3d}, train loss: {message[0]:8.4f}, train accuracy: {message[1]:8.4f}, valid loss: {message[2]:8.4f}, valid accuracy: {message[3]:8.4f}')\n",
    "        torch.save(net.state_dict(), f'ViT_CIFAR_{cfg.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bba096-b7e6-4daa-9bf1-637ed35d7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, version):\n",
    "        ############### model hyperparameters ###############\n",
    "        self.version = version\n",
    "        self.N = 8\n",
    "        self.patch_size = 8\n",
    "        self.emb_dim = 128\n",
    "        self.hidden_dim = 256\n",
    "        self.num_heads = 4\n",
    "        self.dropout = 0.\n",
    "        self.max_length = 100\n",
    "        self.image_size = 32\n",
    "        self.num_channels = 3\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_classes = 10\n",
    "        ############### train hyperparameters ###############\n",
    "        self.batch_size = 512\n",
    "        self.lr = 1e-1\n",
    "        self.weight_decay = 0\n",
    "        self.warmup_steps = 500\n",
    "        self.num_epochs = 10\n",
    "        self.num_workers = 0\n",
    "        #####################################################\n",
    "    def __str__(self):\n",
    "        return self.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd44121-6449-4107-b0ea-00a47203d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Configuration('Version5')\n",
    "net = ViT(cfg).to(device)\n",
    "net.print_num_params()\n",
    "train_ViT(net, cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
