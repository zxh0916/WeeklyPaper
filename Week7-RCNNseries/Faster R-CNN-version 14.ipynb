{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f50890-76d8-4f53-a396-6dd398035abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0a741d-c1f8-4ae5-8bac-788bb9d7006d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PascalVOC2012(torch.utils.data.Dataset):\n",
    "    \"\"\"PASCAL VOC 2012 数据集\"\"\"\n",
    "    def __init__(self, train=True, scale_ratio=1.0):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        self.scale_ratio = scale_ratio # 图片缩放倍数\n",
    "        self.data = torchvision.datasets.VOCDetection(root='../../data',\n",
    "                                                      year='2012',\n",
    "                                                      image_set='train' if train else 'val',\n",
    "                                                      download=False)\n",
    "        # 训练集对亮度、对比度、饱和度和色调进行扰动，随后归一化\n",
    "        self.trans_train = T.Compose([T.ToTensor(),\n",
    "                                      T.ColorJitter(brightness=0.2,\n",
    "                                                    contrast=0.2,\n",
    "                                                    saturation=0.2,\n",
    "                                                    hue=0.1),\n",
    "                                      T.Normalize(mean=[0.4570, 0.4382, 0.4062],\n",
    "                                                   std=[0.2391, 0.2351, 0.2397],)])\n",
    "        # 验证集仅做归一化处理\n",
    "        self.trans_valid = T.Compose([T.ToTensor(),\n",
    "                                      T.Normalize(mean=[0.4570, 0.4382, 0.4062],\n",
    "                                                   std=[0.2391, 0.2351, 0.2397],)])\n",
    "        self.horizontal_flip = T.RandomHorizontalFlip(p=1)\n",
    "        # 类别列表，共20个类别：人、6种动物、7种交通工具和6种日用品\n",
    "        self.cls_labels = ['person',\n",
    "                           'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "                           'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "                           'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集样本个数\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"从数据集中取出图片和对应标签\"\"\"\n",
    "        if self.train:\n",
    "            # 格式转换、色彩扰动、归一化\n",
    "            image = self.trans_train(self.data[index][0])\n",
    "        else:\n",
    "            # 格式转换、归一化\n",
    "            image = self.trans_valid(self.data[index][0])\n",
    "        box_labels, box_coords = self.get_label_list(self.data[index][1])\n",
    "        # 水平翻转\n",
    "        image, box_coords = self.RandomHorizontalFlip(image, box_coords)\n",
    "        # 按缩放倍数进行缩放\n",
    "        image = T.Resize(size=int(min(image.shape[-1], image.shape[-2]) * self.scale_ratio))(image)\n",
    "        return image, (box_labels, (box_coords * self.scale_ratio).int())\n",
    "    \n",
    "    def get_label_list(self, label):\n",
    "        \"\"\"从标签字典中取出各物体类别和对应边界框\"\"\"\n",
    "        obj_list = label['annotation']['object']\n",
    "        box_labels = [self.cls_labels.index(obj['name'] if type(obj['name']) == str else obj['name'][0]) for obj in obj_list]\n",
    "        box_coords = []\n",
    "        for obj in obj_list:\n",
    "            coord = []\n",
    "            # 真实边界框为xyxy格式\n",
    "            for k in ['xmin', 'ymin', 'xmax', 'ymax']:\n",
    "                v = obj['bndbox'][k]\n",
    "                coord.append(int(v if type(v) == str else v[0]))\n",
    "            box_coords.append(coord)\n",
    "            # 返回两个张量，box_labels形状为（物体数），box_coords形状为（物体数，4）\n",
    "        return (torch.tensor(box_labels), torch.tensor(box_coords))\n",
    "\n",
    "    def RandomHorizontalFlip(self, image, box_coords):\n",
    "        \"\"\"训练集中，将图片以0.5的概率水平翻转\"\"\"\n",
    "        if self.train and random.random() > 0.5:\n",
    "            w = image.shape[-1]\n",
    "            # 水平翻转图片\n",
    "            image = self.horizontal_flip(image)\n",
    "            # 水平翻转边界框：\n",
    "            # x1 = w - x2, x2 = w - x1\n",
    "            x1, x2 = box_coords[:, 0], box_coords[:, 2]\n",
    "            box_coords[:, 0], box_coords[:, 2] = w - x2, w - x1\n",
    "        return image, box_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba16247b-0a1c-4099-a439-54eef034e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = PascalVOC2012(train=True)\n",
    "voc_val = PascalVOC2012(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289652b9-3f68-41c1-9261-826849b803a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_box(box_cxcywh, shift):\n",
    "    \"\"\"使用偏移系数修正锚框/候选框，输入输出皆为cxcywh格式\"\"\"\n",
    "    box = box_cxcywh.to(shift.device)\n",
    "    p_cx = box[:, 2] * shift[:, 0] + box[:, 0]\n",
    "    p_cy = box[:, 3] * shift[:, 1] + box[:, 1]\n",
    "    p_w = box[:, 2] * torch.exp(shift[:, 2])\n",
    "    p_h = box[:, 3] * torch.exp(shift[:, 3])\n",
    "    return torch.stack([p_cx, p_cy, p_w, p_h], dim=1)\n",
    "\n",
    "def coord_to_shift(src_cxcywh, tgt_cxcywh):\n",
    "    \"\"\"使用源框和目标框计算从源框到目标框的偏移系数\"\"\"\n",
    "    assert src_cxcywh.shape == tgt_cxcywh.shape\n",
    "    t_x = (tgt_cxcywh[:, 0] - src_cxcywh[:, 0]) / src_cxcywh[:, 2]\n",
    "    t_y = (tgt_cxcywh[:, 1] - src_cxcywh[:, 1]) / src_cxcywh[:, 3]\n",
    "    t_w = torch.log(tgt_cxcywh[:, 2] / src_cxcywh[:, 2])\n",
    "    t_h = torch.log(tgt_cxcywh[:, 3] / src_cxcywh[:, 3])\n",
    "    return torch.stack([t_x, t_y, t_w, t_h], dim=1)\n",
    "\n",
    "# 边界框格式转换\n",
    "def cxcywh2xyxy(boxes):\n",
    "    return torchvision.ops.box_convert(boxes, 'cxcywh', 'xyxy').int()\n",
    "def xyxy2cxcywh(boxes):\n",
    "    return torchvision.ops.box_convert(boxes, 'xyxy', 'cxcywh').int()\n",
    "\n",
    "# 固定/解除固定模型参数\n",
    "def freeze(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(False)\n",
    "def unfreeze(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "# \n",
    "def batched_nms(boxes, # 预测框集合[N, 4]\n",
    "                scores, # 预测框对应类别的置信度\n",
    "                idxs, # 预测框对应类别\n",
    "                iou_threshold # IOU阈值，与置信度最高的预测框的IOU高于此阈值的同类别预测框会被丢弃\n",
    "                ):\n",
    "    \"\"\"\n",
    "    逐类进行非极大值抑制\n",
    "    Args:\n",
    "        boxes (Tensor): 预测框集合，形状为[N, 4]\n",
    "        scores (Tensor): 预测框对应类别的置信度，形状为[N]\n",
    "        idxs (Tensor): 预测框对应类别，形状为[N]\n",
    "        iou_threshold (float): IOU阈值，与置信度最高的预测框的IOU高于此阈值的同类别预测框会被丢弃\n",
    "    \"\"\"\n",
    "    keep_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "    # 遍历所有类别\n",
    "    for class_id in torch.unique(idxs):\n",
    "        curr_indices = torch.where(idxs == class_id)[0]\n",
    "        # 逐类NMS\n",
    "        curr_keep_indices = torchvision.ops.nms(boxes[curr_indices], scores[curr_indices], iou_threshold)\n",
    "        keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "    keep_indices = torch.where(keep_mask)[0]\n",
    "    # 将留下的预测框按置信度降序排列\n",
    "    return keep_indices[scores[keep_indices].sort(descending=True)[1]]\n",
    "\n",
    "def init_weight(module):\n",
    "    \"\"\"递归初始化模型参数\"\"\"\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.normal_(module.weight, std=0.01)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.Sequential, nn.ModuleList)):\n",
    "        for m in module:\n",
    "            init_weight(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68234d31-49ff-486a-a8b1-78cc49507872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_pred_to_gt(pred,\n",
    "                      gt,\n",
    "                      pos_threshold,\n",
    "                      neg_threshold,\n",
    "                      allow_low_quality_matches=True):\n",
    "    \"\"\"\n",
    "    按阈值将锚框/候选框分为正、负样本，并将正样本分配给真实边界框\n",
    "    Args:\n",
    "        pred (Tensor): 待分配的锚框，xyxy格式，形状为[N, 4]。\n",
    "        gt (Tensor): 真实边界框，xyxy格式，形状为[M, 4]。\n",
    "        pos_threshold (float): 正样本IOU阈值，与某真实边界框IOU超过此阈值的\n",
    "            锚框/候选框被标记为正样本。\n",
    "        neg_threshold (float, tuple): 负样本IOU阈值，与所有真实边界框最高IOU\n",
    "            在此区间或低于此值的锚框/候选框被标记为正样本。\n",
    "        allow_low_quality_matches (bool): 允许低IOU的匹配，保证每个真实边界框\n",
    "            都存在至少一个锚框/候选框与之对应。\n",
    "    \"\"\"\n",
    "    iou_table = torchvision.ops.box_iou(pred, gt)\n",
    "    pos_pred_indices, pos_gt_indices = [], []\n",
    "    max_values, max_indices = iou_table.max(dim=1)\n",
    "    # 正样本：与某真实边界框IOU超过阈值的锚框/候选框\n",
    "    positive = max_values > pos_threshold\n",
    "    pos_pred_indices.append(torch.arange(0, pred.shape[0], 1, device=pred.device)[positive])\n",
    "    pos_gt_indices.append(max_indices[positive])\n",
    "    iou_table[positive] = -1 # 防止该锚框/候选框再次被选中成为正/负样本\n",
    "    \n",
    "    # 用来兜底的低质量正样本：与每个真实边界框的IOU最高的锚框/候选框\n",
    "    if allow_low_quality_matches:\n",
    "        for i in range(gt.shape[0]): # 遍历所有真实边界框\n",
    "            argmax = iou_table[:, i].argmax().reshape(1)\n",
    "            if iou_table[argmax, i] > 0: # 已被选中过的锚框/候选框不参与本轮分配\n",
    "                pos_pred_indices.append(argmax)\n",
    "                pos_gt_indices.append(torch.tensor([i], device=pred.device))\n",
    "                iou_table[argmax] = -1\n",
    "    # 负样本：与所有真实边界框的最大IOU在区间内或低于阈值的\n",
    "    max_values, max_indices = iou_table.max(dim=1)\n",
    "    if isinstance(neg_threshold, float): # 若负样本IOU阈值为一小数\n",
    "        negative = (max_values < neg_threshold) & (max_values > 0.)\n",
    "    elif isinstance(neg_threshold, tuple): # 若负样本IOU阈值为一区间\n",
    "        negative = (max_values < neg_threshold[1]) & (max_values > neg_threshold[0])\n",
    "    neg_pred_indices = torch.arange(0, pred.shape[0], 1, device=pred.device)[negative]\n",
    "    \n",
    "    pos_pred_indices = torch.concat(pos_pred_indices)\n",
    "    pos_gt_indices = torch.concat(pos_gt_indices)\n",
    "    # 确保每个锚框/候选框仅被分配给了一个真实边界框\n",
    "    assert pos_pred_indices.unique().shape == pos_pred_indices.shape,\\\n",
    "        'there is at least one predicted box assigned to multiple ground truth boxes'\n",
    "    return pos_pred_indices, pos_gt_indices, neg_pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44e37e0-bfb1-45a3-96ee-9c8f7fee874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(pos_pred_indices,\n",
    "                    pos_gt_indices,\n",
    "                    neg_pred_indices,\n",
    "                    pred_xyxy,\n",
    "                    pred_conf,\n",
    "                    gt_xyxy,\n",
    "                    gt_label,\n",
    "                    neg_cls_target,\n",
    "                    pos_ratio,\n",
    "                    batch_size):\n",
    "    \"\"\"\n",
    "    对assign_pred_to_gt分配的结果（正负样本集）按指定正样本比例和批量大小进行采样，\n",
    "    Args:\n",
    "        pos_pred_indices (Tensor): 被选中成为正样本的锚框/预测框的下标。\n",
    "        pos_gt_indices (Tensor): 各个成为正样本的锚框/预测框对应的真实边界框。\n",
    "        neg_pred_indices (Tensor): 被选中成为负样本的锚框/预测框的下标。\n",
    "        pred_xyxy (Tensor): 全部预测框/锚框。\n",
    "        pred_conf (Tensor): 网络在各个预测框/锚框（在各个类别上）的回归输出。\n",
    "        gt_xyxy (Tensor): 真实边界框，xyxy格式。\n",
    "        gt_label (Tensor): 各个真实边界框所属的类别编号。\n",
    "        neg_cls_target (Tensor): 负样本对应的类别（背景类）编号。\n",
    "        pos_ratio (float): 一个小批量中正样本占比上限。\n",
    "        batch_size (int): 批量大小。\n",
    "    \"\"\"\n",
    "    # 采样正样本\n",
    "    pos_indices = [i for i in range(len(pos_pred_indices))]\n",
    "    random.shuffle(pos_indices)\n",
    "    pos_indices = pos_indices[:int(batch_size * pos_ratio)]\n",
    "    \n",
    "    # 采样负样本\n",
    "    neg_indices = [i for i in range(len(neg_pred_indices))]\n",
    "    random.shuffle(neg_indices)\n",
    "    # 正样本不够就用负样本补全 batch\n",
    "    neg_indices = neg_indices[:max(batch_size-len(pos_indices), int(batch_size * (1 - pos_ratio)))]\n",
    "    \n",
    "    pos_pred_indices = pos_pred_indices[pos_indices]\n",
    "    pos_gt_indices = pos_gt_indices[pos_indices]\n",
    "    neg_pred_indices = neg_pred_indices[neg_indices]\n",
    "    \n",
    "    # 被选中成为正样本的锚框/候选框到其对应的真实边界框的偏移系数就是网络的回归目标\n",
    "    pos_reg_target = coord_to_shift(xyxy2cxcywh(pred_xyxy)[pos_pred_indices],\n",
    "                                    xyxy2cxcywh(gt_xyxy)[pos_gt_indices])\n",
    "    \n",
    "    # 网络在正负样本上输出的（各个类别的）置信度\n",
    "    pos_cls_conf = pred_conf[pos_pred_indices]\n",
    "    neg_cls_conf = pred_conf[neg_pred_indices]\n",
    "    \n",
    "    # 各个正样本对应的真实边界框的类别编号\n",
    "    pos_cls_target = gt_label[pos_gt_indices]\n",
    "    \n",
    "    # 背景类类别编号\n",
    "    neg_cls_target = torch.empty(neg_pred_indices.shape,\n",
    "                                 device=pred_conf.device,\n",
    "                                 dtype=int).fill_(neg_cls_target)\n",
    "    \n",
    "    # 正负样本的类别置信度和类别编号\n",
    "    cls_conf = torch.concat([pos_cls_conf, neg_cls_conf])\n",
    "    cls_target = torch.concat([pos_cls_target, neg_cls_target])\n",
    "    return pos_reg_target, (cls_conf, cls_target), pos_pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b61842-9d06-40b7-a64e-7a8d748453f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBR(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a87644-b032-41ec-88e2-fd4c8f5cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"区域提议网络（不含特征提取部分）。\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, area, ratio, downsample_rate):\n",
    "        super().__init__()\n",
    "        # RPN独有的卷积层\n",
    "        self.conv = [CBR(in_channels, hidden_channels), ]\n",
    "        for i in range(1, num_layers):\n",
    "            self.conv.append(CBR(hidden_channels, hidden_channels))\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        \n",
    "        self.area = area\n",
    "        self.ratio = ratio\n",
    "        self.num_boxes = len(area) * len(ratio)\n",
    "        # 回归分支，输出 H * W * num_boxes * 4个偏移系数\n",
    "        self.branch_reg = nn.Conv2d(hidden_channels, self.num_boxes * 4, kernel_size=1, stride=1, padding=0)\n",
    "        # 分类分支，输出 H * W * num_boxes * 1个置信度\n",
    "        self.branch_cls = nn.Conv2d(hidden_channels, self.num_boxes, kernel_size=1, stride=1, padding=0)\n",
    "        # 单个位置上的锚框集合\n",
    "        self.boxes = [(self.area[i], self.ratio[j]) for i in range(len(self.area)) for j in range(len(self.ratio))]\n",
    "        # backboneCNN的下采样率（所有层的步长的乘积）\n",
    "        self.conv_downsample_rate = downsample_rate\n",
    "        \n",
    "        # 初始化网络权重\n",
    "        init_weight(self.conv)\n",
    "        init_weight(self.branch_reg)\n",
    "        init_weight(self.branch_cls)\n",
    "    \n",
    "    def forward(self, feature_map):\n",
    "        \"\"\"输入backbone CNN输出的特征图，输出每个位置上每个锚框的物体置信度和4个偏移系数。\"\"\"\n",
    "        hidden = self.conv(feature_map)\n",
    "        objectness = self.branch_cls(hidden)\n",
    "        shift = self.branch_reg(hidden)\n",
    "        # shape of objectness: [H * W * self.num_boxes]\n",
    "        # shape of shift: [H * W * self.num_boxes, 4]\n",
    "        return objectness.permute(0, 2, 3, 1).flatten(), shift.permute(0, 2, 3, 1).reshape(-1, 4)\n",
    "    \n",
    "    def generate_anchor(self, output_size):\n",
    "        \"\"\"给定backbone CNN输出的特征图尺寸[C * H * W]，生成H * W * self.num_boxes个锚框，cxcywh格式。\"\"\"\n",
    "        # 锚框的高宽与其位置无关，统一计算后复制到各个位置上\n",
    "        wh = torch.zeros(self.num_boxes, 2, device=device)\n",
    "        for k, (area, ratio) in enumerate(self.boxes):\n",
    "            # 高宽比ratio = h / w\n",
    "            w = int((area / ratio) ** 0.5)\n",
    "            h = int(w * ratio)\n",
    "            wh[k, 0] = w\n",
    "            wh[k, 1] = h\n",
    "        # 计算各个位置上锚框的中心点位置\n",
    "        # 原文将锚框的中心点放置在了特征图上对应像素的感受野中心，\n",
    "        # 这里从距离左上角(下采样率/2)个像素开始放置，步长为下采样率\n",
    "        cx = torch.arange(0, output_size[-1], 1, device=device).reshape(1, output_size[-1], 1, 1)\\\n",
    "            * self.conv_downsample_rate + self.conv_downsample_rate // 2\n",
    "        cy = torch.arange(0, output_size[-2], 1, device=device).reshape(output_size[-2], 1, 1, 1)\\\n",
    "            * self.conv_downsample_rate + self.conv_downsample_rate // 2\n",
    "        # 将高宽和中心点位置拼接起来，形成 H * W * self.num_boxes个锚框\n",
    "        anchors = torch.concat([cx.expand(output_size[-2], -1, self.num_boxes, -1),\n",
    "                                cy.expand(-1, output_size[-1], self.num_boxes, -1),\n",
    "                                wh.expand(output_size[-2], output_size[-1], -1, -1)], dim=-1)\n",
    "        # shape of anchors: [H * W * self.num_boxes, 4]\n",
    "        return anchors.reshape(-1, 4)\n",
    "    \n",
    "    def get_proposal(self, feature_map):\n",
    "        \"\"\"利用RPN输出的偏移系数和锚框坐标来对锚框进行修正得到候选框\"\"\"\n",
    "        with torch.no_grad():\n",
    "            objectness, shift = self.forward(feature_map)\n",
    "        anchor = self.generate_anchor(feature_map.shape)\n",
    "        proposal_cxcywh = refine_box(anchor, shift)\n",
    "        proposal_xyxy = cxcywh2xyxy(proposal_cxcywh)\n",
    "        return proposal_xyxy, torch.sigmoid(objectness)\n",
    "\n",
    "    def generate_training_data(self,\n",
    "                               feature_map,\n",
    "                               objectness,\n",
    "                               gtboxes_xyxy,\n",
    "                               pos_threshold,\n",
    "                               neg_threshold,\n",
    "                               pos_ratio=0.5,\n",
    "                               batch_size=256):\n",
    "        \"\"\"生成训练数据\"\"\"\n",
    "        # 生成锚框\n",
    "        anchor_cxcywh = self.generate_anchor(feature_map.shape)\n",
    "        anchor_xyxy = cxcywh2xyxy(anchor_cxcywh)\n",
    "        # 匹配锚框和真实边界框\n",
    "        pos_anchor_indices, pos_gtbox_indices, neg_anchor_indices = \\\n",
    "            assign_pred_to_gt(anchor_xyxy,\n",
    "                              gtboxes_xyxy,\n",
    "                              pos_threshold,\n",
    "                              neg_threshold,\n",
    "                              allow_low_quality_matches=True)\n",
    "        # 采样，生成训练数据\n",
    "        pos_reg_target, (cls_logits, cls_target), pos_anchor_indices = \\\n",
    "            random_sampling(pos_anchor_indices,\n",
    "                            pos_gtbox_indices,\n",
    "                            neg_anchor_indices,\n",
    "                            anchor_xyxy,\n",
    "                            objectness,\n",
    "                            gtboxes_xyxy,\n",
    "                            gt_label=torch.ones_like(objectness),\n",
    "                            neg_cls_target=0,\n",
    "                            pos_ratio=pos_ratio,\n",
    "                            batch_size=batch_size)\n",
    "        return anchor_xyxy, pos_reg_target, (cls_logits, cls_target), pos_anchor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7039def1-1428-4162-a8e2-db9bbc9eb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN(nn.Module):\n",
    "    \"\"\"Fast R-CNN（不含特征提取部分）。\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_channels,\n",
    "                 num_layers,\n",
    "                 roi_output_size,\n",
    "                 downsample_rate,\n",
    "                 num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ROI Pooling层的输出特征图大小\n",
    "        self.roi_output_size = roi_output_size\n",
    "        # 类别数\n",
    "        self.num_classes = num_classes\n",
    "        # backbone CNN 下采样率\n",
    "        self.conv_downsample_rate = downsample_rate\n",
    "        \n",
    "        # Fast R-CNN独有的卷积层\n",
    "        self.conv = [CBR(in_channels, hidden_channels), ]\n",
    "        for i in range(1, num_layers):\n",
    "            self.conv.append(CBR(hidden_channels, hidden_channels))\n",
    "        self.conv.append(nn.Flatten())\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        # 分类分支，每个候选框输出 num_classes + 1个置信度\n",
    "        self.branch_cls = nn.Linear(hidden_channels * self.roi_output_size**2, num_classes + 1)\n",
    "        # 回归分支，每个候选框输出 4 * num_classes个偏移系数\n",
    "        self.branch_reg = nn.Linear(hidden_channels * self.roi_output_size**2, 4 * num_classes)\n",
    "        \n",
    "        # 权重初始化\n",
    "        init_weight(self.conv)\n",
    "        init_weight(self.branch_reg)\n",
    "        init_weight(self.branch_cls)\n",
    "        \n",
    "    def forward(self, feature_map, proposals):\n",
    "        \"\"\"输入backbone CNN输出的特征图和一堆候选框，Fast R-CNN输出每个候选框的类别置信度和相对于候选框的偏移系数\"\"\"\n",
    "        # torchvision.ops.roi_pool输入候选框的格式要求为[N, 5]，每行第一个数为该框\n",
    "        # 对应的特征图编号，后四个数为xyxy格式的候选框坐标。\n",
    "        proposals = torch.concat([torch.zeros(proposals.shape[0], 1, device=proposals.device), proposals], dim=-1)\n",
    "        rois = torchvision.ops.roi_pool(feature_map, proposals, self.roi_output_size, 1/self.conv_downsample_rate)\n",
    "        feature = self.conv(rois)\n",
    "        category_confidence = self.branch_cls(feature)\n",
    "        category_shift = self.branch_reg(feature)\n",
    "        # shape of category_confidence: [num_proposals, num_classes + 1]\n",
    "        # shape of category_shift: [num_proposals, num_classes, 4]\n",
    "        return category_confidence, category_shift.reshape(-1, self.num_classes, 4)\n",
    "\n",
    "    def generate_training_data(self,\n",
    "                               proposal,\n",
    "                               category_confidence,\n",
    "                               category_shift,\n",
    "                               gtbox_labels,\n",
    "                               gtbox_coords,\n",
    "                               pos_threshold=0.5,\n",
    "                               neg_threshold=(0.1, 0.5),\n",
    "                               pos_ratio=0.5,\n",
    "                               batch_size=64):\n",
    "        \"\"\"生成训练数据\"\"\"\n",
    "        # 匹配候选框和真实边界框\n",
    "        pos_proposal_indices, pos_gtbox_indices, neg_proposal_indices = \\\n",
    "            assign_pred_to_gt(proposal,\n",
    "                              gtbox_coords,\n",
    "                              pos_threshold,\n",
    "                              neg_threshold,\n",
    "                              allow_low_quality_matches=True)\n",
    "        # 采样，生成训练数据\n",
    "        pos_reg_target, (cls_logits, cls_target), pos_proposal_indices = \\\n",
    "            random_sampling(pos_proposal_indices,\n",
    "                            pos_gtbox_indices,\n",
    "                            neg_proposal_indices,\n",
    "                            proposal,\n",
    "                            category_confidence,\n",
    "                            gtbox_coords,\n",
    "                            gtbox_labels,\n",
    "                            neg_cls_target=self.num_classes,\n",
    "                            pos_ratio=pos_ratio,\n",
    "                            batch_size=batch_size)\n",
    "        # 正样本对应的真实边界框的类别编号\n",
    "        pos_cls_target = cls_target[:pos_proposal_indices.shape[0]]\n",
    "        # 确保正样本的分类目标非背景\n",
    "        assert (pos_cls_target == self.num_classes).sum() == 0\n",
    "        return pos_reg_target, (cls_logits, cls_target), pos_proposal_indices, pos_cls_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02995d50-abc8-4fbd-8b90-e0688c722fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    \"\"\"Faster RCNN\"\"\"\n",
    "    def __init__(self, rpn_backbone, frcn_backbone, cfg):\n",
    "        super().__init__()\n",
    "        # Fast R-CNN所用的backbone CNN\n",
    "        self.frcn_backbone = frcn_backbone\n",
    "        # RPN所用的backbone CNN\n",
    "        self.rpn_backbone = rpn_backbone\n",
    "        # 类别数量\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.FRCN = Fast_RCNN(in_channels=self.frcn_backbone(torch.zeros(1, 3, 64, 64)).shape[1],\n",
    "                              hidden_channels=cfg.frcn_hidden_channels,\n",
    "                              num_layers=cfg.frcn_num_layers,\n",
    "                              roi_output_size=cfg.frcn_roi_output_size,\n",
    "                              downsample_rate=cfg.downsample_rate,\n",
    "                              num_classes=self.num_classes)\n",
    "        self.RPN = RPN(in_channels=self.rpn_backbone(torch.zeros(1, 3, 64, 64)).shape[1],\n",
    "                       hidden_channels=cfg.rpn_hidden_channels,\n",
    "                       num_layers=cfg.rpn_num_layers,\n",
    "                       area=cfg.anchor_area,\n",
    "                       ratio=cfg.anchor_ratio,\n",
    "                       downsample_rate=cfg.downsample_rate)\n",
    "        # 记录RPN和Fast R-CNN 两部分能否用同一个backbone CNN输出的特征图作为输入\n",
    "        # 需要手动维护\n",
    "        self.same_backbone = None\n",
    "        \n",
    "    def forward(self, input, proposal_iou_threshold):\n",
    "        \"\"\"Faster R-CNN的前向传播\"\"\"\n",
    "        # 从[rpn_backbone, RPN]处拿到rpn_backbone输出的特征图和候选框\n",
    "        conv_rpn_out, proposal, _ = self.get_proposal(input, proposal_iou_threshold)\n",
    "        # 如果 RPN 和 Fast R-CNN 使用的backbone CNN相同的话，仅需抽取一次特征即可\n",
    "        if self.same_backbone:\n",
    "            conv_frcn_out = conv_rpn_out\n",
    "        # 训练时二者的 backbone 可能不同，需要抽取两次特征\n",
    "        else:\n",
    "            conv_frcn_out = self.frcn_backbone(input)\n",
    "        # Fast R-CNN根据特征图计算各个候选框的类别置信度和偏移系数\n",
    "        confidence, shift = self.FRCN(conv_frcn_out, proposal)\n",
    "        return proposal, confidence, shift\n",
    "    \n",
    "    def get_prediction(self,\n",
    "                       input,\n",
    "                       proposal_iou_threshold,\n",
    "                       confidence_threshold,\n",
    "                       prediction_iou_threshold):\n",
    "        \"\"\"\n",
    "        输入图片，输出预测框和类别置信度\n",
    "        Args:\n",
    "            input (Tensor): 输入图片。\n",
    "            proposal_iou_threshold (float): 作用在候选框上的NMS的IOU阈值。\n",
    "            confidence_threshold (float): 置信度阈值，最高置信度低于此值的预测框将被丢弃。\n",
    "            prediction_iou_threshold (float): 作用在预测框上的NMS的IOU阈值。\n",
    "        \"\"\"\n",
    "        # 前向传播，得到候选框、各个候选框的类别置信度和偏移系数\n",
    "        with torch.no_grad():\n",
    "            proposal, confidence, shift = self.forward(input, proposal_iou_threshold)\n",
    "        confidence = confidence.softmax(dim=1)\n",
    "        # 置信度最高的类的置信度和类别编号\n",
    "        max_values, max_indices = confidence.max(dim=1)\n",
    "        # 非背景类且最高置信度大于置信度阈值confidence_threshold\n",
    "        not_background = (max_indices != self.num_classes) & (max_values >= confidence_threshold)\n",
    "        # 非背景类且置信度高于阈值的候选框的编号\n",
    "        selected_proposal_indices = torch.arange(0, confidence.shape[0],\n",
    "                                                 device=confidence.device)[not_background]\n",
    "        selected_proposals = proposal[selected_proposal_indices]\n",
    "        selected_category = max_indices[selected_proposal_indices]\n",
    "        selected_confidence = max_values[selected_proposal_indices]\n",
    "        # 取出各候选框对应置信度最高的类别的偏移系数\n",
    "        selected_shift = shift[selected_proposal_indices, selected_category, :]\n",
    "        # 使用偏移系数对满足条件的候选框进行修正，得到预测框\n",
    "        prediction = cxcywh2xyxy(refine_box(xyxy2cxcywh(selected_proposals), selected_shift))\n",
    "        # 对预测框应用类内NMS消除冗余预测框，得到被保留下来的预测框的编号\n",
    "        remained = batched_nms(prediction.float(),\n",
    "                               selected_confidence,\n",
    "                               selected_category,\n",
    "                               prediction_iou_threshold)\n",
    "        return prediction[remained], selected_category[remained], selected_confidence[remained]\n",
    "    \n",
    "    def get_proposal(self, input, proposal_iou_threshold=1.0):\n",
    "        \"\"\"使用RPN提出候选框，并对候选框应用NMS，消除冗余候选框\"\"\"\n",
    "        with torch.no_grad():\n",
    "            conv_rpn_out = self.rpn_backbone(input)\n",
    "            proposal, objectness = self.RPN.get_proposal(conv_rpn_out)\n",
    "        remained = torchvision.ops.nms(proposal.float(), objectness, proposal_iou_threshold)\n",
    "        return conv_rpn_out, proposal[remained], objectness[remained]\n",
    "    \n",
    "    def load_params(self, version):\n",
    "        \"\"\"加载权重\"\"\"\n",
    "        self.rpn_backbone.load_state_dict(torch.load(f'models/{version}_rpn_backbone.pth'))\n",
    "        self.RPN.load_state_dict(torch.load(f'models/{version}_rpn.pth'))\n",
    "        self.frcn_backbone.load_state_dict(torch.load(f'models/{version}_frcn_backbone.pth'))\n",
    "        self.FRCN.load_state_dict(torch.load(f'models/{version}_frcn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ec0c4b-af80-45b2-a1fd-1aab85439fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxes(image, box1=None, box2=None, display=True, scale=2.0):\n",
    "    \"\"\"将box1和box2分别用红色和绿色显示在图片上\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if image.dim() == 4:\n",
    "            image = image.squeeze(0)\n",
    "        image = image.clone()\n",
    "        # 反归一化\n",
    "        image *= torch.tensor([0.2391, 0.2351, 0.2397], device=image.device).reshape(3, 1, 1)\n",
    "        image += torch.tensor([0.4570, 0.4382, 0.4062], device=image.device).reshape(3, 1, 1)\n",
    "        # 按指定倍率缩放\n",
    "        image = T.Resize(int(scale * min(image.shape[-1], image.shape[-2])))(image)\n",
    "        image = T.ToPILImage()(image)\n",
    "    image = np.array(image)\n",
    "    if box2 is not None:\n",
    "        box2 = (box2 * scale).int()\n",
    "        for box in box2:\n",
    "            cv2.rectangle(image,\n",
    "                          (box[0].item(), box[1].item()),\n",
    "                          (box[2].item(), box[3].item()),\n",
    "                          (0, 255, 0), int(2*scale))\n",
    "    if box1 is not None:\n",
    "        box1 = (box1 * scale).int()\n",
    "        for box in box1:\n",
    "            cv2.rectangle(image,\n",
    "                          (box[0].item(), box[1].item()),\n",
    "                          (box[2].item(), box[3].item()),\n",
    "                          (255, 0, 0), int(1*scale))\n",
    "            cv2.circle(image,\n",
    "                       ((box[0].item()+box[2].item())//2,\n",
    "                        (box[1].item()+box[3].item())//2),\n",
    "                       int(1*scale), (128, 128, 255), -1)\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 10), dpi=int(60*scale))\n",
    "        plt.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c05f5c0-cf15-4b9a-a2df-4a3042e3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(net,\n",
    "                     data,\n",
    "                     proposal_iou_threshold,\n",
    "                     confidence_threshold,\n",
    "                     prediction_iou_threshold,\n",
    "                     display=True,\n",
    "                     scale=2.0):\n",
    "    \"\"\"\n",
    "    给定模型和数据，应用前向传播，得到预测框，并将预测框、对应类别和置信度\n",
    "    和真实边界框一同显示在图片上。\n",
    "    \"\"\"\n",
    "    image, (box_labels, box_coords_xyxy) = data\n",
    "    image = image.to(device)\n",
    "    box_labels = box_labels.to(device)\n",
    "    box_coords_xyxy = box_coords_xyxy.to(device)\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    else:\n",
    "        box_labels = box_labels.squeeze(0)\n",
    "        box_coords_xyxy = box_coords_xyxy.squeeze(0)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net.get_prediction(image,\n",
    "                                    proposal_iou_threshold,\n",
    "                                    confidence_threshold,\n",
    "                                    prediction_iou_threshold)\n",
    "    label_text = ['person',\n",
    "                  'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "                  'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "                  'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "    if output[0].numel() != 0:\n",
    "        image = show_boxes(image, output[0], box_coords_xyxy, display=False, scale=scale)\n",
    "        for prediction, category, confidence in zip(*output):\n",
    "            # 将类别和置信度显示在预测框左上角上方\n",
    "            text_pos = (prediction * scale)[:2]\n",
    "            text_pos[1] -= scale * 2\n",
    "            text_pos = text_pos.int().cpu().numpy()\n",
    "            cv2.putText(image, f'{label_text[category]} {confidence.item():.2f}',\n",
    "                        text_pos,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.4 * scale,\n",
    "                        (255, 0, 0),\n",
    "                        round(1 * scale))\n",
    "    else:\n",
    "        # 没预测框就不画\n",
    "        image = show_boxes(image, None, box_coords_xyxy, display=False, scale=scale)\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 10), dpi=int(60*scale))\n",
    "        plt.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a085ac-278a-4597-9a70-5152480a46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rpn_one_step(net,\n",
    "                       data,\n",
    "                       cfg,\n",
    "                       optimizer,\n",
    "                       criterion_reg,\n",
    "                       criterion_cls):\n",
    "    \"\"\"进行一次前向传播和一次反向传播以训练RPN\"\"\"\n",
    "    image, (box_labels, box_coords_xyxy) = data\n",
    "    image = image.to(device)\n",
    "    box_labels = box_labels.to(device).squeeze(0)\n",
    "    box_coords_xyxy = box_coords_xyxy.to(device)\n",
    "    if box_coords_xyxy.dim() == 3:\n",
    "        box_coords_xyxy = box_coords_xyxy.squeeze(0)\n",
    "    # 前向传播\n",
    "    feature_map = net.rpn_backbone(image)\n",
    "    logits, shift = net.RPN(feature_map)\n",
    "    # 利用前向传播的结果生成训练数据\n",
    "    anchor_xyxy, pos_reg_target, (cls_logits, cls_target), pos_indices = \\\n",
    "        net.RPN.generate_training_data(feature_map,\n",
    "                                       logits,\n",
    "                                       box_coords_xyxy,\n",
    "                                       pos_threshold=cfg.rpn_pos_threshold,\n",
    "                                       neg_threshold=cfg.rpn_neg_threshold,\n",
    "                                       pos_ratio=cfg.rpn_pos_ratio,\n",
    "                                       batch_size=cfg.rpn_batch_size)\n",
    "    # 计算分类和回归分支的Loss\n",
    "    loss_reg = criterion_reg(shift[pos_indices], pos_reg_target)\n",
    "    loss_cls = criterion_cls(cls_logits, cls_target)\n",
    "    # 使用一个系数对分类和回归分支的Loss进行加权求和\n",
    "    loss = loss_reg + cfg.rpn_alpha * loss_cls\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    return loss_reg.item(), loss_cls.item(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd4424a-aae6-460b-b1c5-c7c58c789b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rpn(net, cfg, stage, lr, num_epochs, update_backbone=True):\n",
    "    # 预测框回归Loss：平滑L1Loss\n",
    "    criterion_reg = nn.SmoothL1Loss()\n",
    "    # Objectness回归Loss：交叉熵\n",
    "    # 由于正样本相对于负样本来说数量较少，\n",
    "    # 因此需要对正样本的Loss乘以一个大于1的权重以平衡二者对损失函数的影响\n",
    "    criterion_cls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(cfg.rpn_pos_weight))\n",
    "    # 若要连带着backbone CNN一起训练，则需将backbone CNN的参数也传递给优化器\n",
    "    if update_backbone:\n",
    "        optimizer = torch.optim.SGD(nn.ModuleList([net.rpn_backbone, net.RPN]).parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.rpn_weight_decay, momentum=0.9)\n",
    "    # 否则仅需将RPN独有的层的参数传递给优化器\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(net.RPN.parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.rpn_weight_decay, momentum=0.9)\n",
    "    # 余弦退火学习率\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    voc_train = PascalVOC2012(train=True, scale_ratio=cfg.scale_ratio)\n",
    "    voc_train_dataloader = torch.utils.data.DataLoader(voc_train,\n",
    "                                                       batch_size=1,\n",
    "                                                       shuffle=True,\n",
    "                                                       num_workers=cfg.num_workers)\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg.version}/RPN/{stage}')\n",
    "    freeze(net)\n",
    "    net.eval()\n",
    "    # 仅解锁网络需要更新的部分的参数\n",
    "    unfreeze(net.RPN)\n",
    "    net.RPN.train()\n",
    "    if update_backbone:\n",
    "        unfreeze(net.rpn_backbone)\n",
    "        net.rpn_backbone.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        for i, data in enumerate(voc_train_dataloader):\n",
    "            # 迭代一步\n",
    "            loss_reg, loss_cls, loss = train_rpn_one_step(net,\n",
    "                                                          data,\n",
    "                                                          cfg,\n",
    "                                                          optimizer,\n",
    "                                                          criterion_reg,\n",
    "                                                          criterion_cls)\n",
    "            if (global_step+1) % 5 == 0:\n",
    "                writer.add_scalars('train/loss', {'reg': loss_reg, \n",
    "                                                  'cls': loss_cls,\n",
    "                                                  'weighted sum': loss}, global_step=global_step)\n",
    "            global_step += 1\n",
    "            epoch_loss.append(loss)\n",
    "            \n",
    "            # 打印Loss，画图\n",
    "            if (global_step+1) % (len(voc_train) // 5) == 0:\n",
    "                moving_average = epoch_loss[-(len(voc_train) // 5):]\n",
    "                print(f'epoch {epoch+1:4d}, iter {global_step+1:8d}, loss={sum(moving_average) / len(moving_average):8.4f}')\n",
    "                \n",
    "                image, (box_labels, box_coords_xyxy) = data\n",
    "                image = image.to(device)\n",
    "                box_labels = box_labels.to(device).squeeze(0)\n",
    "                if box_coords_xyxy.dim() == 3:\n",
    "                    box_coords_xyxy = box_coords_xyxy.squeeze(0)\n",
    "                with torch.no_grad():\n",
    "                    _, proposal, objectness = net.get_proposal(image, proposal_iou_threshold=0.8)\n",
    "                proposal = proposal[objectness > 0.6]\n",
    "                image_with_proposals = show_boxes(image, proposal, box_coords_xyxy, display=False)\n",
    "                writer.add_image('train/images_with_proposals',\n",
    "                                 image_with_proposals,\n",
    "                                 global_step=global_step,\n",
    "                                 dataformats='HWC')\n",
    "        scheduler.step()\n",
    "        # 每个epoch将参数导出至硬盘\n",
    "        torch.save(net.rpn_backbone.state_dict(), f'models/{cfg.version}_rpn_backbone.pth')\n",
    "        torch.save(net.RPN.state_dict(), f'models/{cfg.version}_rpn.pth')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd27f52-d3d4-46f6-a970-183f738ec7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_frcn_one_step(net,\n",
    "                        data,\n",
    "                        cfg,\n",
    "                        optimizer,\n",
    "                        criterion_reg,\n",
    "                        criterion_cls):\n",
    "    \"\"\"进行一次前向传播和一次反向传播以训练Fast R-CNN\"\"\"\n",
    "    image, (box_labels, box_coords_xyxy) = data\n",
    "    image = image.to(device)\n",
    "    box_labels = box_labels.to(device).squeeze(0)\n",
    "    box_coords_xyxy = box_coords_xyxy.to(device)\n",
    "    if box_coords_xyxy.dim() == 3:\n",
    "        box_coords_xyxy = box_coords_xyxy.squeeze(0)\n",
    "    # 利用RPN提出候选框，无需计算梯度\n",
    "    with torch.no_grad():\n",
    "        _, proposal, objectness = net.get_proposal(image, proposal_iou_threshold=1.0)\n",
    "    # 前向传播\n",
    "    feature_map = net.frcn_backbone(image)\n",
    "    category_confidence, category_shift = net.FRCN(feature_map, proposal)\n",
    "    # 生成训练数据\n",
    "    pos_reg_target, (cls_logits, cls_target), pos_proposal_indices, pos_cls_target = \\\n",
    "        net.FRCN.generate_training_data(proposal,\n",
    "                                        category_confidence,\n",
    "                                        category_shift,\n",
    "                                        box_labels,\n",
    "                                        box_coords_xyxy,\n",
    "                                        pos_threshold=cfg.frcn_pos_threshold,\n",
    "                                        neg_threshold=cfg.frcn_neg_threshold,\n",
    "                                        pos_ratio=cfg.frcn_pos_ratio,\n",
    "                                        batch_size=cfg.frcn_batch_size)\n",
    "    # 计算回归分支的Loss\n",
    "    loss_reg = criterion_reg(category_shift[pos_proposal_indices, pos_cls_target, :], pos_reg_target)\n",
    "    # 计算分类分支的Loss\n",
    "    loss_cls = criterion_cls(cls_logits, cls_target)\n",
    "    # 加权求和以平衡两个分支的Loss对于梯度的影响\n",
    "    loss = loss_reg + cfg.frcn_alpha * loss_cls\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    return loss_reg.item(), loss_cls.item(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42a7f3ea-dce8-4404-b5cf-afea29a3321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_frcn(net, cfg, stage, lr, num_epochs, update_backbone=True):\n",
    "    criterion_reg = nn.SmoothL1Loss()\n",
    "    # 按正负样本比例1:1时计算的各个类别样本占比的倒数\n",
    "    # 进行一定平滑处理后当做各个类别的分类权重\n",
    "    weight = torch.tensor([ 3.1429, 26.6453, 25.9015, 44.4338, 20.5391, 41.8409, 30.9902,\n",
    "                           33.5617,  8.4732, 31.0512, 49.7603, 13.2443, 42.0640, 48.2385,\n",
    "                           21.0601, 10.8264, 42.2895, 28.3196, 39.5338, 38.2864,  1.0000],\n",
    "                          device=device) / 10 + 0.9\n",
    "    # 也可以对所有正类样本加一个统一的权重\n",
    "    # weight = torch.ones(cfg.num_classes + 1, device=device)\n",
    "    # weight[:cfg.num_classes] *= cfg.frcn_pos_weight\n",
    "    print(weight)\n",
    "    criterion_cls = nn.CrossEntropyLoss(weight=weight)\n",
    "    if update_backbone:\n",
    "        optimizer = torch.optim.SGD(nn.ModuleList([net.frcn_backbone, net.FRCN]).parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.frcn_weight_decay, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(net.FRCN.parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.frcn_weight_decay, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    voc_train = PascalVOC2012(train=True, scale_ratio=cfg.scale_ratio)\n",
    "    voc_train_dataloader = torch.utils.data.DataLoader(voc_train,\n",
    "                                                       batch_size=1,\n",
    "                                                       shuffle=True,\n",
    "                                                       num_workers=cfg.num_workers)\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg.version}/FRCN/{stage}')\n",
    "    \n",
    "    freeze(net)\n",
    "    net.eval()\n",
    "    unfreeze(net.FRCN)\n",
    "    net.FRCN.train()\n",
    "    if update_backbone:\n",
    "        unfreeze(net.frcn_backbone)\n",
    "        net.frcn_backbone.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        for i, data in enumerate(voc_train_dataloader):\n",
    "            loss_reg, loss_cls, loss = train_frcn_one_step(net,\n",
    "                                                           data,\n",
    "                                                           cfg,\n",
    "                                                           optimizer,\n",
    "                                                           criterion_reg,\n",
    "                                                           criterion_cls)\n",
    "            if (global_step+1) % 5 == 0:\n",
    "                writer.add_scalars('train/loss', {'reg': loss_reg, \n",
    "                                                  'cls': loss_cls,\n",
    "                                                  'weighted sum': loss}, global_step=global_step)\n",
    "            global_step += 1\n",
    "            if str(loss) != 'nan':\n",
    "                epoch_loss.append(loss)\n",
    "            if (global_step+1) % (len(voc_train) // 5) == 0:\n",
    "                moving_average = epoch_loss[-(len(voc_train) // 5):]\n",
    "                print(f'epoch {epoch+1:4d}, iter {global_step+1:8d}, loss={sum(moving_average) / len(moving_average):8.4f}')\n",
    "                \n",
    "                images_with_predictions = show_predictions(net,\n",
    "                                                           data,\n",
    "                                                           proposal_iou_threshold=0.6,\n",
    "                                                           confidence_threshold=0.6,\n",
    "                                                           prediction_iou_threshold=0.6,\n",
    "                                                           display=False,\n",
    "                                                           scale=2.0)\n",
    "                writer.add_image('train/images_with_predictions',\n",
    "                                 images_with_predictions,\n",
    "                                 global_step=global_step,\n",
    "                                 dataformats='HWC')\n",
    "        scheduler.step()\n",
    "        torch.save(net.frcn_backbone.state_dict(), f'models/{cfg.version}_frcn_backbone.pth')\n",
    "        torch.save(net.FRCN.state_dict(), f'models/{cfg.version}_frcn.pth')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0afbd0d5-7da1-458c-bf19-792578fe3de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, save=False):\n",
    "        self.version = 'version 14'\n",
    "        # 使用的backboneCNN\n",
    "        self.backbone = 'resnet50'\n",
    "        # 数据增强手段\n",
    "        self.augmentation = 'ColorJitter, RandomHorizontalFlip'\n",
    "        # backboneCNN的下采样率（所有层步长的乘积）\n",
    "        self.downsample_rate = 32\n",
    "        self.num_classes = 20\n",
    "        # 数据集的图片放大倍率\n",
    "        self.scale_ratio = 1.0\n",
    "        self.num_workers = 0\n",
    "\n",
    "        ################# RPN ARCHITECTURE HYPERPARAMETERS #################\n",
    "        # 锚框面积\n",
    "        self.anchor_area = [32**2, 64**2, 128**2, 256**2, 512**2]\n",
    "        # 锚框宽高比\n",
    "        self.anchor_ratio = [2., 1., 0.5]\n",
    "        # RPN内卷积层输出通道数\n",
    "        self.rpn_hidden_channels = 256\n",
    "        # RPN内卷积层个数\n",
    "        self.rpn_num_layers = 4\n",
    "\n",
    "        ################# FAST R-CNN ARCHITECTURE HYPERPARAMETERS #################\n",
    "        # ROI Pooling对于每个候选框的输出特征图大小\n",
    "        self.frcn_roi_output_size = 5\n",
    "        # Fast R-CNN内卷积层通道数\n",
    "        self.frcn_hidden_channels = 256\n",
    "        # Fast R-CNN内卷积层个数\n",
    "        self.frcn_num_layers = 5\n",
    "\n",
    "        ################# RPN TRAINING HYPERPARAMETERS #################\n",
    "        # 四阶段训练中第一、三阶段的学习率\n",
    "        self.rpn_lr_stage_1 = 1e-3\n",
    "        self.rpn_lr_stage_3 = 1e-4\n",
    "        # 权重衰减\n",
    "        self.rpn_weight_decay = 1e-5\n",
    "        # 四阶段训练中第一、三阶段的训练轮次\n",
    "        self.rpn_num_epochs_stage_1 = 25\n",
    "        self.rpn_num_epochs_stage_3 = 10\n",
    "        # 训练RPN时与分类分支的Loss相乘的trade off项\n",
    "        self.rpn_alpha = 0.02\n",
    "        # 正样本Loss增益\n",
    "        self.rpn_pos_weight = 6.5\n",
    "        # 正、负样本IOU阈值\n",
    "        self.rpn_pos_threshold = 0.6\n",
    "        self.rpn_neg_threshold = 0.3\n",
    "        # 小批量内正样本占比上限\n",
    "        self.rpn_pos_ratio = 1/2\n",
    "        # 批量大小\n",
    "        self.rpn_batch_size = 256\n",
    "\n",
    "        ################# FAST R-CNN TRAINING HYPERPARAMETERS #################\n",
    "        self.frcn_lr_stage_2 = 3e-3\n",
    "        self.frcn_lr_stage_4 = 1e-4\n",
    "        self.frcn_weight_decay = 1e-5\n",
    "        self.frcn_num_epochs_stage_2 = 20\n",
    "        self.frcn_num_epochs_stage_4 = 10\n",
    "        self.frcn_alpha = 0.01\n",
    "        self.frcn_pos_weight = 4.5\n",
    "        self.frcn_pos_threshold = 0.5\n",
    "        self.frcn_neg_threshold = (0.1, 0.5)\n",
    "        self.frcn_pos_ratio = 1/2\n",
    "        self.frcn_batch_size = 64\n",
    "\n",
    "        if save:\n",
    "            self.save_config()\n",
    "            \n",
    "    def __str__(self):\n",
    "        return self.version\n",
    "    \n",
    "    def save_config(self):\n",
    "        with open('configs.txt', 'a') as f:\n",
    "            f.write('{\\n')\n",
    "            for k, v in self.__dict__.items():\n",
    "                f.write(k + ': ' + str(v) + \"\\n\")\n",
    "            f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d956e47-860f-41e7-857e-7ffe39d1c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Configuration(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6799cd-c47d-4a94-aa85-89f77df70c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.save_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4c4464-cd80-4459-9067-bae136dc22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_backbone():\n",
    "    modules = models.resnet50(pretrained=True)._modules\n",
    "    backbone = nn.Sequential(*[modules[key] for key in list(modules.keys())[:-2]])\n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93a25395-ebeb-41f6-bbcf-3bbcc92548f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPN(\n",
      "  (conv): Sequential(\n",
      "    (0): CBR(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (branch_reg): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (branch_cls): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Fast_RCNN(\n",
      "  (conv): Sequential(\n",
      "    (0): CBR(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): CBR(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (branch_cls): Linear(in_features=6400, out_features=21, bias=True)\n",
      "  (branch_reg): Linear(in_features=6400, out_features=80, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rpn_backbone = get_resnet_backbone()\n",
    "frcn_backbone = get_resnet_backbone()\n",
    "net = FasterRCNN(rpn_backbone, frcn_backbone, cfg).to(device)\n",
    "net.same_backbone = False\n",
    "print(net.RPN)\n",
    "print(net.FRCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db15267-aacf-49e3-aaf3-cea35589fd32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1, iter     1143, loss=  0.0433\n",
      "epoch    1, iter     2286, loss=  0.0369\n",
      "epoch    1, iter     3429, loss=  0.0345\n",
      "epoch    1, iter     4572, loss=  0.0330\n",
      "epoch    1, iter     5715, loss=  0.0323\n",
      "epoch    2, iter     6858, loss=  0.0291\n",
      "epoch    2, iter     8001, loss=  0.0287\n",
      "epoch    2, iter     9144, loss=  0.0276\n",
      "epoch    2, iter    10287, loss=  0.0271\n",
      "epoch    2, iter    11430, loss=  0.0255\n",
      "epoch    3, iter    12573, loss=  0.0229\n",
      "epoch    3, iter    13716, loss=  0.0234\n",
      "epoch    3, iter    14859, loss=  0.0227\n",
      "epoch    3, iter    16002, loss=  0.0232\n",
      "epoch    3, iter    17145, loss=  0.0229\n",
      "epoch    4, iter    18288, loss=  0.0202\n",
      "epoch    4, iter    19431, loss=  0.0198\n",
      "epoch    4, iter    20574, loss=  0.0202\n",
      "epoch    4, iter    21717, loss=  0.0206\n",
      "epoch    4, iter    22860, loss=  0.0192\n",
      "epoch    5, iter    24003, loss=  0.0178\n",
      "epoch    5, iter    25146, loss=  0.0175\n",
      "epoch    5, iter    26289, loss=  0.0185\n",
      "epoch    5, iter    27432, loss=  0.0181\n",
      "epoch    5, iter    28575, loss=  0.0178\n",
      "epoch    6, iter    29718, loss=  0.0165\n",
      "epoch    6, iter    30861, loss=  0.0167\n",
      "epoch    6, iter    32004, loss=  0.0162\n",
      "epoch    6, iter    33147, loss=  0.0158\n",
      "epoch    6, iter    34290, loss=  0.0162\n",
      "epoch    7, iter    35433, loss=  0.0156\n",
      "epoch    7, iter    36576, loss=  0.0153\n",
      "epoch    7, iter    37719, loss=  0.0149\n",
      "epoch    7, iter    38862, loss=  0.0147\n",
      "epoch    7, iter    40005, loss=  0.0147\n",
      "epoch    8, iter    41148, loss=  0.0140\n",
      "epoch    8, iter    42291, loss=  0.0134\n",
      "epoch    8, iter    43434, loss=  0.0137\n",
      "epoch    8, iter    44577, loss=  0.0141\n",
      "epoch    8, iter    45720, loss=  0.0144\n",
      "epoch    9, iter    46863, loss=  0.0132\n",
      "epoch    9, iter    48006, loss=  0.0131\n",
      "epoch    9, iter    49149, loss=  0.0131\n",
      "epoch    9, iter    50292, loss=  0.0129\n",
      "epoch    9, iter    51435, loss=  0.0130\n",
      "epoch   10, iter    52578, loss=  0.0124\n",
      "epoch   10, iter    53721, loss=  0.0121\n",
      "epoch   10, iter    54864, loss=  0.0121\n",
      "epoch   10, iter    56007, loss=  0.0125\n",
      "epoch   10, iter    57150, loss=  0.0125\n"
     ]
    }
   ],
   "source": [
    "\"\"\"第一阶段，RPN和rpn_backbone参与训练\"\"\"\n",
    "train_rpn(net,\n",
    "          cfg,\n",
    "          'stage1',\n",
    "          cfg.rpn_lr_stage_1,\n",
    "          num_epochs=cfg.rpn_num_epochs_stage_1,\n",
    "          update_backbone=True)\n",
    "# 训练完毕后，rpn_backbone 的参数发生改变，与frcn_backbone不同\n",
    "net.same_backbone = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93292d-02a1-4f96-9a4a-d8a1505b6ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"第二阶段，Fast R-CNN和frcn_backbone参与训练\"\"\"\n",
    "train_frcn(net,\n",
    "           cfg,\n",
    "           'stage2',\n",
    "           cfg.frcn_lr_stage_2,\n",
    "           num_epochs=cfg.frcn_num_epochs_stage_2,\n",
    "           update_backbone=True)\n",
    "# 训练完毕后，frcn_backbone 的参数也发生改变，但仍与rpn_backbone不同\n",
    "net.same_backbone = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4dd81-520d-425b-9529-697937d76bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.load_params(cfg.version)\n",
    "# 将rpn_backbone的参数替换为frcn_backbone的参数\n",
    "net.rpn_backbone.load_state_dict(torch.load(f'models/{cfg.version}_frcn_backbone.pth'))\n",
    "# 此时两个backbone的参数相同\n",
    "net.same_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741554ab-7b43-4298-9a17-3638b5612c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"第三阶段，仅更新RPN的参数\"\"\"\n",
    "train_rpn(net,\n",
    "          cfg,\n",
    "          'stage3',\n",
    "          cfg.rpn_lr_stage_3,\n",
    "          num_epochs=cfg.rpn_num_epochs_stage_3,\n",
    "          update_backbone=False)\n",
    "# 由于rpn_backbone的参数固定不变，因此训练完毕后两个backbone参数仍然相同\n",
    "net.same_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffd259-90a9-49e1-bc54-7f500a63e72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"第四阶段，仅更新Fast R-CNN的参数\"\"\"\n",
    "train_frcn(net,\n",
    "           cfg,\n",
    "           'stage4',\n",
    "           cfg.frcn_lr_stage_4,\n",
    "           num_epochs=cfg.frcn_num_epochs_stage_4,\n",
    "           update_backbone=False)\n",
    "# 由于frcn_backbone的参数固定不变，因此训练完毕后两个backbone参数仍然相同\n",
    "net.same_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6a678-d55f-4274-ae4b-837ebbd78cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_params(cfg.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d41cb96-fc78-4a05-83b9-5461abfc7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dataset = PascalVOC2012(False)\n",
    "for i in range(len(voc_dataset)):\n",
    "    data = voc_dataset[i]\n",
    "    img = show_predictions(net,\n",
    "                           data,\n",
    "                           proposal_iou_threshold=0.3,\n",
    "                           confidence_threshold=0.8,\n",
    "                           prediction_iou_threshold=0.5,\n",
    "                           scale=2.,\n",
    "                           display=False)\n",
    "    plt.imsave(f'./outputs/{cfg.version}/{i}.jpg', img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
