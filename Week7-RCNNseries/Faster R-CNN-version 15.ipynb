{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f50890-76d8-4f53-a396-6dd398035abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0a741d-c1f8-4ae5-8bac-788bb9d7006d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PascalVOC(torch.utils.data.Dataset):\n",
    "    \"\"\"PASCAL VOC 2007 + 2012 数据集\"\"\"\n",
    "    def __init__(self, train=True, image_sizes=None):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        self.data07 = torchvision.datasets.VOCDetection(root='../data',\n",
    "                                                      year='2007',\n",
    "                                                      image_set='train' if train else 'val',\n",
    "                                                      download=False)\n",
    "        self.data12 = torchvision.datasets.VOCDetection(root='../data',\n",
    "                                                      year='2012',\n",
    "                                                      image_set='train' if train else 'val',\n",
    "                                                      download=False)\n",
    "        self.trans_train = T.Compose([T.ToTensor(),\n",
    "                                      T.ColorJitter(brightness=0.2,\n",
    "                                                    contrast=0.2,\n",
    "                                                    saturation=0.2,\n",
    "                                                    hue=0.1),\n",
    "                                      T.Normalize(mean=[0.4541, 0.4336, 0.4016],\n",
    "                                                   std=[0.2396, 0.2349, 0.2390],)])\n",
    "        self.trans_valid = T.Compose([T.ToTensor(),\n",
    "                                      T.Normalize(mean=[0.4541, 0.4336, 0.4016],\n",
    "                                                   std=[0.2396, 0.2349, 0.2390],)])\n",
    "        self.cls_labels = ['person',\n",
    "                           'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "                           'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "                           'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "        if image_sizes is not None:\n",
    "            self.img_sizes = image_sizes\n",
    "        else:\n",
    "            self.img_sizes = [i * 32 + 320 for i in range(10)]\n",
    "        self.current_shape = None\n",
    "        self.random_size()\n",
    "        assert self.current_shape is not None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data07) + len(self.data12)\n",
    "    \n",
    "    def random_size(self):\n",
    "        self.current_shape = self.img_sizes[random.randint(0, len(self.img_sizes) - 1)]\n",
    "        return self.current_shape\n",
    "    \n",
    "    def Resize(self, image, box_coords, size):\n",
    "        if isinstance(size, (int, float)):\n",
    "            size = (int(size), int(size))\n",
    "        h, w = image.size[1], image.size[0]\n",
    "        resize_ratio = (size[0] / w, size[1] / h)\n",
    "        image = T.Resize(size)(image)\n",
    "        box_coords[:, 0::2] = (box_coords[:, 0::2] * resize_ratio[0]).int()\n",
    "        box_coords[:, 1::2] = (box_coords[:, 1::2] * resize_ratio[1]).int()\n",
    "        return image, box_coords\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data07 if index < len(self.data07) else self.data12\n",
    "        index = index if index < len(self.data07) else index - len(self.data07)\n",
    "        image = data[index][0]\n",
    "        box_labels, box_coords = self.get_label_list(data[index][1])\n",
    "        if self.train:\n",
    "            image, box_coords = self.Resize(image, box_coords, self.current_shape)\n",
    "            image, box_coords = self.RandomHorizontalFlip(image, box_coords)\n",
    "            image = self.trans_train(image)\n",
    "        else:\n",
    "            image, box_coords = self.Resize(image, box_coords, 416)\n",
    "            image = self.trans_valid(image)\n",
    "        return image, torch.cat((torch.zeros_like(box_labels, dtype=int),\n",
    "                                    box_labels, box_coords), dim=1)\n",
    "    \n",
    "    def get_label_list(self, label):\n",
    "        obj_list = label['annotation']['object']\n",
    "        box_labels = [self.cls_labels.index(obj['name'] if type(obj['name']) == str else obj['name'][0]) for obj in obj_list]\n",
    "        box_coords = []\n",
    "        for obj in obj_list:\n",
    "            coord = []\n",
    "            for k in ['xmin', 'ymin', 'xmax', 'ymax']:\n",
    "                v = obj['bndbox'][k]\n",
    "                coord.append(int(v if type(v) == str else v[0]))\n",
    "            box_coords.append(coord)\n",
    "        return (torch.tensor(box_labels)[:, None], torch.tensor(box_coords))\n",
    "\n",
    "    def RandomHorizontalFlip(self, image, box_coords):\n",
    "        if random.random() > 0.5:\n",
    "            w = image.size[0]\n",
    "            image = T.RandomHorizontalFlip(p=1)(image)\n",
    "            x1, x2 = box_coords[:, 0], box_coords[:, 2]\n",
    "            box_coords[:, 0], box_coords[:, 2] = w - x2, w - x1\n",
    "        return image, box_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9a020a-24cc-4fc2-9609-0d051cb9b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    image, labels = zip(*batch)\n",
    "    image = torch.stack(image, 0)\n",
    "    for i, label in enumerate(labels):\n",
    "        label[:, 0] = i\n",
    "    return image, torch.cat(labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba16247b-0a1c-4099-a439-54eef034e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = PascalVOC(train=True)\n",
    "voc_val = PascalVOC(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289652b9-3f68-41c1-9261-826849b803a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_box(box_cxcywh, shift):\n",
    "    \"\"\"使用偏移系数修正锚框/候选框，输入输出皆为cxcywh格式\"\"\"\n",
    "    box = box_cxcywh.to(shift.device)\n",
    "    p_cx = box[:, 2] * shift[:, 0] + box[:, 0]\n",
    "    p_cy = box[:, 3] * shift[:, 1] + box[:, 1]\n",
    "    p_w = box[:, 2] * torch.exp(shift[:, 2])\n",
    "    p_h = box[:, 3] * torch.exp(shift[:, 3])\n",
    "    return torch.stack([p_cx, p_cy, p_w, p_h], dim=1)\n",
    "\n",
    "def coord_to_shift(src_cxcywh, tgt_cxcywh):\n",
    "    \"\"\"使用源框和目标框计算从源框到目标框的偏移系数\"\"\"\n",
    "    assert src_cxcywh.shape == tgt_cxcywh.shape\n",
    "    t_x = (tgt_cxcywh[:, 0] - src_cxcywh[:, 0]) / src_cxcywh[:, 2]\n",
    "    t_y = (tgt_cxcywh[:, 1] - src_cxcywh[:, 1]) / src_cxcywh[:, 3]\n",
    "    t_w = torch.log(tgt_cxcywh[:, 2] / src_cxcywh[:, 2])\n",
    "    t_h = torch.log(tgt_cxcywh[:, 3] / src_cxcywh[:, 3])\n",
    "    return torch.stack([t_x, t_y, t_w, t_h], dim=1)\n",
    "\n",
    "# 边界框格式转换\n",
    "def cxcywh2xyxy(boxes):\n",
    "    return torchvision.ops.box_convert(boxes, 'cxcywh', 'xyxy').int()\n",
    "def xyxy2cxcywh(boxes):\n",
    "    return torchvision.ops.box_convert(boxes, 'xyxy', 'cxcywh').int()\n",
    "\n",
    "# 固定/解除固定模型参数\n",
    "def freeze(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(False)\n",
    "def unfreeze(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "# \n",
    "def batched_nms(boxes, # 预测框集合[N, 4]\n",
    "                scores, # 预测框对应类别的置信度\n",
    "                idxs, # 预测框对应类别\n",
    "                iou_thres # IOU阈值，与置信度最高的预测框的IOU高于此阈值的同类别预测框会被丢弃\n",
    "                ):\n",
    "    \"\"\"\n",
    "    逐类进行非极大值抑制\n",
    "    Args:\n",
    "        boxes (Tensor): 预测框集合，形状为[N, 4]\n",
    "        scores (Tensor): 预测框对应类别的置信度，形状为[N]\n",
    "        idxs (Tensor): 预测框对应类别，形状为[N]\n",
    "        iou_thres (float): IOU阈值，与置信度最高的预测框的IOU高于此阈值的同类别预测框会被丢弃\n",
    "    \"\"\"\n",
    "    keep_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "    for class_id in torch.unique(idxs):\n",
    "        curr_indices = torch.where(idxs == class_id)[0]\n",
    "        curr_keep_indices = torchvision.ops.nms(boxes[curr_indices], scores[curr_indices], iou_thres)\n",
    "        keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "    keep_indices = torch.where(keep_mask)[0]\n",
    "    return keep_indices[scores[keep_indices].sort(descending=True)[1]]\n",
    "\n",
    "def init_weight(module):\n",
    "    \"\"\"递归初始化模型参数\"\"\"\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.normal_(module.weight, std=0.01)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.Sequential, nn.ModuleList)):\n",
    "        for m in module:\n",
    "            init_weight(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68234d31-49ff-486a-a8b1-78cc49507872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_pred_to_gt(pred,\n",
    "                      gt,\n",
    "                      pos_thres,\n",
    "                      neg_thres,\n",
    "                      allow_low_quality_matches=True):\n",
    "    \"\"\"\n",
    "    按阈值将锚框/候选框分为正、负样本，并将正样本分配给真实边界框\n",
    "    Args:\n",
    "        pred (Tensor): 待分配的锚框，xyxy格式，形状为[N, 4]。\n",
    "        gt (Tensor): 真实边界框，xyxy格式，形状为[M, 4]。\n",
    "        pos_thres (float): 正样本IOU阈值，与某真实边界框IOU超过此阈值的\n",
    "            锚框/候选框被标记为正样本。\n",
    "        neg_thres (float, tuple): 负样本IOU阈值，与所有真实边界框最高IOU\n",
    "            在此区间或低于此值的锚框/候选框被标记为正样本。\n",
    "        allow_low_quality_matches (bool): 允许低IOU的匹配，保证每个真实边界框\n",
    "            都存在至少一个锚框/候选框与之对应。\n",
    "    \"\"\"\n",
    "    iou_table = torchvision.ops.box_iou(pred, gt)\n",
    "    pos_pred_indices, pos_gt_indices = [], []\n",
    "    max_values, max_indices = iou_table.max(dim=1)\n",
    "    positive = max_values > pos_thres\n",
    "    pos_pred_indices.append(torch.arange(0, pred.shape[0], 1, device=pred.device)[positive])\n",
    "    pos_gt_indices.append(max_indices[positive])\n",
    "    iou_table[positive] = -1 # 防止该锚框/候选框再次被选中成为正/负样本\n",
    "    \n",
    "    if allow_low_quality_matches:\n",
    "        for i in range(gt.shape[0]): # 遍历所有真实边界框\n",
    "            argmax = iou_table[:, i].argmax().reshape(1)\n",
    "            if iou_table[argmax, i] > 0: # 已被选中过的锚框/候选框不参与本轮分配\n",
    "                pos_pred_indices.append(argmax)\n",
    "                pos_gt_indices.append(torch.tensor([i], device=pred.device))\n",
    "                iou_table[argmax] = -1\n",
    "    max_values, max_indices = iou_table.max(dim=1)\n",
    "    if isinstance(neg_thres, float): # 若负样本IOU阈值为一小数\n",
    "        negative = (max_values < neg_thres) & (max_values > 0.)\n",
    "    elif isinstance(neg_thres, tuple): # 若负样本IOU阈值为一区间\n",
    "        negative = (max_values < neg_thres[1]) & (max_values > neg_thres[0])\n",
    "    neg_pred_indices = torch.arange(0, pred.shape[0], 1, device=pred.device)[negative]\n",
    "    \n",
    "    pos_pred_indices = torch.concat(pos_pred_indices)\n",
    "    pos_gt_indices = torch.concat(pos_gt_indices)\n",
    "    assert pos_pred_indices.unique().shape == pos_pred_indices.shape,\\\n",
    "        'there is at least one predicted box assigned to multiple ground truth boxes'\n",
    "    return pos_pred_indices, pos_gt_indices, neg_pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e44e37e0-bfb1-45a3-96ee-9c8f7fee874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(pos_pred_indices,\n",
    "                    pos_gt_indices,\n",
    "                    neg_pred_indices,\n",
    "                    pred_xyxy,\n",
    "                    pred_conf,\n",
    "                    gt_xyxy,\n",
    "                    gt_label,\n",
    "                    neg_cls_target,\n",
    "                    pos_ratio,\n",
    "                    num_samples_per_image):\n",
    "    \"\"\"\n",
    "    对assign_pred_to_gt分配的结果（正负样本集）按指定正样本比例和批量大小进行采样，\n",
    "    Args:\n",
    "        pos_pred_indices (Tensor): 被选中成为正样本的锚框/预测框的下标。\n",
    "        pos_gt_indices (Tensor): 各个成为正样本的锚框/预测框对应的真实边界框。\n",
    "        neg_pred_indices (Tensor): 被选中成为负样本的锚框/预测框的下标。\n",
    "        pred_xyxy (Tensor): 全部候选框/锚框。\n",
    "        pred_conf (Tensor): 网络在各个预测框/锚框（在各个类别上）的回归输出。\n",
    "        gt_xyxy (Tensor): 真实边界框，xyxy格式。\n",
    "        gt_label (Tensor): 各个真实边界框所属的类别编号。\n",
    "        neg_cls_target (Tensor): 负样本对应的类别（背景类）编号。\n",
    "        pos_ratio (float): 一个小批量中正样本占比上限。\n",
    "        num_samples_per_image (int): 每张图片的样本个数。\n",
    "    \"\"\"\n",
    "    # 采样正样本\n",
    "    pos_indices = [i for i in range(len(pos_pred_indices))]\n",
    "    random.shuffle(pos_indices)\n",
    "    pos_indices = pos_indices[:int(num_samples_per_image * pos_ratio)]\n",
    "    \n",
    "    # 采样负样本\n",
    "    neg_indices = [i for i in range(len(neg_pred_indices))]\n",
    "    random.shuffle(neg_indices)\n",
    "    # 正样本不够就用负样本补全 batch\n",
    "    neg_indices = neg_indices[:max(num_samples_per_image-len(pos_indices),\n",
    "                                   int(num_samples_per_image * (1 - pos_ratio)))]\n",
    "    \n",
    "    pos_pred_indices = pos_pred_indices[pos_indices]\n",
    "    pos_gt_indices = pos_gt_indices[pos_indices]\n",
    "    neg_pred_indices = neg_pred_indices[neg_indices]\n",
    "    \n",
    "    # 被选中成为正样本的锚框/候选框到其对应的真实边界框的偏移系数就是网络的回归目标\n",
    "    pos_reg_target = coord_to_shift(xyxy2cxcywh(pred_xyxy)[pos_pred_indices],\n",
    "                                    xyxy2cxcywh(gt_xyxy)[pos_gt_indices])\n",
    "    \n",
    "    # 网络在正负样本上输出的（各个类别的）置信度\n",
    "    pos_cls_conf = pred_conf[pos_pred_indices]\n",
    "    neg_cls_conf = pred_conf[neg_pred_indices]\n",
    "    \n",
    "    # 各个正样本对应的真实边界框的类别编号\n",
    "    pos_cls_target = gt_label[pos_gt_indices]\n",
    "    \n",
    "    # 背景类类别编号\n",
    "    neg_cls_target = torch.empty(neg_pred_indices.shape,\n",
    "                                 device=pred_conf.device,\n",
    "                                 dtype=int).fill_(neg_cls_target)\n",
    "    \n",
    "    # 正负样本的类别置信度和类别编号\n",
    "    cls_conf = torch.concat([pos_cls_conf, neg_cls_conf])\n",
    "    cls_target = torch.concat([pos_cls_target, neg_cls_target])\n",
    "    return pos_reg_target, (cls_conf, cls_target), pos_pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9afcd56-5f9d-4b50-86bf-70d32aa3db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        assert input.shape[-2] % 2 == 0 and input.shape[-1] % 2 == 0\n",
    "        four_corners = torch.cat([input[:, :, i::2, j::2] for i in (0, 1) for j in (0, 1)], dim=1)\n",
    "        return four_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b61842-9d06-40b7-a64e-7a8d748453f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBL(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c36df2-48fe-438c-8346-135e7193fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super().__init__()\n",
    "        module_dict = {\n",
    "            'resnet18': (models.resnet18,\n",
    "                         ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                          'layer1', 'layer2', 'layer3'],\n",
    "                         ['layer4']),\n",
    "            'resnet34': (models.resnet34,\n",
    "                         ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                          'layer1', 'layer2', 'layer3'],\n",
    "                         ['layer4']),\n",
    "            'resnet50': (models.resnet50,\n",
    "                         ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                          'layer1', 'layer2', 'layer3'],\n",
    "                         ['layer4']),\n",
    "            'resnet101': (models.resnet101,\n",
    "                          ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                           'layer1', 'layer2', 'layer3'],\n",
    "                          ['layer4']),\n",
    "            'resnet152': (models.resnet152,\n",
    "                          ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                           'layer1', 'layer2', 'layer3'],\n",
    "                          ['layer4']),\n",
    "            'densenet121': (models.densenet121,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2', 'transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'densenet161': (models.densenet161,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2', 'transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'densenet169': (models.densenet169,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2', 'transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'densenet201': (models.densenet201,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2', 'transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'mobilenet_v3_small': (models.mobilenet_v3_small,\n",
    "                                   ['0', '1', '2', '3', '4',\n",
    "                                    '5', '6', '7', '8'],\n",
    "                                   ['9', '10', '11', '12']),\n",
    "            'mobilenet_v3_large': (models.mobilenet_v3_large,\n",
    "                                   ['0', '1', '2', '3', '4', '5', '6',\n",
    "                                    '7', '8', '9', '10', '11', '12'],\n",
    "                                   ['13', '14', '15', '16'])\n",
    "        }\n",
    "        assert backbone_name in list(module_dict.keys())\n",
    "        raw_backbone = module_dict[backbone_name][0](pretrained=True)._modules\n",
    "        if backbone_name[:6] != 'resnet':\n",
    "            raw_backbone = raw_backbone['features']._modules\n",
    "        self.backbone_ds16 = nn.Sequential(*[raw_backbone[key] for key in module_dict[backbone_name][1]])\n",
    "        self.backbone_ds2  = nn.Sequential(*[raw_backbone[key] for key in module_dict[backbone_name][2]])\n",
    "        self.focus = Focus()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.backbone_ds16(input)\n",
    "        out = torch.cat([self.focus(out), self.backbone_ds2(out)], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a87644-b032-41ec-88e2-fd4c8f5cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"区域提议网络（不含特征提取部分）。\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, area, ratio, downsample_rate):\n",
    "        super().__init__()\n",
    "        self.conv = [CBL(in_channels, hidden_channels), ]\n",
    "        for i in range(1, num_layers):\n",
    "            self.conv.append(CBL(hidden_channels, hidden_channels))\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        \n",
    "        self.area = area\n",
    "        self.ratio = ratio\n",
    "        self.num_boxes = len(area) * len(ratio)\n",
    "        self.branch_reg = nn.Conv2d(hidden_channels, self.num_boxes * 4, kernel_size=1, stride=1, padding=0)\n",
    "        self.branch_cls = nn.Conv2d(hidden_channels, self.num_boxes, kernel_size=1, stride=1, padding=0)\n",
    "        self.boxes = [(self.area[i], self.ratio[j]) for i in range(len(self.area)) for j in range(len(self.ratio))]\n",
    "        self.conv_downsample_rate = downsample_rate\n",
    "        \n",
    "        init_weight(self.conv)\n",
    "        init_weight(self.branch_reg)\n",
    "        init_weight(self.branch_cls)\n",
    "    \n",
    "    def forward(self, feature_map):\n",
    "        \"\"\"输入backbone CNN输出的特征图，输出每个位置上每个锚框的物体置信度和4个偏移系数。\"\"\"\n",
    "        n = feature_map.shape[0]\n",
    "        hidden = self.conv(feature_map)\n",
    "        objectness = self.branch_cls(hidden)\n",
    "        shift = self.branch_reg(hidden)\n",
    "        return objectness.permute(0, 2, 3, 1).reshape(n, -1), shift.permute(0, 2, 3, 1).reshape(n, -1, 4)\n",
    "    \n",
    "    def generate_anchor(self, output_size):\n",
    "        \"\"\"给定backbone CNN输出的特征图尺寸[C * H * W]，生成H * W * self.num_boxes个锚框，cxcywh格式。\"\"\"\n",
    "        wh = torch.zeros(self.num_boxes, 2, device=device)\n",
    "        for k, (area, ratio) in enumerate(self.boxes):\n",
    "            # 高宽比ratio = h / w\n",
    "            w = int((area / ratio) ** 0.5)\n",
    "            h = int(w * ratio)\n",
    "            wh[k, 0] = w\n",
    "            wh[k, 1] = h\n",
    "        cx = torch.arange(0, output_size[-1], 1, device=device).reshape(1, output_size[-1], 1, 1)\\\n",
    "            * self.conv_downsample_rate + self.conv_downsample_rate // 2\n",
    "        cy = torch.arange(0, output_size[-2], 1, device=device).reshape(output_size[-2], 1, 1, 1)\\\n",
    "            * self.conv_downsample_rate + self.conv_downsample_rate // 2\n",
    "        anchors = torch.concat([cx.expand(output_size[-2], -1, self.num_boxes, -1),\n",
    "                                cy.expand(-1, output_size[-1], self.num_boxes, -1),\n",
    "                                wh.expand(output_size[-2], output_size[-1], -1, -1)], dim=-1)\n",
    "        return anchors.reshape(-1, 4)\n",
    "    \n",
    "    def get_proposal(self, feature_map, iou_thres=1.0, num_proposals=300):\n",
    "        \"\"\"利用RPN输出的偏移系数和锚框坐标来对锚框进行修正得到候选框\"\"\"\n",
    "        proposals = []\n",
    "        with torch.no_grad():\n",
    "            objectness, shift = self.forward(feature_map)\n",
    "        objectness = torch.sigmoid(objectness)\n",
    "        anchor = self.generate_anchor(feature_map.shape)\n",
    "        for i in range(feature_map.shape[0]):\n",
    "            proposal_cxcywh = refine_box(anchor, shift[i])\n",
    "            proposal_xyxy = cxcywh2xyxy(proposal_cxcywh)\n",
    "            remained = torchvision.ops.nms(proposal_xyxy.float(), objectness[i], iou_thres)\n",
    "            proposal_xyxy = proposal_xyxy[remained[:num_proposals]]\n",
    "            proposals.append(torch.cat([torch.ones(proposal_xyxy.shape[0], 1,\n",
    "                                                   dtype=proposal_xyxy.dtype,\n",
    "                                                   device=proposal_xyxy.device) * i, proposal_xyxy], dim=1))\n",
    "        return torch.cat(proposals, dim=0)\n",
    "\n",
    "    def generate_training_data(self,\n",
    "                               feature_map,\n",
    "                               pred,\n",
    "                               labels,\n",
    "                               pos_thres,\n",
    "                               neg_thres,\n",
    "                               pos_ratio=0.5,\n",
    "                               num_samples_per_image=256):\n",
    "        \"\"\"生成训练数据\"\"\"\n",
    "        reg_outputs, reg_targets = [], []\n",
    "        cls_outputs, cls_targets = [], []\n",
    "        # 生成锚框\n",
    "        anchor_cxcywh = self.generate_anchor(feature_map.shape)\n",
    "        anchor_xyxy = cxcywh2xyxy(anchor_cxcywh)\n",
    "        objectness, shift = pred\n",
    "        for i in range(feature_map.shape[0]):\n",
    "            gt_xyxy = labels[labels[:, 0]==i][:, 2:]\n",
    "            # 匹配锚框和真实边界框\n",
    "            pos_anchor_indices, pos_gtbox_indices, neg_anchor_indices = \\\n",
    "                assign_pred_to_gt(anchor_xyxy,\n",
    "                                  gt_xyxy,\n",
    "                                  pos_thres,\n",
    "                                  neg_thres,\n",
    "                                  allow_low_quality_matches=True)\n",
    "            # 采样，生成训练数据\n",
    "            reg_target, (cls_output, cls_target), pos_anchor_indices = \\\n",
    "                random_sampling(pos_anchor_indices,\n",
    "                                pos_gtbox_indices,\n",
    "                                neg_anchor_indices,\n",
    "                                anchor_xyxy,\n",
    "                                objectness[i],\n",
    "                                gt_xyxy,\n",
    "                                gt_label=torch.ones_like(objectness[i]),\n",
    "                                neg_cls_target=0,\n",
    "                                pos_ratio=pos_ratio,\n",
    "                                num_samples_per_image=num_samples_per_image)\n",
    "            reg_output = shift[i, pos_anchor_indices]\n",
    "            \n",
    "            reg_outputs.append(reg_output)\n",
    "            reg_targets.append(reg_target)\n",
    "            cls_outputs.append(cls_output)\n",
    "            cls_targets.append(cls_target)\n",
    "        \n",
    "        return (torch.cat(reg_outputs, 0), torch.cat(reg_targets, 0)), \\\n",
    "               (torch.cat(cls_outputs),    torch.cat(cls_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d511529-4fd7-419f-b91e-102e29c3885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn = RPN(128, 256, 1, [128**2], [1.], 32)\n",
    "rpn.get_proposal(torch.zeros(2, 128, 7, 7), 1.0, 5)[:, 0].max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7039def1-1428-4162-a8e2-db9bbc9eb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN(nn.Module):\n",
    "    \"\"\"Fast R-CNN（不含特征提取部分）。\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_channels,\n",
    "                 num_layers,\n",
    "                 roi_output_size,\n",
    "                 downsample_rate,\n",
    "                 num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.roi_output_size = roi_output_size\n",
    "        self.num_classes = num_classes\n",
    "        self.conv_downsample_rate = downsample_rate\n",
    "        \n",
    "        self.conv = [CBL(in_channels, hidden_channels), ]\n",
    "        for i in range(1, num_layers):\n",
    "            self.conv.append(CBL(hidden_channels, hidden_channels))\n",
    "        self.conv.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.conv.append(nn.Flatten())\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        self.branch_cls = nn.Linear(hidden_channels, num_classes + 1)\n",
    "        self.branch_reg = nn.Linear(hidden_channels, 4 * num_classes)\n",
    "        # self.branch_cls = nn.Linear(hidden_channels * self.roi_output_size**2, num_classes + 1)\n",
    "        # self.branch_reg = nn.Linear(hidden_channels * self.roi_output_size**2, 4 * num_classes)\n",
    "        \n",
    "        init_weight(self.conv)\n",
    "        init_weight(self.branch_reg)\n",
    "        init_weight(self.branch_cls)\n",
    "        \n",
    "    def forward(self, feature_map, proposals):\n",
    "        \"\"\"输入backbone CNN输出的特征图和一堆候选框，Fast R-CNN输出每个候选框的类别置信度和相对于候选框的偏移系数\"\"\"\n",
    "        rois = torchvision.ops.roi_pool(feature_map, proposals.float(), self.roi_output_size, 1/self.conv_downsample_rate)\n",
    "        feature = self.conv(rois)\n",
    "        category_confidence = self.branch_cls(feature)\n",
    "        category_shift = self.branch_reg(feature)\n",
    "        return category_confidence, category_shift.reshape(-1, self.num_classes, 4)\n",
    "\n",
    "    def generate_training_data(self,\n",
    "                               proposals,\n",
    "                               pred,\n",
    "                               labels,\n",
    "                               pos_thres=0.5,\n",
    "                               neg_thres=(0.1, 0.5),\n",
    "                               pos_ratio=0.5,\n",
    "                               num_samples_per_image=64):\n",
    "        \"\"\"生成训练数据\"\"\"\n",
    "        reg_outputs, reg_targets = [], []\n",
    "        cls_outputs, cls_targets = [], []\n",
    "        category_confidence, category_shift = pred\n",
    "        for i in range(proposals[:, 0].max().item()):\n",
    "            label_mask = labels[:, 0] == i\n",
    "            gtbox_labels, gtbox_coords = labels[label_mask][:, 1], labels[label_mask][:, 2:]\n",
    "            pred_mask = proposals[:, 0] == i\n",
    "            proposal = proposals[pred_mask][:, 1:]\n",
    "            pos_proposal_indices, pos_gtbox_indices, neg_proposal_indices = \\\n",
    "                assign_pred_to_gt(proposal,\n",
    "                                  gtbox_coords,\n",
    "                                  pos_thres,\n",
    "                                  neg_thres,\n",
    "                                  allow_low_quality_matches=True)\n",
    "            assert proposal.shape[0] == category_confidence[pred_mask].shape[0] == category_shift[pred_mask].shape[0]\n",
    "            reg_target, (cls_output, cls_target), pos_proposal_indices = \\\n",
    "                random_sampling(pos_proposal_indices,\n",
    "                                pos_gtbox_indices,\n",
    "                                neg_proposal_indices,\n",
    "                                proposal,\n",
    "                                category_confidence[pred_mask],\n",
    "                                gtbox_coords,\n",
    "                                gtbox_labels,\n",
    "                                neg_cls_target=self.num_classes,\n",
    "                                pos_ratio=pos_ratio,\n",
    "                                num_samples_per_image=num_samples_per_image)\n",
    "            pos_cls_target = cls_target[:pos_proposal_indices.shape[0]]\n",
    "            reg_output = category_shift[pred_mask][pos_proposal_indices, pos_cls_target, :]\n",
    "            assert (pos_cls_target == self.num_classes).sum() == 0\n",
    "            \n",
    "            reg_outputs.append(reg_output)\n",
    "            reg_targets.append(reg_target)\n",
    "            cls_outputs.append(cls_output)\n",
    "            cls_targets.append(cls_target)\n",
    "            \n",
    "        return (torch.cat(reg_outputs, 0), torch.cat(reg_targets, 0)), \\\n",
    "               (torch.cat(cls_outputs, 0), torch.cat(cls_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02995d50-abc8-4fbd-8b90-e0688c722fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    \"\"\"Faster RCNN\"\"\"\n",
    "    def __init__(self, rpn_backbone, frcn_backbone, cfg):\n",
    "        super().__init__()\n",
    "        self.frcn_backbone = frcn_backbone\n",
    "        self.rpn_backbone = rpn_backbone\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.FRCN = Fast_RCNN(in_channels=self.frcn_backbone(torch.zeros(1, 3, 64, 64)).shape[1],\n",
    "                              hidden_channels=cfg.frcn_hidden_channels,\n",
    "                              num_layers=cfg.frcn_num_layers,\n",
    "                              roi_output_size=cfg.frcn_roi_output_size,\n",
    "                              downsample_rate=cfg.downsample_rate,\n",
    "                              num_classes=self.num_classes)\n",
    "        self.RPN = RPN(in_channels=self.rpn_backbone(torch.zeros(1, 3, 64, 64)).shape[1],\n",
    "                       hidden_channels=cfg.rpn_hidden_channels,\n",
    "                       num_layers=cfg.rpn_num_layers,\n",
    "                       area=cfg.anchor_area,\n",
    "                       ratio=cfg.anchor_ratio,\n",
    "                       downsample_rate=cfg.downsample_rate)\n",
    "        self.same_backbone = None\n",
    "        \n",
    "    def forward(self, input, proposal_iou_thres, num_proposals):\n",
    "        \"\"\"Faster R-CNN的前向传播\"\"\"\n",
    "        conv_rpn_out, proposals = self.get_proposal(input, proposal_iou_thres, num_proposals)\n",
    "        if self.same_backbone:\n",
    "            conv_frcn_out = conv_rpn_out\n",
    "        else:\n",
    "            conv_frcn_out = self.frcn_backbone(input)\n",
    "        confidence, shift = self.FRCN(conv_frcn_out, proposals)\n",
    "        return proposals, confidence, shift\n",
    "    \n",
    "    def get_prediction(self,\n",
    "                       input,\n",
    "                       proposal_iou_thres,\n",
    "                       num_proposals,\n",
    "                       confidence_thres,\n",
    "                       prediction_iou_thres):\n",
    "        \"\"\"\n",
    "        输入图片，输出预测框和类别置信度\n",
    "        Args:\n",
    "            input (Tensor): 输入图片。\n",
    "            proposal_iou_thres (float): 作用在候选框上的NMS的IOU阈值。\n",
    "            num_proposals (int): 每张图片最大候选框数量。\n",
    "            confidence_thres (float): 置信度阈值，最高置信度低于此值的预测框将被丢弃。\n",
    "            prediction_iou_thres (float): 作用在预测框上的NMS的IOU阈值。\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            proposals, confidence, shift = self.forward(input, proposal_iou_thres, num_proposals)\n",
    "        confidence = confidence.softmax(dim=1)\n",
    "        max_values, max_indices = confidence.max(dim=1)\n",
    "        not_background = (max_indices != self.num_classes) & (max_values >= confidence_thres)\n",
    "        \n",
    "        for i in range(input.shape[0]):\n",
    "            pred_mask = (proposals[:, 0] == i) & not_background\n",
    "            selected_proposal_indices = torch.where(pred_mask)[0]\n",
    "            selected_proposals = proposals[selected_proposal_indices, 1:]\n",
    "            selected_category = max_indices[selected_proposal_indices]\n",
    "            selected_confidence = max_values[selected_proposal_indices]\n",
    "            selected_shift = shift[selected_proposal_indices, selected_category, :]\n",
    "            pred_box = cxcywh2xyxy(refine_box(xyxy2cxcywh(selected_proposals), selected_shift))\n",
    "            remained = batched_nms(pred_box.float(),\n",
    "                                   selected_confidence,\n",
    "                                   selected_category,\n",
    "                                   prediction_iou_thres)\n",
    "            result = torch.cat([selected_category[remained, None],\n",
    "                                selected_confidence[remained, None],\n",
    "                                pred_box[remained]], dim=1)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def get_proposal(self, input, proposal_iou_thres=1.0, num_proposals=300):\n",
    "        \"\"\"使用RPN提出候选框，并对候选框应用NMS，消除冗余候选框\"\"\"\n",
    "        with torch.no_grad():\n",
    "            conv_rpn_out = self.rpn_backbone(input)\n",
    "            proposals = self.RPN.get_proposal(conv_rpn_out, proposal_iou_thres, num_proposals)\n",
    "        return conv_rpn_out, proposals\n",
    "    \n",
    "    def load_params(self, version):\n",
    "        \"\"\"加载权重\"\"\"\n",
    "        self.rpn_backbone.load_state_dict(torch.load(f'models/{version}_rpn_backbone.pth'))\n",
    "        self.RPN.load_state_dict(torch.load(f'models/{version}_rpn.pth'))\n",
    "        self.frcn_backbone.load_state_dict(torch.load(f'models/{version}_frcn_backbone.pth'))\n",
    "        self.FRCN.load_state_dict(torch.load(f'models/{version}_frcn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ec0c4b-af80-45b2-a1fd-1aab85439fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxes(image, box1=None, box2=None, display=True, scale=2.0):\n",
    "    \"\"\"将box1和box2分别用红色和绿色显示在图片上\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if image.dim() == 4:\n",
    "            image = image.squeeze(0)\n",
    "        image = image.clone()\n",
    "        # 反归一化\n",
    "        image *= torch.tensor([0.2396, 0.2349, 0.2390], device=image.device).reshape(3, 1, 1)\n",
    "        image += torch.tensor([0.4541, 0.4336, 0.4016], device=image.device).reshape(3, 1, 1)\n",
    "        # 按指定倍率缩放\n",
    "        image = T.Resize(int(scale * min(image.shape[-1], image.shape[-2])))(image)\n",
    "        image = T.ToPILImage()(image)\n",
    "    image = np.array(image)\n",
    "    if box2 is not None:\n",
    "        box2 = (box2 * scale).int()\n",
    "        for box in box2:\n",
    "            cv2.rectangle(image,\n",
    "                          (box[0].item(), box[1].item()),\n",
    "                          (box[2].item(), box[3].item()),\n",
    "                          (0, 255, 0), int(2*scale))\n",
    "    if box1 is not None:\n",
    "        box1 = (box1 * scale).int()\n",
    "        for box in box1:\n",
    "            cv2.rectangle(image,\n",
    "                          (box[0].item(), box[1].item()),\n",
    "                          (box[2].item(), box[3].item()),\n",
    "                          (255, 0, 0), int(1*scale))\n",
    "            cv2.circle(image,\n",
    "                       ((box[0].item()+box[2].item())//2,\n",
    "                        (box[1].item()+box[3].item())//2),\n",
    "                       int(1*scale), (128, 128, 255), -1)\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 10), dpi=int(60*scale))\n",
    "        plt.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7f0ac20-0cc6-4901-b326-8c16860b9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(net,\n",
    "                     data,\n",
    "                     proposal_iou_thres,\n",
    "                     num_proposals,\n",
    "                     confidence_thres,\n",
    "                     prediction_iou_thres,\n",
    "                     display=True,\n",
    "                     scale=2.0):\n",
    "    \"\"\"\n",
    "    给定模型和数据，应用前向传播，得到预测框，并将预测框、对应类别和置信度\n",
    "    和真实边界框一同显示在图片上。\n",
    "    \"\"\"\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    if images.dim() == 3:\n",
    "        images = images.unsqueeze(0)\n",
    "    net.eval()\n",
    "    n = images.shape[0]\n",
    "    with torch.no_grad():\n",
    "        preds = net.get_prediction(images,\n",
    "                                   proposal_iou_thres,\n",
    "                                   num_proposals,\n",
    "                                   confidence_thres,\n",
    "                                   prediction_iou_thres)\n",
    "    label_text = ['person',\n",
    "                  'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "                  'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "                  'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        pred = preds[i]\n",
    "        label = labels[labels[:, 0]==i][:, 2:]\n",
    "        if pred.shape[0] != 0:\n",
    "            image = show_boxes(images[i], pred[:, 2:].int(), label, display=False, scale=scale)\n",
    "        else:\n",
    "            image = show_boxes(images[i], None, label, display=False, scale=scale)\n",
    "        for j in range(pred.shape[0]):\n",
    "            category, confidence = int(pred[j, 0]), pred[j, 1].item()\n",
    "            text_pos = pred[j, 2:4] * scale\n",
    "            text_pos[1] -= scale * 2\n",
    "            text_pos = text_pos.int().cpu().numpy()\n",
    "            cv2.putText(image, f'{label_text[category]} {confidence:.2f}',\n",
    "                        text_pos,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.4 * scale,\n",
    "                        (255, 0, 0),\n",
    "                        max(1, round(scale)))\n",
    "        results.append(image)\n",
    "    if display:\n",
    "        for i in range(n):\n",
    "            plt.figure(figsize=(10, 10), dpi=int(60*scale))\n",
    "            plt.imshow(results[i])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a085ac-278a-4597-9a70-5152480a46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rpn_one_step(net,\n",
    "                       data,\n",
    "                       cfg,\n",
    "                       optimizer,\n",
    "                       criterion_reg,\n",
    "                       criterion_cls):\n",
    "    \"\"\"进行一次前向传播和一次反向传播以训练RPN\"\"\"\n",
    "    image, labels = data\n",
    "    image = image.to(device)\n",
    "    labels = labels.to(device)\n",
    "    feature_map = net.rpn_backbone(image)\n",
    "    pred = net.RPN(feature_map)\n",
    "    (reg_outputs, reg_targets), (cls_outputs, cls_targets) = \\\n",
    "        net.RPN.generate_training_data(feature_map,\n",
    "                                       pred,\n",
    "                                       labels,\n",
    "                                       pos_thres=cfg.rpn_pos_thres,\n",
    "                                       neg_thres=cfg.rpn_neg_thres,\n",
    "                                       pos_ratio=cfg.rpn_pos_ratio,\n",
    "                                       num_samples_per_image=cfg.rpn_num_samples_per_image)\n",
    "    loss_reg = criterion_reg(reg_outputs, reg_targets)\n",
    "    loss_cls = criterion_cls(cls_outputs, cls_targets)\n",
    "    loss = loss_reg + cfg.rpn_alpha * loss_cls\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss_reg.item(), loss_cls.item(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cd4424a-aae6-460b-b1c5-c7c58c789b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rpn(net, cfg, stage, lr, num_epochs, update_backbone=True):\n",
    "    criterion_reg = nn.SmoothL1Loss()\n",
    "    criterion_cls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(cfg.rpn_pos_weight))\n",
    "    if update_backbone:\n",
    "        optimizer = torch.optim.SGD(nn.ModuleList([net.rpn_backbone, net.RPN]).parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.rpn_weight_decay, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(net.RPN.parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.rpn_weight_decay, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    voc_train = PascalVOC(train=True, image_sizes=cfg.image_sizes)\n",
    "    voc_train_dataloader = torch.utils.data.DataLoader(voc_train,\n",
    "                                                       batch_size=cfg.rpn_batch_size,\n",
    "                                                       shuffle=True,\n",
    "                                                       collate_fn=collate,\n",
    "                                                       num_workers=cfg.num_workers)\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg.version}/RPN/{stage}')\n",
    "    freeze(net)\n",
    "    net.eval()\n",
    "    unfreeze(net.RPN)\n",
    "    net.RPN.train()\n",
    "    if update_backbone:\n",
    "        unfreeze(net.rpn_backbone)\n",
    "        net.rpn_backbone.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        for i, data in enumerate(voc_train_dataloader):\n",
    "            loss_reg, loss_cls, loss = train_rpn_one_step(net,\n",
    "                                                          data,\n",
    "                                                          cfg,\n",
    "                                                          optimizer,\n",
    "                                                          criterion_reg,\n",
    "                                                          criterion_cls)\n",
    "            if global_step % 10 == 0:\n",
    "                voc_train.random_size()\n",
    "                writer.add_scalars('train/loss', {'reg': loss_reg, \n",
    "                                                  'cls': loss_cls,\n",
    "                                                  'weighted sum': loss}, global_step=global_step)\n",
    "            global_step += 1\n",
    "            epoch_loss.append(loss)\n",
    "            if global_step % int((len(voc_train) / cfg.rpn_batch_size) / 5) == 0:\n",
    "                moving_average = epoch_loss[-int((len(voc_train) / cfg.rpn_batch_size) / 5):]\n",
    "                print(f'epoch {epoch+1:4d}, iter {global_step+1:8d}, loss={sum(moving_average) / len(moving_average):8.4f}')\n",
    "                \n",
    "                image, labels = data\n",
    "                image = image.to(device)\n",
    "                labels = labels.to(device)\n",
    "                box_coords_xyxy = labels[labels[:, 0]==0][:, 2:]\n",
    "                with torch.no_grad():\n",
    "                    _, proposal = net.get_proposal(image[0].unsqueeze(0), proposal_iou_thres=0.8, num_proposals=20)\n",
    "                image_with_proposals = show_boxes(image[0], proposal[:, 1:], box_coords_xyxy, display=False)\n",
    "                writer.add_image('train/images_with_proposals',\n",
    "                                 image_with_proposals,\n",
    "                                 global_step=global_step,\n",
    "                                 dataformats='HWC')\n",
    "        scheduler.step()\n",
    "        torch.save(net.rpn_backbone.state_dict(), f'models/{cfg.version}_rpn_backbone.pth')\n",
    "        torch.save(net.RPN.state_dict(), f'models/{cfg.version}_rpn.pth')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd27f52-d3d4-46f6-a970-183f738ec7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_frcn_one_step(net,\n",
    "                        data,\n",
    "                        cfg,\n",
    "                        optimizer,\n",
    "                        criterion_reg,\n",
    "                        criterion_cls):\n",
    "    image, labels = data\n",
    "    image = image.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        conv_rpn_out, proposal = net.get_proposal(image,\n",
    "                                                  proposal_iou_thres=cfg.rpn_proposal_iou_thres,\n",
    "                                                  num_proposals=cfg.rpn_num_proposals)\n",
    "    if net.same_backbone:\n",
    "        feature_map = conv_rpn_out\n",
    "    else:\n",
    "        feature_map = net.frcn_backbone(image)\n",
    "    pred = net.FRCN(feature_map, proposal)\n",
    "    (reg_outputs, reg_targets), (cls_outputs, cls_targets) = \\\n",
    "        net.FRCN.generate_training_data(proposal,\n",
    "                                        pred,\n",
    "                                        labels,\n",
    "                                        pos_thres=cfg.frcn_pos_thres,\n",
    "                                        neg_thres=cfg.frcn_neg_thres,\n",
    "                                        pos_ratio=cfg.frcn_pos_ratio,\n",
    "                                        num_samples_per_image=cfg.frcn_num_samples_per_image)\n",
    "    loss_reg = criterion_reg(reg_outputs, reg_targets)\n",
    "    loss_cls = criterion_cls(cls_outputs, cls_targets)\n",
    "    loss = loss_reg + cfg.frcn_alpha * loss_cls\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss_reg.item(), loss_cls.item(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a7f3ea-dce8-4404-b5cf-afea29a3321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_frcn(net, cfg, stage, lr, num_epochs, update_backbone=True):\n",
    "    criterion_reg = nn.SmoothL1Loss()\n",
    "    weight = torch.tensor([ 3.1429, 26.6453, 25.9015, 44.4338, 20.5391, 41.8409, 30.9902,\n",
    "                           33.5617,  8.4732, 31.0512, 49.7603, 13.2443, 42.0640, 48.2385,\n",
    "                           21.0601, 10.8264, 42.2895, 28.3196, 39.5338, 38.2864,  1.0000],\n",
    "                          device=device) / 10 + 0.9\n",
    "    # weight = torch.ones(cfg.num_classes + 1, device=device)\n",
    "    # weight[:cfg.num_classes] *= cfg.frcn_pos_weight\n",
    "    print(weight)\n",
    "    criterion_cls = nn.CrossEntropyLoss(weight=weight)\n",
    "    if update_backbone:\n",
    "        optimizer = torch.optim.SGD(nn.ModuleList([net.frcn_backbone, net.FRCN]).parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.frcn_weight_decay, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(net.FRCN.parameters(),\n",
    "                                    lr=lr, weight_decay=cfg.frcn_weight_decay, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    voc_train = PascalVOC(train=True, image_sizes=cfg.image_sizes)\n",
    "    voc_train_dataloader = torch.utils.data.DataLoader(voc_train,\n",
    "                                                       batch_size=cfg.frcn_batch_size,\n",
    "                                                       shuffle=True,\n",
    "                                                       collate_fn=collate,\n",
    "                                                       num_workers=cfg.num_workers)\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg.version}/FRCN/{stage}')\n",
    "    \n",
    "    freeze(net)\n",
    "    net.eval()\n",
    "    unfreeze(net.FRCN)\n",
    "    net.FRCN.train()\n",
    "    if update_backbone:\n",
    "        unfreeze(net.frcn_backbone)\n",
    "        net.frcn_backbone.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        for i, data in enumerate(voc_train_dataloader):\n",
    "            loss_reg, loss_cls, loss = train_frcn_one_step(net,\n",
    "                                                           data,\n",
    "                                                           cfg,\n",
    "                                                           optimizer,\n",
    "                                                           criterion_reg,\n",
    "                                                           criterion_cls)\n",
    "            if global_step % 10 == 0:\n",
    "                voc_train.random_size()\n",
    "                writer.add_scalars('train/loss', {'reg': loss_reg, \n",
    "                                                  'cls': loss_cls,\n",
    "                                                  'weighted sum': loss}, global_step=global_step)\n",
    "            global_step += 1\n",
    "            if str(loss) != 'nan':\n",
    "                epoch_loss.append(loss)\n",
    "            if global_step % int((len(voc_train) / cfg.frcn_batch_size) / 5) == 0:\n",
    "                moving_average = epoch_loss[-int((len(voc_train) / cfg.frcn_batch_size) / 5):]\n",
    "                print(f'epoch {epoch+1:4d}, iter {global_step+1:8d}, loss={sum(moving_average) / len(moving_average):8.4f}')\n",
    "                data = (data[0][0], data[1][data[1][:, 0]==0])\n",
    "                images_with_predictions = show_predictions(net,\n",
    "                                                           data,\n",
    "                                                           proposal_iou_thres=0.6,\n",
    "                                                           num_proposals=300,\n",
    "                                                           confidence_thres=0.6,\n",
    "                                                           prediction_iou_thres=0.6,\n",
    "                                                           display=False,\n",
    "                                                           scale=2.0)\n",
    "                writer.add_image('train/images_with_predictions',\n",
    "                                 images_with_predictions[0],\n",
    "                                 global_step=global_step,\n",
    "                                 dataformats='HWC')\n",
    "        scheduler.step()\n",
    "        torch.save(net.frcn_backbone.state_dict(), f'models/{cfg.version}_frcn_backbone.pth')\n",
    "        torch.save(net.FRCN.state_dict(), f'models/{cfg.version}_frcn.pth')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0afbd0d5-7da1-458c-bf19-792578fe3de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, save=False):\n",
    "        self.version = 'version 15'\n",
    "        # 使用的backboneCNN\n",
    "        self.backbone = 'resnet18'\n",
    "        # 数据增强手段\n",
    "        self.augmentation = 'ColorJitter, RandomHorizontalFlip'\n",
    "        # backboneCNN的下采样率（所有层步长的乘积）\n",
    "        self.downsample_rate = 32\n",
    "        self.num_classes = 20\n",
    "        self.num_workers = 8\n",
    "        self.image_sizes = [i * 32 + 320 for i in range(10)]\n",
    "\n",
    "        ################# RPN ARCHITECTURE HYPERPARAMETERS #################\n",
    "        self.anchor_area = [32**2, 64**2, 128**2, 256**2, 512**2]\n",
    "        self.anchor_ratio = [2., 1., 0.5]\n",
    "        self.rpn_hidden_channels = 256\n",
    "        self.rpn_num_layers = 2\n",
    "\n",
    "        ################# FAST R-CNN ARCHITECTURE HYPERPARAMETERS #################\n",
    "        self.frcn_roi_output_size = 7\n",
    "        self.frcn_hidden_channels = 256\n",
    "        self.frcn_num_layers = 3\n",
    "\n",
    "        ################# RPN TRAINING HYPERPARAMETERS #################\n",
    "        self.rpn_lr_stage_1 = 1e-2\n",
    "        self.rpn_lr_stage_3 = 1e-3\n",
    "        self.rpn_weight_decay = 5e-4\n",
    "        self.rpn_num_epochs_stage_1 = 15\n",
    "        self.rpn_num_epochs_stage_3 = 5\n",
    "        self.rpn_alpha = 0.02\n",
    "        self.rpn_pos_weight = 6.5\n",
    "        self.rpn_pos_thres = 0.6\n",
    "        self.rpn_neg_thres = 0.3\n",
    "        self.rpn_pos_ratio = 1/2\n",
    "        self.rpn_batch_size = 16\n",
    "        self.rpn_num_samples_per_image = 256\n",
    "\n",
    "        ################# FAST R-CNN TRAINING HYPERPARAMETERS #################\n",
    "        self.frcn_lr_stage_2 = 1e-2\n",
    "        self.frcn_lr_stage_4 = 1e-3\n",
    "        self.frcn_weight_decay = 5e-4\n",
    "        self.frcn_num_epochs_stage_2 = 5\n",
    "        self.frcn_num_epochs_stage_4 = 2\n",
    "        self.frcn_alpha = 0.01\n",
    "        self.frcn_pos_weight = 4.5\n",
    "        self.frcn_pos_thres = 0.5\n",
    "        self.frcn_neg_thres = (0.1, 0.5)\n",
    "        self.frcn_pos_ratio = 1/2\n",
    "        self.frcn_batch_size = 2\n",
    "        self.frcn_num_samples_per_image = 64\n",
    "        self.rpn_num_proposals = 300\n",
    "        self.rpn_proposal_iou_thres = 0.8\n",
    "\n",
    "        if save:\n",
    "            self.save_config()\n",
    "            \n",
    "    def __str__(self):\n",
    "        return self.version\n",
    "    \n",
    "    def save_config(self):\n",
    "        with open('configs.txt', 'a') as f:\n",
    "            f.write('{\\n')\n",
    "            for k, v in self.__dict__.items():\n",
    "                f.write(k + ': ' + str(v) + \"\\n\")\n",
    "            f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d956e47-860f-41e7-857e-7ffe39d1c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Configuration(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6799cd-c47d-4a94-aa85-89f77df70c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.save_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93a25395-ebeb-41f6-bbcf-3bbcc92548f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxh0916/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zxh0916/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPN(\n",
      "  (conv): Sequential(\n",
      "    (0): CBL(\n",
      "      (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): CBL(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (branch_reg): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (branch_cls): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Fast_RCNN(\n",
      "  (conv): Sequential(\n",
      "    (0): CBL(\n",
      "      (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): CBL(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): CBL(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (branch_cls): Linear(in_features=256, out_features=21, bias=True)\n",
      "  (branch_reg): Linear(in_features=256, out_features=80, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rpn_backbone = Backbone(cfg.backbone)\n",
    "frcn_backbone = Backbone(cfg.backbone)\n",
    "net = FasterRCNN(rpn_backbone, frcn_backbone, cfg).to(device)\n",
    "net.same_backbone = False\n",
    "print(net.RPN)\n",
    "print(net.FRCN)\n",
    "# net.RPN.load_state_dict(torch.load(f'models/{cfg.version}_rpn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2db15267-aacf-49e3-aaf3-cea35589fd32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1, iter      103, loss=  0.0366\n",
      "epoch    1, iter      205, loss=  0.0330\n",
      "epoch    1, iter      307, loss=  0.0301\n",
      "epoch    1, iter      409, loss=  0.0281\n",
      "epoch    1, iter      511, loss=  0.0279\n",
      "epoch    2, iter      613, loss=  0.0267\n",
      "epoch    2, iter      715, loss=  0.0248\n",
      "epoch    2, iter      817, loss=  0.0241\n",
      "epoch    2, iter      919, loss=  0.0237\n",
      "epoch    2, iter     1021, loss=  0.0226\n",
      "epoch    3, iter     1123, loss=  0.0248\n",
      "epoch    3, iter     1225, loss=  0.0248\n",
      "epoch    3, iter     1327, loss=  0.0231\n",
      "epoch    3, iter     1429, loss=  0.0226\n",
      "epoch    3, iter     1531, loss=  0.0229\n",
      "epoch    4, iter     1633, loss=  0.0193\n",
      "epoch    4, iter     1735, loss=  0.0193\n",
      "epoch    4, iter     1837, loss=  0.0193\n",
      "epoch    4, iter     1939, loss=  0.0189\n",
      "epoch    4, iter     2041, loss=  0.0190\n",
      "epoch    5, iter     2143, loss=  0.0221\n",
      "epoch    5, iter     2245, loss=  0.0205\n",
      "epoch    5, iter     2347, loss=  0.0219\n",
      "epoch    5, iter     2449, loss=  0.0215\n",
      "epoch    5, iter     2551, loss=  0.0207\n",
      "epoch    6, iter     2653, loss=  0.0224\n",
      "epoch    6, iter     2755, loss=  0.0213\n",
      "epoch    6, iter     2857, loss=  0.0216\n",
      "epoch    6, iter     2959, loss=  0.0214\n",
      "epoch    6, iter     3061, loss=  0.0208\n",
      "epoch    7, iter     3163, loss=  0.0179\n",
      "epoch    7, iter     3265, loss=  0.0181\n",
      "epoch    7, iter     3367, loss=  0.0185\n",
      "epoch    7, iter     3469, loss=  0.0178\n",
      "epoch    7, iter     3571, loss=  0.0176\n",
      "epoch    8, iter     3673, loss=  0.0172\n",
      "epoch    8, iter     3775, loss=  0.0172\n",
      "epoch    8, iter     3877, loss=  0.0174\n",
      "epoch    8, iter     3979, loss=  0.0174\n",
      "epoch    8, iter     4081, loss=  0.0170\n",
      "epoch    9, iter     4183, loss=  0.0178\n",
      "epoch    9, iter     4285, loss=  0.0176\n",
      "epoch    9, iter     4387, loss=  0.0178\n",
      "epoch    9, iter     4489, loss=  0.0175\n",
      "epoch    9, iter     4591, loss=  0.0180\n",
      "epoch   10, iter     4693, loss=  0.0177\n",
      "epoch   10, iter     4795, loss=  0.0168\n",
      "epoch   10, iter     4897, loss=  0.0166\n",
      "epoch   10, iter     4999, loss=  0.0164\n",
      "epoch   10, iter     5101, loss=  0.0162\n",
      "epoch   11, iter     5203, loss=  0.0150\n",
      "epoch   11, iter     5305, loss=  0.0149\n",
      "epoch   11, iter     5407, loss=  0.0154\n",
      "epoch   11, iter     5509, loss=  0.0153\n",
      "epoch   11, iter     5611, loss=  0.0149\n",
      "epoch   12, iter     5713, loss=  0.0167\n",
      "epoch   12, iter     5815, loss=  0.0176\n",
      "epoch   12, iter     5917, loss=  0.0174\n",
      "epoch   12, iter     6019, loss=  0.0170\n",
      "epoch   12, iter     6121, loss=  0.0172\n",
      "epoch   13, iter     6223, loss=  0.0197\n",
      "epoch   13, iter     6325, loss=  0.0184\n",
      "epoch   13, iter     6427, loss=  0.0177\n",
      "epoch   13, iter     6529, loss=  0.0180\n",
      "epoch   13, iter     6631, loss=  0.0176\n",
      "epoch   14, iter     6733, loss=  0.0180\n",
      "epoch   14, iter     6835, loss=  0.0170\n",
      "epoch   14, iter     6937, loss=  0.0174\n",
      "epoch   14, iter     7039, loss=  0.0175\n",
      "epoch   14, iter     7141, loss=  0.0175\n",
      "epoch   15, iter     7243, loss=  0.0178\n",
      "epoch   15, iter     7345, loss=  0.0177\n",
      "epoch   15, iter     7447, loss=  0.0180\n",
      "epoch   15, iter     7549, loss=  0.0177\n",
      "epoch   15, iter     7651, loss=  0.0185\n"
     ]
    }
   ],
   "source": [
    "\"\"\"第一阶段，RPN和rpn_backbone参与训练\"\"\"\n",
    "train_rpn(net,\n",
    "          cfg,\n",
    "          'stage1',\n",
    "          cfg.rpn_lr_stage_1,\n",
    "          num_epochs=cfg.rpn_num_epochs_stage_1,\n",
    "          update_backbone=True)\n",
    "net.same_backbone = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d93292d-02a1-4f96-9a4a-d8a1505b6ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2143, 3.5645, 3.4901, 5.3434, 2.9539, 5.0841, 3.9990, 4.2562, 1.7473,\n",
      "        4.0051, 5.8760, 2.2244, 5.1064, 5.7239, 3.0060, 1.9826, 5.1290, 3.7320,\n",
      "        4.8534, 4.7286, 1.0000], device='cuda:0')\n",
      "epoch    1, iter      822, loss=  0.0607\n",
      "epoch    1, iter     1643, loss=  0.0509\n",
      "epoch    1, iter     2464, loss=  0.0538\n",
      "epoch    1, iter     3285, loss=  0.0489\n",
      "epoch    1, iter     4106, loss=  0.0513\n",
      "epoch    2, iter     4927, loss=  0.0501\n",
      "epoch    2, iter     5748, loss=  0.0487\n",
      "epoch    2, iter     6569, loss=  0.0506\n",
      "epoch    2, iter     7390, loss=  0.0467\n",
      "epoch    2, iter     8211, loss=  0.0492\n",
      "epoch    3, iter     9032, loss=  0.0486\n",
      "epoch    3, iter     9853, loss=  0.0483\n",
      "epoch    3, iter    10674, loss=  0.0459\n",
      "epoch    3, iter    11495, loss=  0.0452\n",
      "epoch    3, iter    12316, loss=  0.0457\n",
      "epoch    4, iter    13137, loss=  0.0475\n",
      "epoch    4, iter    13958, loss=  0.0445\n",
      "epoch    4, iter    14779, loss=  0.0470\n",
      "epoch    4, iter    15600, loss=  0.0475\n",
      "epoch    4, iter    16421, loss=  0.0468\n",
      "epoch    5, iter    17242, loss=  0.0472\n",
      "epoch    5, iter    18063, loss=  0.0429\n",
      "epoch    5, iter    18884, loss=  0.0458\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"第二阶段，Fast R-CNN和frcn_backbone参与训练\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_frcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstage2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrcn_lr_stage_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrcn_num_epochs_stage_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43mupdate_backbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m net\u001b[38;5;241m.\u001b[39msame_backbone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mtrain_frcn\u001b[0;34m(net, cfg, stage, lr, num_epochs, update_backbone)\u001b[0m\n\u001b[1;32m     35\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(voc_train_dataloader):\n\u001b[0;32m---> 37\u001b[0m     loss_reg, loss_cls, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_frcn_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mcriterion_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mcriterion_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m         voc_train\u001b[38;5;241m.\u001b[39mrandom_size()\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtrain_frcn_one_step\u001b[0;34m(net, data, cfg, optimizer, criterion_reg, criterion_cls)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, loss_cls\u001b[38;5;241m.\u001b[39mitem(), loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"第二阶段，Fast R-CNN和frcn_backbone参与训练\"\"\"\n",
    "train_frcn(net,\n",
    "           cfg,\n",
    "           'stage2',\n",
    "           cfg.frcn_lr_stage_2,\n",
    "           num_epochs=cfg.frcn_num_epochs_stage_2,\n",
    "           update_backbone=True)\n",
    "net.same_backbone = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4dd81-520d-425b-9529-697937d76bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.load_params(cfg.version)\n",
    "net.rpn_backbone.load_state_dict(torch.load(f'models/{cfg.version}_frcn_backbone.pth'))\n",
    "net.same_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741554ab-7b43-4298-9a17-3638b5612c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"第三阶段，仅更新RPN的参数\"\"\"\n",
    "train_rpn(net,\n",
    "          cfg,\n",
    "          'stage3',\n",
    "          cfg.rpn_lr_stage_3,\n",
    "          num_epochs=cfg.rpn_num_epochs_stage_3,\n",
    "          update_backbone=False)\n",
    "net.same_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffd259-90a9-49e1-bc54-7f500a63e72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"第四阶段，仅更新Fast R-CNN的参数\"\"\"\n",
    "train_frcn(net,\n",
    "           cfg,\n",
    "           'stage4',\n",
    "           cfg.frcn_lr_stage_4,\n",
    "           num_epochs=cfg.frcn_num_epochs_stage_4,\n",
    "           update_backbone=False)\n",
    "net.same_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6a678-d55f-4274-ae4b-837ebbd78cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_params(cfg.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41cb96-fc78-4a05-83b9-5461abfc7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dataset = PascalVOC2012(False)\n",
    "for i in range(len(voc_dataset)):\n",
    "    data = voc_dataset[i]\n",
    "    img = show_predictions(net,\n",
    "                           data,\n",
    "                           proposal_iou_thres=0.3,\n",
    "                           confidence_thres=0.8,\n",
    "                           prediction_iou_thres=0.5,\n",
    "                           scale=2.,\n",
    "                           display=False)\n",
    "    plt.imsave(f'./outputs/{cfg.version}/{i}.jpg', img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
