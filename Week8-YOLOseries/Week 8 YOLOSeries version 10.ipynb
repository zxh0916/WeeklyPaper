{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435bf4f2-dbd9-4e82-9127-c16dde4ef821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ed884b-a6cd-4676-96b5-77437453379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOC(torch.utils.data.Dataset):\n",
    "    \"\"\"PASCAL VOC 2007 + 2012 数据集\"\"\"\n",
    "    def __init__(self, train=True, image_sizes=None, ratio=1.0):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        # PASCAL VOC 2007\n",
    "        self.data07 = torchvision.datasets.VOCDetection(root='../data',\n",
    "                                                        year='2007',\n",
    "                                                        image_set='train' if train else 'val',\n",
    "                                                        download=False)\n",
    "        # PASCAL VOC 2012\n",
    "        self.data12 = torchvision.datasets.VOCDetection(root='../data',\n",
    "                                                        year='2012',\n",
    "                                                        image_set='train' if train else 'val',\n",
    "                                                        download=False)\n",
    "        # 设定要用多少比例的数据，方便使用少量数据调试代码\n",
    "        if ratio != 1.:\n",
    "            size07, size12 = int(len(self.data07) * ratio), int(len(self.data12) * ratio)\n",
    "            self.data07, _ = torch.utils.data.random_split(self.data07, [size07, len(self.data07)-size07])\n",
    "            self.data12, _ = torch.utils.data.random_split(self.data12, [size12, len(self.data12)-size12])\n",
    "        # 类型转换、色彩扰动和归一化\n",
    "        self.trans_train = T.Compose([T.ToTensor(),\n",
    "                                      T.ColorJitter(brightness=0.2,\n",
    "                                                    contrast=0.2,\n",
    "                                                    saturation=0.2,\n",
    "                                                    hue=0.1),\n",
    "                                      T.Normalize(mean=[0.4541, 0.4336, 0.4016],\n",
    "                                                   std=[0.2396, 0.2349, 0.2390],)])\n",
    "        self.trans_valid = T.Compose([T.ToTensor(),\n",
    "                                      T.Normalize(mean=[0.4541, 0.4336, 0.4016],\n",
    "                                                   std=[0.2396, 0.2349, 0.2390],)])\n",
    "        # 标签列表\n",
    "        self.cls_labels = ['person',\n",
    "                           'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "                           'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "                           'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "        # YOLOv2(http://arxiv.org/abs/1612.08242)中提到，为了获得缩放不变性，\n",
    "        # 训练时每10个step，在{320, 352, ..., 608}中随机挑选一个数作为训练图片的尺寸。\n",
    "        if image_sizes is not None:\n",
    "            self.img_sizes = image_sizes\n",
    "        else:\n",
    "            self.img_sizes = [i * 32 + 320 for i in range(10)]\n",
    "        self.current_shape = None\n",
    "        self.random_size()\n",
    "        assert self.current_shape is not None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data07) + len(self.data12)\n",
    "    \n",
    "    def random_size(self):\n",
    "        \"\"\"从尺寸集合中随机挑选一个图片尺寸\"\"\"\n",
    "        if self.train:\n",
    "            self.current_shape = self.img_sizes[random.randint(0, len(self.img_sizes) - 1)]\n",
    "        else:\n",
    "            self.current_shape = 416\n",
    "        return self.current_shape\n",
    "    \n",
    "    def Resize(self, image, box_coords, size):\n",
    "        \"\"\"调整图片和其对应的真实边界框的尺寸\"\"\"\n",
    "        if isinstance(size, (int, float)):\n",
    "            size = (int(size), int(size))\n",
    "        h, w = image.size[1], image.size[0]\n",
    "        resize_ratio = (size[0] / w, size[1] / h)\n",
    "        image = T.Resize(size)(image)\n",
    "        box_coords[:, 0::2] = (box_coords[:, 0::2] * resize_ratio[0]).int()\n",
    "        box_coords[:, 1::2] = (box_coords[:, 1::2] * resize_ratio[1]).int()\n",
    "        return image, box_coords\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 判断是使用07年的数据还是12年的数据\n",
    "        data = self.data07 if index < len(self.data07) else self.data12\n",
    "        index = index if index < len(self.data07) else index - len(self.data07)\n",
    "        image = data[index][0]\n",
    "        box_labels, box_coords = self.get_label_list(data[index][1])\n",
    "        if self.train:\n",
    "            image, box_coords = self.Resize(image, box_coords, self.current_shape)\n",
    "            image, box_coords = self.RandomHorizontalFlip(image, box_coords)\n",
    "            image = self.trans_train(image)\n",
    "        else:\n",
    "            image, box_coords = self.Resize(image, box_coords, 416)\n",
    "            image = self.trans_valid(image)\n",
    "        return image, torch.cat((torch.zeros_like(box_labels, dtype=int),\n",
    "                                 box_labels, box_coords), dim=1)\n",
    "    \n",
    "    def get_label_list(self, label):\n",
    "        \"\"\"获取图片中物体的类别和真实边界框的xyxy坐标\"\"\"\n",
    "        obj_list = label['annotation']['object']\n",
    "        box_labels = [self.cls_labels.index(obj['name'] if type(obj['name']) == str else obj['name'][0]) for obj in obj_list]\n",
    "        box_coords = []\n",
    "        for obj in obj_list:\n",
    "            coord = []\n",
    "            for k in ['xmin', 'ymin', 'xmax', 'ymax']:\n",
    "                v = obj['bndbox'][k]\n",
    "                coord.append(int(v if type(v) == str else v[0]))\n",
    "            box_coords.append(coord)\n",
    "        return (torch.tensor(box_labels)[:, None], torch.tensor(box_coords))\n",
    "\n",
    "    def RandomHorizontalFlip(self, image, box_coords):\n",
    "        \"\"\"随机水平翻转\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            w = image.size[0]\n",
    "            image = T.RandomHorizontalFlip(p=1)(image)\n",
    "            x1, x2 = box_coords[:, 0], box_coords[:, 2]\n",
    "            box_coords[:, 0], box_coords[:, 2] = w - x2, w - x1\n",
    "        return image, box_coords\n",
    "    \n",
    "    def collate(self, batch):\n",
    "        \"\"\"将一个批量的数据整合成两个张量\"\"\"\n",
    "        image, labels = zip(*batch)\n",
    "        image = torch.stack(image, 0)\n",
    "        for i, label in enumerate(labels):\n",
    "            label[:, 0] = i\n",
    "        # 第一个返回值是图片，形状为 [batch_size, C, H, W]\n",
    "        # 第二个返回值是标签，形状为 [batch_size, 6]\n",
    "        # 其中每行的第一个数为这行标签对应的图片样本下标，\n",
    "        # 第二个数为这行标签所对应的物体的类别编号，\n",
    "        # 后四个数为真实边界框的xyxy坐标。\n",
    "        return image, torch.cat(labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f628442-15ff-40f9-8edc-8a6aef5462ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(x):\n",
    "    return -torch.log(torch.pow(torch.clamp(x, 1e-6, 1.-1e-6), -1) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ee8c9-b7c8-499e-8340-a9e3c0a025ed",
   "metadata": {},
   "source": [
    "![预测框解码方式](../pictures/detection-yolov2-box.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a74e3e-cdf7-404d-ac40-6b614d529abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_box(box_cxcywh, shift, downsample_rate=32):\n",
    "    \"\"\"由锚框坐标和网络输出计算预测框\"\"\"\n",
    "    box = box_cxcywh.to(shift.device)\n",
    "    # 上图中默认方形区域的边长为1，而实际上原图上方形区域的边长为该特征图的下采样率\n",
    "    # 故需将坐标计算出来之后乘一个下采样率\n",
    "    p_cx = downsample_rate * (torch.sigmoid(shift[:, 0]) + (box[:, 0] / downsample_rate).floor())\n",
    "    p_cy = downsample_rate * (torch.sigmoid(shift[:, 1]) + (box[:, 1] / downsample_rate).floor())\n",
    "    p_w = box[:, 2] * torch.exp(shift[:, 2])\n",
    "    p_h = box[:, 3] * torch.exp(shift[:, 3])\n",
    "    return torch.stack([p_cx, p_cy, p_w, p_h], dim=1)\n",
    "\n",
    "def coord_to_shift(src_cxcywh, tgt_cxcywh, downsample_rate=32):\n",
    "    \"\"\"由锚框和预测框反算出期望的网络输出\"\"\"\n",
    "    assert src_cxcywh.shape == tgt_cxcywh.shape\n",
    "    t_x = inv_sigmoid(tgt_cxcywh[:, 0] / downsample_rate - (tgt_cxcywh[:, 0] / downsample_rate).floor())\n",
    "    t_y = inv_sigmoid(tgt_cxcywh[:, 1] / downsample_rate - (tgt_cxcywh[:, 1] / downsample_rate).floor())\n",
    "    t_w = torch.log(tgt_cxcywh[:, 2] / src_cxcywh[:, 2])\n",
    "    t_h = torch.log(tgt_cxcywh[:, 3] / src_cxcywh[:, 3])\n",
    "    return torch.stack([t_x, t_y, t_w, t_h], dim=1)\n",
    "\n",
    "# 边界框格式转换\n",
    "def cxcywh2xyxy(boxes_cxcywh):\n",
    "    dim = boxes_cxcywh.dim()\n",
    "    if dim == 1:\n",
    "        boxes_cxcywh = boxes_cxcywh.unsqueeze(0)\n",
    "    boxes_xyxy = torchvision.ops.box_convert(boxes_cxcywh, 'cxcywh', 'xyxy').int()\n",
    "    if dim == 1:\n",
    "        boxes_xyxy = boxes_xyxy.squeeze(0)\n",
    "    return boxes_xyxy\n",
    "def xyxy2cxcywh(boxes_xyxy):\n",
    "    dim = boxes_xyxy.dim()\n",
    "    if dim == 1:\n",
    "        boxes_xyxy = boxes_xyxy.unsqueeze(0)\n",
    "    boxes_cxcywh = torchvision.ops.box_convert(boxes_xyxy, 'xyxy', 'cxcywh').int()\n",
    "    if dim == 1:\n",
    "        boxes_cxcywh = boxes_cxcywh.squeeze(0)\n",
    "    return boxes_cxcywh\n",
    "\n",
    "# 锁定/解锁模型参数\n",
    "def freeze(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(False)\n",
    "def unfreeze(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "# 逐类别非极大值抑制\n",
    "def batched_nms(boxes, scores, idxs, iou_threshold):\n",
    "    keep_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "    for class_id in torch.unique(idxs):\n",
    "        curr_indices = torch.where(idxs == class_id)[0]\n",
    "        curr_keep_indices = torchvision.ops.nms(boxes[curr_indices], scores[curr_indices], iou_threshold)\n",
    "        keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "    keep_indices = torch.where(keep_mask)[0]\n",
    "    return keep_indices[scores[keep_indices].sort(descending=True)[1]]\n",
    "\n",
    "def init_weight(module):\n",
    "    \"\"\"递归初始化模型参数\"\"\"\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.normal_(module.weight, std=0.01)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.Sequential, nn.ModuleList)):\n",
    "        for m in module:\n",
    "            init_weight(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514faa09-0184-4819-abd3-dbdc1ba7f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxes(image, box1=None, box2=None, display=True, scale=2.0):\n",
    "    \"\"\"把框画在图片上\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if image.dim() == 4:\n",
    "            image = image.squeeze(0)\n",
    "        image = image.clone()\n",
    "        image *= torch.tensor([0.2396, 0.2349, 0.2390], device=image.device).reshape(3, 1, 1)\n",
    "        image += torch.tensor([0.4541, 0.4336, 0.4016], device=image.device).reshape(3, 1, 1)\n",
    "        image = T.Resize(int(scale * min(image.shape[-1], image.shape[-2])))(image)\n",
    "        image = T.ToPILImage()(image)\n",
    "    image = np.array(image)\n",
    "    if box2 is not None:\n",
    "        box2 = (box2 * scale).int()\n",
    "        for box in box2:\n",
    "            cv2.rectangle(image,\n",
    "                          (box[0].item(), box[1].item()),\n",
    "                          (box[2].item(), box[3].item()),\n",
    "                          (0, 255, 0), int(2*scale))\n",
    "    if box1 is not None:\n",
    "        box1 = (box1 * scale).int()\n",
    "        for box in box1:\n",
    "            cv2.rectangle(image,\n",
    "                          (box[0].item(), box[1].item()),\n",
    "                          (box[2].item(), box[3].item()),\n",
    "                          (255, 0, 0), int(1*scale))\n",
    "            cv2.circle(image,\n",
    "                       ((box[0].item()+box[2].item())//2,\n",
    "                        (box[1].item()+box[3].item())//2),\n",
    "                       int(1*scale), (128, 128, 255), -1)\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 10), dpi=int(60*scale))\n",
    "        plt.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0311f5-1b57-4f58-b754-a4be66b8cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(net,\n",
    "                     data,\n",
    "                     conf_thres,\n",
    "                     iou_thres,\n",
    "                     display=True,\n",
    "                     scale=2.0):\n",
    "    \"\"\"\n",
    "    给定模型和数据，应用前向传播，得到预测框，并将预测框、对应类别和置信度\n",
    "    和真实边界框一同显示在图片上。\n",
    "    \"\"\"\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    if images.dim() == 3:\n",
    "        images = images.unsqueeze(0)\n",
    "    net.eval()\n",
    "    n = images.shape[0]\n",
    "    with torch.no_grad():\n",
    "        preds = net.get_prediction(images,\n",
    "                                   iou_thres,\n",
    "                                   conf_thres)\n",
    "    label_text = ['person',\n",
    "                  'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "                  'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "                  'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        pred = preds[i]\n",
    "        pred[:, 2::2] = torch.clamp(pred[:, 2::2], 0, images[i].shape[-1])\n",
    "        pred[:, 3::2] = torch.clamp(pred[:, 3::2], 0, images[i].shape[-2])\n",
    "        label = labels[labels[:, 0]==i][:, 2:]\n",
    "        if pred.shape[0] != 0:\n",
    "            image = show_boxes(images[i], pred[:, 2:].int(), label, display=False, scale=scale)\n",
    "        else:\n",
    "            image = show_boxes(images[i], None, label, display=False, scale=scale)\n",
    "        for j in range(pred.shape[0]):\n",
    "            category, confidence = int(pred[j, 0]), pred[j, 1].item()\n",
    "            text_pos = pred[j, 2:4] * scale\n",
    "            text_pos[1] -= scale * 2\n",
    "            text_pos = text_pos.int().cpu().numpy()\n",
    "            cv2.putText(image, f'{label_text[category]} {confidence:.2f}',\n",
    "                        text_pos,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.4 * scale,\n",
    "                        (255, 0, 0),\n",
    "                        max(1, round(scale)))\n",
    "        results.append(image)\n",
    "    if display:\n",
    "        for i in range(n):\n",
    "            plt.figure(figsize=(10, 10), dpi=int(60*scale))\n",
    "            plt.imshow(results[i])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c79c80b-087f-4d8a-8359-6f3bd9a44348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super().__init__()\n",
    "        # YOLO的backbone需要输出三张特征图\n",
    "        # 三张特征图的下采样率分别为8、16和32\n",
    "        # module_dict中每个值都是一个包含四个元素的元组\n",
    "        # 其中第一个元素是使用torchvision的API取backbone的函数\n",
    "        # 后三个元素分别是输出为上述三种特征图的网络的三个部分的模块列表\n",
    "        module_dict = {\n",
    "            'resnet18': (models.resnet18,\n",
    "                         ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                          'layer1', 'layer2'],\n",
    "                         ['layer3'], ['layer4']),\n",
    "            'resnet34': (models.resnet34,\n",
    "                         ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                          'layer1', 'layer2'],\n",
    "                         ['layer3'], ['layer4']),\n",
    "            'resnet50': (models.resnet50,\n",
    "                         ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                          'layer1', 'layer2'],\n",
    "                         ['layer3'], ['layer4']),\n",
    "            'resnet101': (models.resnet101,\n",
    "                          ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                           'layer1', 'layer2'],\n",
    "                          ['layer3'], ['layer4']),\n",
    "            'resnet152': (models.resnet152,\n",
    "                          ['conv1', 'bn1', 'relu', 'maxpool',\n",
    "                           'layer1', 'layer2'],\n",
    "                          ['layer3'], ['layer4']),\n",
    "            'densenet121': (models.densenet121,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2'],\n",
    "                            ['transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'densenet161': (models.densenet161,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2'],\n",
    "                            ['transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'densenet169': (models.densenet169,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2'],\n",
    "                            ['transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'densenet201': (models.densenet201,\n",
    "                            ['conv0', 'norm0', 'relu0', 'pool0',\n",
    "                             'denseblock1', 'transition1', 'denseblock2'],\n",
    "                            ['transition2', 'denseblock3'],\n",
    "                            ['transition3', 'denseblock4', 'norm5']),\n",
    "            'mobilenet_v3_small': (models.mobilenet_v3_small,\n",
    "                                   ['0', '1', '2', '3'],\n",
    "                                   ['4', '5', '6', '7', '8'],\n",
    "                                   ['9', '10', '11', '12']),\n",
    "            'mobilenet_v3_large': (models.mobilenet_v3_large,\n",
    "                                   ['0', '1', '2', '3', '4', '5', '6'],\n",
    "                                   ['7', '8', '9', '10', '11', '12'],\n",
    "                                   ['13', '14', '15', '16'])\n",
    "        }\n",
    "        assert backbone_name in list(module_dict.keys())\n",
    "        raw_backbone = module_dict[backbone_name][0](pretrained=True)._modules\n",
    "        if backbone_name[:6] != 'resnet':\n",
    "            raw_backbone = raw_backbone['features']._modules\n",
    "        self.backbone_ds8  = nn.Sequential(*[raw_backbone[key] for key in module_dict[backbone_name][1]])\n",
    "        self.backbone_ds16 = nn.Sequential(*[raw_backbone[key] for key in module_dict[backbone_name][2]])\n",
    "        self.backbone_ds32 = nn.Sequential(*[raw_backbone[key] for key in module_dict[backbone_name][3]])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"用网络的三个部分依次计算下采样率为8、16和32的特诊图\"\"\"\n",
    "        fmap_s8 = self.backbone_ds8(input)\n",
    "        fmap_s16 = self.backbone_ds16(fmap_s8)\n",
    "        fmap_s32 = self.backbone_ds32(fmap_s16)\n",
    "        return fmap_s8, fmap_s16, fmap_s32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308158af-96fe-4841-b34b-1556e4f713ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        assert input.shape[-2] % 2 == 0 and input.shape[-1] % 2 == 0\n",
    "        reshaped_fmap = torch.cat([input[:, :, i::2, j::2] for i in (0, 1) for j in (0, 1)], dim=1)\n",
    "        return reshaped_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd2d6c6-68a8-4763-8d59-68ea7fe3a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPP(nn.Module):\n",
    "    \"\"\"空间金字塔池化层\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        y1 = F.max_pool2d(x, kernel_size=5,  stride=1, padding=2)\n",
    "        y2 = F.max_pool2d(x, kernel_size=9,  stride=1, padding=4)\n",
    "        y3 = F.max_pool2d(x, kernel_size=13, stride=1, padding=6)\n",
    "        return torch.cat([x, y1, y2, y3], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c3ea061-5740-4d0b-836b-cc20fcad791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBL(nn.Sequential):\n",
    "    \"\"\"网络基本组成模块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels=None, k=3, s=1, p=1):\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=k, stride=s, padding=p, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d73b21e-a780-4ced-9683-cde72263320e",
   "metadata": {},
   "source": [
    "![YOLOv4 网络架构](../pictures/detection-yolov4-archi.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b310f2-21aa-4426-a25c-1a8361f3e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    将语义信息较为丰富的小尺寸特征图\n",
    "    和空间信息较为丰富的大尺寸特征图融合的网络结构，\n",
    "    有利于提升小尺寸物体的检测质量。\n",
    "    \"\"\"\n",
    "    def __init__(self, small_in_channels, big_in_channels, hidden_layers=5, out_channels=256):\n",
    "        super().__init__()\n",
    "        # 对小尺寸特征图进行上采样\n",
    "        self.small_branch = nn.Sequential(\n",
    "            CBL(small_in_channels, out_channels),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU())\n",
    "        # 对大尺寸特征图的通道数进行变换\n",
    "        self.big_branch = CBL(big_in_channels, out_channels, k=1, s=1, p=0)\n",
    "        # 融合拼接后的特征图\n",
    "        self.merge = [CBL(2 * out_channels, out_channels)]\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.merge.append(CBL(out_channels))\n",
    "        self.merge = nn.Sequential(*self.merge)\n",
    "    def forward(self, small, big):\n",
    "        return self.merge(torch.cat([self.small_branch(small), self.big_branch(big)], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f5ab0d-931a-456c-869d-afb72e65f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PANBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    将浅层较为丰富的几何信息再次传递给深层，\n",
    "    进一步增强了网络输出的预测框的精确度。\n",
    "    \"\"\"\n",
    "    def __init__(self, small_in_channels, big_in_channels, hidden_layers=5, out_channels=256):\n",
    "        super().__init__()\n",
    "        # 对大尺寸特征图进行下采样\n",
    "        self.big_branch = CBL(big_in_channels, out_channels, s=2)\n",
    "        # 对小尺寸特征图的通道数进行变换\n",
    "        self.small_branch = CBL(small_in_channels, out_channels, k=1, s=1, p=0)\n",
    "        # 融合拼接后的特征图\n",
    "        self.merge = [CBL(2 * out_channels, out_channels)]\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.merge.append(CBL(out_channels))\n",
    "        self.merge = nn.Sequential(*self.merge)\n",
    "    def forward(self, small, big):\n",
    "        return self.merge(torch.cat([self.small_branch(small), self.big_branch(big)], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eed113be-fe73-47dc-8e37-919ba9082ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Neck(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv4网络结构的Neck部分，\n",
    "    将backbone输出的特征图使用PAN结构进行融合后\n",
    "    送至Head进行预测\n",
    "    \"\"\"\n",
    "    def __init__(self, ds8_outchannels, ds16_outchannels, ds32_outchannels, hidden_layers=5, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.trans_3_4 = nn.Sequential(\n",
    "            CBL(ds32_outchannels, out_channels), CBL(out_channels), CBL(out_channels),\n",
    "            SPP(),\n",
    "            CBL(4 * out_channels, out_channels), CBL(out_channels), CBL(out_channels))\n",
    "        self.trans_42_5 = FPNBlock(out_channels, ds16_outchannels, hidden_layers, out_channels)\n",
    "        self.trans_51_6 = FPNBlock(out_channels, ds8_outchannels, hidden_layers, out_channels)\n",
    "        self.trans_56_7 = PANBlock(out_channels, out_channels, hidden_layers, out_channels)\n",
    "        self.trans_47_8 = PANBlock(out_channels, out_channels, hidden_layers, out_channels)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        fmap_1, fmap_2, fmap_3 = input\n",
    "        fmap_4 = self.trans_3_4(fmap_3)\n",
    "        fmap_5 = self.trans_42_5(fmap_4, fmap_2)\n",
    "        fmap_6 = self.trans_51_6(fmap_5, fmap_1)\n",
    "        fmap_7 = self.trans_56_7(fmap_5, fmap_6)\n",
    "        fmap_8 = self.trans_47_8(fmap_4, fmap_7)\n",
    "        return fmap_6, fmap_7, fmap_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2431ac28-5962-41e7-bafe-410570fb3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"YOLO网络结构中的检测头，三个检测头参数各不相同\"\"\"\n",
    "    def __init__(self, in_channels, num_classes, num_anchors, hidden_layers):\n",
    "        super().__init__()\n",
    "        out_channels = num_anchors * (5 + num_classes)\n",
    "        self.head_big, self.head_mid, self.head_sml = [], [], []\n",
    "        for i in range(hidden_layers):\n",
    "            self.head_big.append(CBL(in_channels))\n",
    "            self.head_mid.append(CBL(in_channels))\n",
    "            self.head_sml.append(CBL(in_channels))\n",
    "        self.head_big.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0))\n",
    "        self.head_mid.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0))\n",
    "        self.head_sml.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0))\n",
    "        self.head_big = nn.Sequential(*self.head_big)\n",
    "        self.head_mid = nn.Sequential(*self.head_mid)\n",
    "        self.head_sml = nn.Sequential(*self.head_sml)\n",
    "    def forward(self, input):\n",
    "        fmap_big, fmap_mid, fmap_sml = input\n",
    "        return self.head_big(fmap_big), self.head_mid(fmap_mid), self.head_sml(fmap_sml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af5d22f-0f96-466e-9124-8ac2c89de1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo(nn.Module):\n",
    "    \"\"\"一个简单的YOLO目标检测模型\"\"\"\n",
    "    def __init__(self, backbone, anchors, num_classes, hidden_channels, neck_hidden_layers, head_hidden_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # 类别数\n",
    "        # 将所有锚框三等分，分别分配给三个检测头\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.anchor_wh = [anchors[0:len(anchors)//3], anchors[len(anchors)//3:-len(anchors)//3], anchors[-len(anchors)//3:]]\n",
    "        self.backbone = Backbone(backbone)\n",
    "        fmap_s8, fmap_s16, fmap_s32 = self.backbone(torch.zeros(1, 3, 64, 64))\n",
    "        self.neck = Neck(fmap_s8.shape[1], fmap_s16.shape[1], fmap_s32.shape[1], neck_hidden_layers, hidden_channels)\n",
    "        self.head = Head(hidden_channels, num_classes, len(anchors) // 3, head_hidden_layers)\n",
    "        init_weight(self.neck)\n",
    "        init_weight(self.head)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        out_big, out_mid, out_sml = self.head(self.neck(self.backbone(input))) # 获取网络输出\n",
    "        objectness, shift, class_conf, anchors_cxcywh = [], [], [], []\n",
    "        # 从每个检测头的输出中分别提取物体评分输出、边界框预测输出和类别概率预测输出\n",
    "        for i, out in enumerate((out_big, out_mid, out_sml)):\n",
    "            out = out.permute(0, 2, 3, 1)\n",
    "            n, h, w, c = out.shape\n",
    "            out = out.reshape(n, h, w, self.num_anchors//3, self.num_classes + 5)\n",
    "            objectness.append(out[:, :, :, :, 0])\n",
    "            shift.append(out[:, :, :, :, 1:5])\n",
    "            class_conf.append(out[:, :, :, :, -self.num_classes:])\n",
    "            # 三个检测头所对应原图上方形区域的边长分别为8、16和32\n",
    "            anchors_cxcywh.append(self.generate_anchor((h, w), self.anchor_wh[i], downsample_rate=8*2**i))\n",
    "        return objectness, shift, class_conf, anchors_cxcywh\n",
    "        \n",
    "    def generate_anchor(self, fmap_size, anchor_wh, downsample_rate):\n",
    "        num_anchors = len(anchor_wh)\n",
    "        img_h, img_w = fmap_size[-1] * downsample_rate, fmap_size[-2] * downsample_rate\n",
    "        # 此处输入的anchor_wh均为锚框的高宽相对于原图高宽的比例，故需与原图高宽相乘\n",
    "        # 锚框的高宽与锚框中心点的位置无关\n",
    "        anchor_wh = torch.tensor([(round(w*img_w), round(h*img_h)) for (w, h) in anchor_wh],\n",
    "                                 device=device).reshape(1, 1, num_anchors, 2)\n",
    "        # 锚框中心点以对应特征图的下采样率为步长均匀分布在整张图片上\n",
    "        cx = torch.arange(0, fmap_size[-1], 1, device=device).reshape(1, fmap_size[-1], 1, 1) \\\n",
    "           * downsample_rate + downsample_rate // 2\n",
    "        cy = torch.arange(0, fmap_size[-2], 1, device=device).reshape(fmap_size[-2], 1, 1, 1) \\\n",
    "           * downsample_rate + downsample_rate // 2\n",
    "        # 将锚框的高宽和中心点坐标拼接起来，形成cxcywh格式\n",
    "        anchor_cxcywh = torch.cat([cx.expand(fmap_size[-2], -1, num_anchors, -1),\n",
    "                                   cy.expand(-1, fmap_size[-1], num_anchors, -1),\n",
    "                                   anchor_wh.expand(fmap_size[-2], fmap_size[-1], -1, -1)], dim=-1)\n",
    "        return anchor_cxcywh\n",
    "    \n",
    "    def get_prediction(self, input, iou_thres=0.4, conf_thres=0.5):\n",
    "        \"\"\"端到端地获取网络的预测输出\"\"\"\n",
    "        if input.dim() == 3:\n",
    "            input = input.unsqueeze(0)\n",
    "        preds = []\n",
    "        # 前向传播\n",
    "        with torch.no_grad():\n",
    "            objectness, shift, class_conf, anchors_cxcywh = self.forward(input)\n",
    "        n = input.shape[0]\n",
    "        # 记录网络各个检测头输出的预测框数量并累加\n",
    "        num_preds = [0] + [objectness[i].shape[1] * objectness[i].shape[2] * objectness[i].shape[3] for i in range(3)]\n",
    "        num_preds_accu = [sum(num_preds[:i+1]) for i in range(len(num_preds))]\n",
    "        # 将三个检测头的输出拼接起来\n",
    "        objectness = torch.cat([obj.reshape(n, -1).unsqueeze(-1) for obj in objectness], dim=1)\n",
    "        shift = torch.cat([sft.reshape(n, -1, 4) for sft in shift], dim=1)\n",
    "        class_conf = torch.cat([cls_conf.reshape(n, -1, self.num_classes) for cls_conf in class_conf], dim=1)\n",
    "        class_conf = torch.sigmoid(class_conf) * torch.sigmoid(objectness) # 置信度等于物体评分与类别概率最大值之乘积\n",
    "        anchors_cxcywh = torch.cat([anchor.reshape(-1, 4) for anchor in anchors_cxcywh], dim=0)\n",
    "        max_conf, max_idx = class_conf.max(dim=-1)\n",
    "        for i in range(n): # 遍历小批量中所有样本\n",
    "            mask = max_conf[i] >= conf_thres # 筛选置信度大于阈值的预测结果\n",
    "            pred_xyxy = []\n",
    "            for j in range(3): # 遍历3个检测头的预测结果\n",
    "                idx = torch.arange(0, sum(num_preds), 1, device=device)\n",
    "                head_mask = (idx >= num_preds_accu[j]) & (idx < num_preds_accu[j+1]) & mask\n",
    "                # 用网络的边界框预测输出对锚框进行修正并转换为xyxy格式\n",
    "                pred_xyxy.append(cxcywh2xyxy(refine_box(anchors_cxcywh[head_mask], shift[i, head_mask], 8*2**j)))\n",
    "            pred_xyxy = torch.cat(pred_xyxy, dim=0) # 拼接三个检测头的输出\n",
    "            # 逐类别非极大值抑制\n",
    "            remains = batched_nms(pred_xyxy.float(), max_conf[i, mask], max_idx[i, mask], iou_thres)\n",
    "            pred_xyxy = pred_xyxy[remains]\n",
    "            remains = torch.where(mask)[0][remains]\n",
    "            pred_conf, pred_idx = max_conf[i, remains], max_idx[i, remains]\n",
    "            # 每条预测结果为一个6维向量：物体类别、置信度和xyxy坐标\n",
    "            pred = torch.cat([pred_idx[:, None], pred_conf[:, None], pred_xyxy], dim=-1)\n",
    "            preds.append(pred)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d59ca4-5bd0-4e81-bef8-eef03862de54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self,\n",
    "                 obj_pos_weight,\n",
    "                 num_classes,\n",
    "                 obj_gain,\n",
    "                 cls_gain,\n",
    "                 reg_gain,\n",
    "                 neg_thres,\n",
    "                 obj_pos_ratio):\n",
    "        self.num_classes = num_classes\n",
    "        if isinstance(obj_pos_weight, (int, float)):\n",
    "            obj_pos_weight = torch.tensor(obj_pos_weight, device=device).float()\n",
    "        # 物体评分和类别概率使用二分类交叉熵作为损失函数\n",
    "        self.criterion_obj = nn.BCEWithLogitsLoss(pos_weight=obj_pos_weight)\n",
    "        self.criterion_cls = nn.BCEWithLogitsLoss()\n",
    "        # 边界框回归使用CIOU_Loss作为损失函数\n",
    "        self.criterion_reg = torchvision.ops.complete_box_iou_loss\n",
    "        \n",
    "        self.obj_gain = obj_gain\n",
    "        self.cls_gain = cls_gain\n",
    "        self.reg_gain = reg_gain\n",
    "        self.neg_thres = neg_thres # 某锚框与所有真实边界框的最大值小于该阈值才被归为负样本\n",
    "        self.obj_pos_ratio = obj_pos_ratio # 物体评分的训练中正样本的比例\n",
    "        \n",
    "    def __call__(self, preds, labels):\n",
    "        \"\"\"计算多任务损失函数\"\"\"\n",
    "        (reg_outputs, reg_targets), (obj_outputs, obj_targets), (cls_outputs, cls_targets) = \\\n",
    "            self.build_target(preds, labels)\n",
    "        loss_obj = self.criterion_obj(obj_outputs, obj_targets)\n",
    "        loss_cls = self.criterion_cls(cls_outputs, F.one_hot(cls_targets, self.num_classes).float())\n",
    "        loss_reg = self.criterion_reg(reg_outputs, reg_targets, reduction='mean')\n",
    "        loss = self.obj_gain * loss_obj + \\\n",
    "               self.cls_gain * loss_cls + \\\n",
    "               self.reg_gain * loss_reg\n",
    "        return loss_obj.item(), loss_cls.item(), loss_reg.item(), loss # 总loss需要计算反向传播，故不取.item()\n",
    "        \n",
    "    def build_target(self, preds, labels):\n",
    "        \"\"\"根据网络输出和标签整理出用于计算损失的数据\"\"\"\n",
    "        objectness, shift, class_conf, anchors_cxcywh = preds\n",
    "        n = objectness[0].shape[0]\n",
    "        # 记录哪些锚框是正/负样本\n",
    "        pos_table = [torch.zeros_like(objectness[i], device=labels.device, dtype=bool) for i in range(3)]\n",
    "        neg_table = [torch.ones_like(objectness[i], device=labels.device, dtype=bool) for i in range(3)]\n",
    "        obj_outputs, obj_targets = [], []\n",
    "        reg_outputs, reg_targets = [], []\n",
    "        cls_outputs, cls_targets = [], []\n",
    "        responsible_anchors = []\n",
    "        # 负样本\n",
    "        for i in range(n):\n",
    "            for j in range(3):\n",
    "                # gt_xyxy = labels[labels[:, 0]==i][:, 2:]\n",
    "                # h, w, c = anchors_cxcywh[j].shape[:3]\n",
    "                # pred_cxcywh = refine_box(anchors_cxcywh[j].reshape(-1, 4), shift[j][i].reshape(-1, 4), 8*2**j)\n",
    "                # pred_xyxy = cxcywh2xyxy(pred_cxcywh)\n",
    "                # # 对每个预测框分别计算其与所有真实边界框的IOU的最大值\n",
    "                # pred_gt_iou = torchvision.ops.box_iou(pred_xyxy, gt_xyxy).reshape(h, w, c, gt_xyxy.shape[0])\n",
    "                # max_values, _ = pred_gt_iou.max(dim=-1)\n",
    "                # neg_table[j][i] = max_values < self.neg_thres\n",
    "                \n",
    "                gt_xyxy = labels[labels[:, 0]==i][:, 2:]\n",
    "                h, w, c = anchors_cxcywh[j].shape[:3]\n",
    "                anchors_xyxy = cxcywh2xyxy(anchors_cxcywh[j].reshape(-1, 4))\n",
    "                # 对每个锚框分别计算其与所有真实边界框的IOU的最大值\n",
    "                anchor_gt_iou = torchvision.ops.box_iou(anchors_xyxy, gt_xyxy).reshape(h, w, c, gt_xyxy.shape[0])\n",
    "                max_values, _ = anchor_gt_iou.max(dim=-1)\n",
    "                # 若锚框与所有真实边界框的IOU都小于给定阈值，则将其标记为负样本\n",
    "                neg_table[j][i] = max_values < self.neg_thres\n",
    "        # 正样本，遍历所有真实边界框\n",
    "        for label in labels:\n",
    "            if label.dim() == 2:\n",
    "                label = label.squeeze(0)\n",
    "            sample_idx, category, gt_xyxy = label[0], label[1].reshape(-1), label[None, 2:]\n",
    "            gt_cxcywh = xyxy2cxcywh(gt_xyxy)\n",
    "            corresponding_anchors_cxcywh = []\n",
    "            for j in range(3):\n",
    "                # 当前真实边界框的中心点所在的网格坐标\n",
    "                gt_cx = int((gt_cxcywh[0, 0] / (8*2**j)).floor().item())\n",
    "                gt_cy = int((gt_cxcywh[0, 1] / (8*2**j)).floor().item())\n",
    "                # 当前真实边界框的中心点所在网格上的锚框\n",
    "                corresponding_anchors_cxcywh.append(anchors_cxcywh[j][gt_cy, gt_cx])\n",
    "            corresponding_anchors_cxcywh = torch.cat(corresponding_anchors_cxcywh, dim=0)\n",
    "            corresponding_anchors_xyxy = cxcywh2xyxy(corresponding_anchors_cxcywh)\n",
    "            \n",
    "            # 找出与真实边界框IOU最高的锚框\n",
    "            gt_anchor_iou = torchvision.ops.box_iou(gt_xyxy,\n",
    "                                                    corresponding_anchors_xyxy).squeeze(0)\n",
    "            idx = int(gt_anchor_iou.argmax())\n",
    "            # 第几个检测头的第几个anchor\n",
    "            head_idx, anchor_idx = idx//(anchors_cxcywh[0].shape[2]), idx%(anchors_cxcywh[0].shape[2])\n",
    "            gt_cx = int((gt_cxcywh[0, 0] / (8*2**head_idx)).floor().item())\n",
    "            gt_cy = int((gt_cxcywh[0, 1] / (8*2**head_idx)).floor().item())\n",
    "            # 如果与当前真实边界框最大的锚框已经与其他真实边界框匹配，那么就选IOU次大的\n",
    "            while pos_table[head_idx][sample_idx, gt_cy, gt_cx, anchor_idx]:\n",
    "                gt_anchor_iou[idx] = -1.\n",
    "                idx = int(gt_anchor_iou.argmax())\n",
    "                head_idx, anchor_idx = idx//(anchors_cxcywh[0].shape[2]), idx%(anchors_cxcywh[0].shape[2])\n",
    "                gt_cx = int((gt_cxcywh[0, 0] / (8*2**head_idx)).floor().item())\n",
    "                gt_cy = int((gt_cxcywh[0, 1] / (8*2**head_idx)).floor().item())\n",
    "                if gt_anchor_iou.max() < 0:\n",
    "                    break\n",
    "            # 如果一个真实边界框中心点所在网格中所有的锚框都已经与其他真实边界框对应\n",
    "            # 那么就忽略这个真实边界框，不参与反向传播\n",
    "            if gt_anchor_iou.max() < 0:\n",
    "                continue\n",
    "            responsible_anchor = corresponding_anchors_cxcywh[None, idx]\n",
    "            reg_target = gt_xyxy.float()\n",
    "            # 用网络的输出修正与该真实边界框匹配的锚框\n",
    "            reg_output = torchvision.ops.box_convert(\n",
    "                refine_box(responsible_anchor,\n",
    "                           shift[head_idx][sample_idx, gt_cy, gt_cx, anchor_idx][None, :],\n",
    "                           downsample_rate=8*2**head_idx),\n",
    "                'cxcywh', 'xyxy')\n",
    "            cls_output = class_conf[head_idx][sample_idx, gt_cy, gt_cx, anchor_idx]\n",
    "            obj_output = objectness[head_idx][sample_idx, gt_cy, gt_cx, anchor_idx].reshape(-1)\n",
    "            reg_outputs.append(reg_output)\n",
    "            reg_targets.append(reg_target)\n",
    "            obj_outputs.append(obj_output)\n",
    "            obj_targets.append(torch.ones_like(obj_output))\n",
    "            cls_outputs.append(cls_output)\n",
    "            cls_targets.append(category)\n",
    "            responsible_anchors.append(responsible_anchor)\n",
    "            # 把被选中的锚框在正负样本table中标记出来\n",
    "            pos_table[head_idx][sample_idx, gt_cy, gt_cx, anchor_idx] = True\n",
    "            neg_table[head_idx][sample_idx, gt_cy, gt_cx, anchor_idx] = False\n",
    "        \n",
    "        # 用正样本比例计算出负样本数量\n",
    "        num_pos_samples = sum([pos_table[i].sum() for i in range(3)])\n",
    "        num_neg_samples = int(((1-self.obj_pos_ratio) / self.obj_pos_ratio) * num_pos_samples)\n",
    "        obj_output = torch.cat([objectness[i][neg_table[i]] for i in range(3)])\n",
    "        # 在所有负样本中随机采样\n",
    "        mask = torch.rand_like(obj_output) < float(num_neg_samples/obj_output.shape[0])\n",
    "        obj_output = obj_output[mask]\n",
    "        obj_outputs.append(obj_output)\n",
    "        obj_targets.append(torch.zeros_like(obj_output))\n",
    "        # 确保所有锚框要么是正样本要么是负样本\n",
    "        assert all(((pos_table[i] & neg_table[i]).sum().item() == 0 for i in range(3)))\n",
    "        reg_outputs, reg_targets = torch.cat(reg_outputs, dim=0), torch.cat(reg_targets, dim=0)\n",
    "        obj_outputs, obj_targets = torch.cat(obj_outputs, dim=0), torch.cat(obj_targets, dim=0)\n",
    "        cls_outputs, cls_targets = torch.stack(cls_outputs, dim=0), torch.cat(cls_targets, dim=0)\n",
    "        num_pos_samples = cls_targets.shape[0]\n",
    "        obj_outputs = obj_outputs[:int(num_pos_samples / self.obj_pos_ratio)]\n",
    "        obj_targets = obj_targets[:int(num_pos_samples / self.obj_pos_ratio)]\n",
    "        # 返回物体评分、边界框预测和类别概率预测三个部分计算损失所用的数据\n",
    "        return (reg_outputs, reg_targets), (obj_outputs, obj_targets),\\\n",
    "               (cls_outputs, cls_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa0af7e6-cb7c-4f1d-b182-f8115c331bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo_one_step(net, data, criterion, optimizer, SAT=False):\n",
    "    \"\"\"训练一步\"\"\"\n",
    "    image, labels = data\n",
    "    image, labels = image.to(device), labels.to(device)        \n",
    "    preds = net(image)\n",
    "    loss_obj, loss_cls, loss_reg, loss = criterion(preds, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss_obj, loss_cls, loss_reg, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b05ecd10-50e9-45c9-a230-0cd77979cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_ratio(warmup_steps, cur_step, power=1.):\n",
    "    if cur_step == 0:\n",
    "        return 0\n",
    "    lr_ratio = min(cur_step ** -power,\n",
    "                   (cur_step * warmup_steps ** -(1.+power))) * warmup_steps ** power\n",
    "    return lr_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aff6c993-7432-46de-98ec-b966bf283b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return (optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "609670c1-342c-45fc-8087-af2f993e64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(net, cfg):\n",
    "    voc_train = PascalVOC(train=True, image_sizes=cfg.image_sizes, ratio=cfg.data_ratio)\n",
    "    # 创建dataloader时需手动指定整合batch中数据的函数\n",
    "    # 否则会因为各个样本的标签张量形状不同而报错\n",
    "    dataloader = torch.utils.data.DataLoader(voc_train,\n",
    "                                             batch_size=cfg.batch_size,\n",
    "                                             collate_fn=voc_train.collate,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=cfg.num_workers)\n",
    "    num_batches = len(dataloader)\n",
    "    criterion = ComputeLoss(cfg.obj_pos_weight,\n",
    "                            cfg.num_classes,\n",
    "                            cfg.obj_gain,\n",
    "                            cfg.cls_gain,\n",
    "                            cfg.reg_gain,\n",
    "                            cfg.neg_thres,\n",
    "                            cfg.obj_pos_ratio)\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                lr=cfg.lr,\n",
    "                                weight_decay=cfg.weight_decay,\n",
    "                                momentum=0.9)\n",
    "    # 迭代步小于指定步数时，学习率线性增加\n",
    "    # 超过指定步数后呈指数衰减，衰减速度由cfg.lr_decay_power控制\n",
    "    warmup_lr = lambda cur_step: warmup_lr_ratio(int(cfg.warmup_steps*cfg.num_epochs*num_batches),\n",
    "                                                 cur_step, cfg.lr_decay_power)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr)\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg.version}')\n",
    "    net.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(1, cfg.num_epochs+1):\n",
    "        epoch_loss = []\n",
    "        pbar = tqdm(enumerate(dataloader), total=num_batches)\n",
    "        for i, data in pbar:\n",
    "            pbar.set_description(f\"epoch {epoch:3d}\")\n",
    "            loss_obj, loss_cls, loss_reg, loss = train_yolo_one_step(net, data, criterion, optimizer)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            pbar.set_postfix(obj=f\"{loss_obj:.4f}\", cls=f\"{loss_cls:.4f}\", reg=f\"{loss_reg:.4f}\", loss=f\"{loss:.4f}\")\n",
    "            # 每10个迭代步，随机改变一次训练图片的尺寸\n",
    "            if global_step % 10 == 0:\n",
    "                voc_train.random_size()\n",
    "                writer.add_scalars('train/loss', {'reg': loss_reg, \n",
    "                                                  'cls': loss_cls,\n",
    "                                                  'obj': loss_obj,\n",
    "                                                  'weighted sum': loss}, global_step=global_step)\n",
    "                writer.add_scalar('train/lr', get_lr(optimizer), global_step=global_step)\n",
    "            epoch_loss.append(loss)\n",
    "            if global_step % (num_batches // 5) == 0:\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    data = (data[0][0][None, :], data[1][data[1][:, 0]==0])\n",
    "                    infer_result = show_predictions(net,\n",
    "                                                    data,\n",
    "                                                    conf_thres=0.5,\n",
    "                                                    iou_thres=0.2,\n",
    "                                                    display=False,\n",
    "                                                    scale=2.0)\n",
    "                net.train()\n",
    "                writer.add_image('train/images_with_predictions',\n",
    "                                 infer_result[0],\n",
    "                                 global_step=global_step,\n",
    "                                 dataformats='HWC')\n",
    "        print(f'epoch {epoch:4d}, loss={sum(epoch_loss) / len(epoch_loss):8.4f}')\n",
    "        torch.save(net.backbone.state_dict(), f'models/{cfg.version}_backbone.pth')\n",
    "        torch.save(net.neck.state_dict(), f'models/{cfg.version}_neck.pth')\n",
    "        torch.save(net.head.state_dict(), f'models/{cfg.version}_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "589ae91d-f894-48da-929e-add57468aa8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        self.version = 'version 10'\n",
    "        self.backbone = 'resnet50'\n",
    "        self.num_classes = 20\n",
    "        self.neck_hidden_layers = 2\n",
    "        self.head_hidden_layers = 2\n",
    "        self.hidden_channels = 256\n",
    "        self.neg_thres = 0.3\n",
    "        \n",
    "        self.data_ratio = 1.0\n",
    "        self.anchors = [(0.07, 0.14), (0.1, 0.1), (0.14, 0.07),\n",
    "                        (0.274, 0.548), (0.387, 0.387), (0.548, 0.274),\n",
    "                        (0.5, 0.8), (0.8, 0.8), (0.8, 0.5)]\n",
    "        self.image_sizes = [i * 32 + 320 for i in range(10)]\n",
    "        self.obj_pos_weight = 10.\n",
    "        self.obj_pos_ratio = 0.05\n",
    "        self.obj_gain = 1.\n",
    "        self.cls_gain = 3.\n",
    "        self.reg_gain = 1.\n",
    "        \n",
    "        self.lr = 1e-2\n",
    "        self.warmup_steps = 0.02\n",
    "        self.lr_decay_power = 0.75\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 100\n",
    "        self.weight_decay = 5e-4\n",
    "        self.num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e57ab404-fd33-4ebf-b44b-3a63d598e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b590f243-42b1-4fb2-8a89-3fd46c024026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo = Yolo(cfg.backbone,\n",
    "            cfg.anchors,\n",
    "            cfg.num_classes,\n",
    "            cfg.hidden_channels,\n",
    "            cfg.neck_hidden_layers,\n",
    "            cfg.head_hidden_layers).to(device)\n",
    "# print(yolo.neck)\n",
    "# print(yolo.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce792e7c-a91b-4ef0-942a-61f320a3ff4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_yolo(yolo, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4cb366d-7b7b-49e6-b4f0-50ba974a9ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo.backbone.load_state_dict(torch.load(f'models/{cfg.version}_backbone.pth'))\n",
    "yolo.neck.load_state_dict(torch.load(f'models/{cfg.version}_neck.pth'))\n",
    "yolo.head.load_state_dict(torch.load(f'models/{cfg.version}_head.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dc504ce-691c-43cf-9dae-19341ae4d04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8333/8333 [15:09<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1091053100761268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tic = time()\n",
    "voc_dataset = PascalVOC(False)\n",
    "voc_dataset.current_shape = 416\n",
    "yolo.eval()\n",
    "for i in tqdm(range(len(voc_dataset))):\n",
    "    data = voc_dataset[i]\n",
    "    img = show_predictions(yolo,\n",
    "                           data,\n",
    "                           conf_thres=0.6,\n",
    "                           iou_thres=0.3,\n",
    "                           display=False,\n",
    "                           scale=2.0)\n",
    "    plt.imsave(f'./outputs/{cfg.version}/{i}.jpg', img[0])\n",
    "print((time() - tic) / len(voc_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdafbb0-5ced-414b-ad32-e5895a8692a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# tic = time()\n",
    "# voc_dataset = PascalVOC(False)\n",
    "# voc_dataset.current_shape = 608\n",
    "# yolo.eval()\n",
    "# for i in tqdm(range(len(voc_dataset))):\n",
    "#     data = voc_dataset[i]\n",
    "#     img = show_predictions(yolo,\n",
    "#                            data,\n",
    "#                            conf_thres=0.8,\n",
    "#                            iou_thres=0.2,\n",
    "#                            display=False,\n",
    "#                            scale=2.0)\n",
    "#     plt.imsave(f'./outputs/{cfg.version}_largescale/{i}.jpg', img[0])\n",
    "# print((time() - tic) / len(voc_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
