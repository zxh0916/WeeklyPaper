{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77a160c-0b8c-46e0-b6bb-4074337bd344",
   "metadata": {},
   "source": [
    "# GoogLeNet简单复现（Kaggle狗狗品种分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d31722-6b5e-4d31-b1ff-5bce18238269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1e49e-9acf-45e8-bc96-fba0e86f8c2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [kaggle狗狗品种分类竞赛地址](https://www.kaggle.com/c/dog-breed-identification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1c0abe-e4de-4722-b374-5935920a80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/dog-breed-identification/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204484f9-d152-4d2f-938c-2b66636acedd",
   "metadata": {},
   "source": [
    "## 读取训练集图片id和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83b2e7b-c00a-4967-8e1e-e4b23949cce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>ffd25009d635cfd16e793503ac5edef0</td>\n",
       "      <td>borzoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>ffd3f636f7f379c51ba3648a9ff8254f</td>\n",
       "      <td>dandie_dinmont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>ffe2ca6c940cddfee68fa3cc6c63213f</td>\n",
       "      <td>airedale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>ffe5f6d8e2bff356e9482a80a6e29aac</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10221</th>\n",
       "      <td>fff43b07992508bc822f33d8ffd902ae</td>\n",
       "      <td>chesapeake_bay_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10222 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                     breed\n",
       "0      000bec180eb18c7604dcecc8fe0dba07               boston_bull\n",
       "1      001513dfcb2ffafc82cccf4d8bbaba97                     dingo\n",
       "2      001cdf01b096e06d78e9e5112d419397                  pekinese\n",
       "3      00214f311d5d2247d5dfe4fe24b2303d                  bluetick\n",
       "4      0021f9ceb3235effd7fcde7f7538ed62          golden_retriever\n",
       "...                                 ...                       ...\n",
       "10217  ffd25009d635cfd16e793503ac5edef0                    borzoi\n",
       "10218  ffd3f636f7f379c51ba3648a9ff8254f            dandie_dinmont\n",
       "10219  ffe2ca6c940cddfee68fa3cc6c63213f                  airedale\n",
       "10220  ffe5f6d8e2bff356e9482a80a6e29aac        miniature_pinscher\n",
       "10221  fff43b07992508bc822f33d8ffd902ae  chesapeake_bay_retriever\n",
       "\n",
       "[10222 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(path + 'labels.csv')\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc124d3-cf28-4d4f-8f10-768c4066fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_csv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0b2a-b9ce-4abe-8288-6f6984b0abdb",
   "metadata": {},
   "source": [
    "## 将类别按字母顺序排序，并读取前十个类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc86a0ea-46a0-456b-8c71-809a268eb576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affenpinscher',\n",
       " 'afghan_hound',\n",
       " 'african_hunting_dog',\n",
       " 'airedale',\n",
       " 'american_staffordshire_terrier',\n",
       " 'appenzeller',\n",
       " 'australian_terrier',\n",
       " 'basenji',\n",
       " 'basset',\n",
       " 'beagle']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted(train_csv['breed'].unique().tolist())\n",
    "label_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde09e5-60b4-472e-a662-7e4147f0c846",
   "metadata": {},
   "source": [
    "## 读取测试集图片id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c879e5-fe85-4264-b0fd-312d88990e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdf2772-34de-443a-8a5a-e5b764f194de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10357, 121)\n"
     ]
    }
   ],
   "source": [
    "print(test_csv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597a4fb-3112-457c-ade9-98b38ebf7e8b",
   "metadata": {},
   "source": [
    "## 在训练集上计算三个通道的均值和标准差\n",
    "\n",
    "均值：\\[0.4736, 0.4504, 0.3909\\]\n",
    "\n",
    "方差：\\[0.2655, 0.2607, 0.2650\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6971bd6c-4bb2-40ed-af2f-6230ca70dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = []\n",
    "# for i in range(len(train_csv['id'])):\n",
    "#     image = (transforms.ToTensor()(Image.open(path + 'train/' + train_csv['id'][i] + '.jpg'))).to(torch.float32).flatten(1, 2)\n",
    "#     images.append(image)\n",
    "# flattened_image = torch.cat(images, dim=1)\n",
    "# print(flattened_image.shape)                                    # torch.Size([3, 1882650608])\n",
    "# print(flattened_image.mean(dim=1), flattened_image.std(dim=1))  # tensor([0.4736, 0.4504, 0.3909]) tensor([0.2655, 0.2607, 0.2650])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c52da1-88dc-49e9-b1f0-aea7f27dd4b4",
   "metadata": {},
   "source": [
    "## 定义训练数据集\n",
    "#### 数据预处理：随机裁剪并缩放（插值方法随机），随机水平翻转，扰动色彩，转化为张量，标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cdc339-804e-44ec-9f22-d8405c6baa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.interpolation_modes = [transforms.InterpolationMode.NEAREST,\n",
    "                                    transforms.InterpolationMode.BILINEAR,\n",
    "                                    transforms.InterpolationMode.BICUBIC]\n",
    "        self.trans = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                         transforms.ColorJitter(brightness=0.2,\n",
    "                                                                contrast=0.2,\n",
    "                                                                saturation=0.2,\n",
    "                                                                hue=0.2),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.4736, 0.4504, 0.3909],\n",
    "                                                              std=[0.2655, 0.2607, 0.2650],\n",
    "                                                              inplace=True)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        resized_crop = transforms.RandomResizedCrop(size=224,\n",
    "                                                    scale=(0.08, 1.0),\n",
    "                                                    ratio=(0.75, 1.3333333333333333),\n",
    "                                                    interpolation=self.interpolation_modes[random.randint(0, 2)])\n",
    "        return self.trans(resized_crop(image)), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda8e54-7f88-4499-bcd4-a5be25d0c6bf",
   "metadata": {},
   "source": [
    "## 定义验证数据集\n",
    "#### 数据预处理：将短边缩放到256，在中心裁剪出224\\*224的一块，转化为张量，标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adcb6799-b5d3-442c-ab71-15f9b3af9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.trans = transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.4736, 0.4504, 0.3909],\n",
    "                                                              std=[0.2655, 0.2607, 0.2650],\n",
    "                                                              inplace=True)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        return self.trans(image), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a40c4-06e8-4bcd-9941-2999e913d7d4",
   "metadata": {},
   "source": [
    "## 定义加载图片的训练验证集，主要是将图片加载出来并配合`data.random_split`将训练集随机分成训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b61f46b-7c1c-47f6-8ee3-d52182ebda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValidDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "                \n",
    "    def __len__(self):\n",
    "        return train_csv.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(path + 'train/' + train_csv['id'][index] + '.jpg')\n",
    "        label = label_list.index(train_csv['breed'][index])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152def70-f047-447d-bcde-233b98353321",
   "metadata": {},
   "source": [
    "## 定义测试数据集\n",
    "#### 数据预处理：\n",
    "1. 将图片短边缩放到如下四个尺寸：\\[256, 288, 320, 352\\]。\n",
    "2. 对于宽而矮的图片，裁出左中右三个正方形；对于窄而高的图片，裁出上中下三个正方形。\n",
    "3. 对于每个正方形子图，在四个角落和正中央裁出五张224\\*224的图片，并把原正方形子图缩放为224\\*224，共6张图片，及其水平翻转版本。\n",
    "\n",
    "#### 一张图片共产生4\\*3\\*6\\*2=144张测试样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ab115d-4174-4d80-b046-2e176a17a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.trans = transforms.Compose([transforms.Lambda(self.get_144_samples),\n",
    "                                         transforms.Lambda(lambda crops: \\\n",
    "                                                           torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                         # 这里Lambda的用法参考了torchvision的doc中transforms.TenCrop()的example\n",
    "                                         transforms.Normalize(mean=[0.4736, 0.4504, 0.3909],\n",
    "                                                              std=[0.2655, 0.2607, 0.2650],\n",
    "                                                              inplace=True)])\n",
    "    def get_144_samples(self, image):\n",
    "        \"\"\"一张图片共产生4*3*6*2=144张测试样本。\"\"\"\n",
    "        samples = []\n",
    "        self.sizes = [256, 288, 320, 352]\n",
    "        for size in self.sizes:\n",
    "            resized_image = transforms.Resize(size)(image)\n",
    "            crops = self.get_3_crops(resized_image)\n",
    "            for crop in crops:\n",
    "                samples.append(transforms.Resize(224)(crop))\n",
    "                samples.append(transforms.RandomHorizontalFlip(1)(samples[-1]))\n",
    "                samples += transforms.TenCrop(224)(crop)\n",
    "        assert(len(samples) == len(self.sizes) * 3 * 12)\n",
    "        return samples\n",
    "            \n",
    "    def get_3_crops(self, image):\n",
    "        \"\"\"对于宽而矮的图片，裁出左中右三个正方形；对于窄而高的图片，裁出上中下三个正方形。\"\"\"\n",
    "        x, y = image.size\n",
    "        if x > y:\n",
    "            boxes = [[0, 0, y, y], [(x-y)//2, 0, (x+y)//2, y], [(x-y), 0, x, y]]\n",
    "        elif x < y:\n",
    "            boxes = [[0, 0, x, x], [0, (y-x)//2, x, (y+x)//2], [0, (y-x), x, y]]\n",
    "        elif x == y:\n",
    "            boxes = [[0, 0, x, y], [0, 0, x, y],  [0, 0, x, y]]\n",
    "        return [image.crop(box) for box in boxes]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return test_csv.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(path + 'test/' + test_csv['id'][index] + '.jpg')\n",
    "        # 在intel i7 10870H上耗时0.16~0.19秒\n",
    "        return self.trans(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c7162-99e8-4d99-8b02-a016197243c6",
   "metadata": {},
   "source": [
    "## 定义网络基本块CBR：卷积  $\\rightarrow$  批归一化（可选）$\\rightarrow$  ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c324598e-755f-4c1d-8b94-01bfaa4719f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=1,\n",
    "                 padding='same',\n",
    "                 stride=1,\n",
    "                 batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=kernel_size, padding=padding,\n",
    "                              stride=stride, bias=not self.batch_norm)\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.ReLU = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            output = self.bn(output)\n",
    "        output = self.ReLU(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19eeac-245c-4d60-b3db-37b155a977f4",
   "metadata": {},
   "source": [
    "## 定义Inception块\n",
    "![InceptionBlock](../pictures/inception_block.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ca6704-73cd-4938-9e43-8813f59e91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int,\n",
    "                 branch_1x1:int,\n",
    "                 branch_3x3:list,\n",
    "                 branch_5x5:list,\n",
    "                 branch_pool:int,\n",
    "                 batch_norm:bool):\n",
    "        super().__init__()\n",
    "        self.branch_1x1 = CBR(in_channels, branch_1x1, batch_norm=batch_norm)\n",
    "        self.branch_3x3 = nn.Sequential(*[CBR(in_channels, branch_3x3[0], batch_norm=batch_norm),\n",
    "                                         CBR(branch_3x3[0], branch_3x3[1], kernel_size=3, batch_norm=batch_norm)])\n",
    "        self.branch_5x5 = nn.Sequential(*[CBR(in_channels, branch_5x5[0], batch_norm=batch_norm),\n",
    "                                         CBR(branch_5x5[0], branch_5x5[1], kernel_size=5, batch_norm=batch_norm)])\n",
    "        self.branch_pool = nn.Sequential(*[nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
    "                                          CBR(in_channels, branch_pool, batch_norm=batch_norm)])\n",
    "    def forward(self, x):\n",
    "        output_1x1 = self.branch_1x1(x)\n",
    "        output_3x3 = self.branch_3x3(x)\n",
    "        output_5x5 = self.branch_5x5(x)\n",
    "        output_pool = self.branch_pool(x)\n",
    "        output = torch.cat((output_1x1, output_3x3, output_5x5, output_pool), dim=1)\n",
    "        assert(output.shape[-2:] == x.shape[-2:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37f4e3-9150-49ac-b49e-890f3d5d8926",
   "metadata": {},
   "source": [
    "## 定义GoogLeNet\n",
    "![GoogLeNet-Table1](..\\pictures\\GoogLeNet-Table1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a11cbd8-0b99-4608-91c1-a3580456bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, batch_norm=False, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.stage_1 = nn.Sequential(*[CBR(3, 64, kernel_size=7, stride=2, padding=3, batch_norm=batch_norm),\n",
    "                                       nn.MaxPool2d(kernel_size=3, stride=2, padding=1)])\n",
    "        \n",
    "        self.stage_2 = nn.Sequential(*[CBR(64, 192, kernel_size=3, batch_norm=batch_norm),\n",
    "                                       nn.MaxPool2d(kernel_size=3, stride=2, padding=1)])\n",
    "        \n",
    "        self.stage_3 = nn.Sequential(*[InceptionBlock(192, 64,  [96,  128], [16, 32],  32,  batch_norm),\n",
    "                                       InceptionBlock(256, 128, [128, 192], [32, 96],  64,  batch_norm),\n",
    "                                       nn.MaxPool2d(kernel_size=3, stride=2, padding=1)])\n",
    "        \n",
    "        self.stage_4 = nn.Sequential(*[InceptionBlock(480, 192, [96,  208], [16, 48],  64,  batch_norm),\n",
    "                                       InceptionBlock(512, 160, [112, 224], [24, 64],  64,  batch_norm),\n",
    "                                       InceptionBlock(512, 128, [128, 256], [24, 64],  64,  batch_norm),\n",
    "                                       InceptionBlock(512, 112, [144, 288], [32, 64],  64,  batch_norm),\n",
    "                                       InceptionBlock(528, 256, [160, 320], [32, 128], 128, batch_norm),\n",
    "                                       nn.MaxPool2d(kernel_size=3, stride=2, padding=1)])\n",
    "        \n",
    "        self.stage_5 = nn.Sequential(*[InceptionBlock(832, 256, [160, 320], [32, 128], 128, batch_norm),\n",
    "                                       InceptionBlock(832, 384, [192, 384], [48, 128], 128, batch_norm),\n",
    "                                       nn.AdaptiveAvgPool2d((1, 1))])\n",
    "        \n",
    "        self.FC = nn.Sequential(*[nn.Flatten(),\n",
    "                                  nn.Dropout(p=dropout),\n",
    "                                  nn.Linear(1024, 120)])\n",
    "\n",
    "    def print_num_params(self):\n",
    "        \"\"\"打印网络参数数量\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'{total_params:,} total parameters.')\n",
    "        total_trainable_params = sum(\n",
    "            p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f'{total_trainable_params:,} trainable parameters.')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(1, 6):\n",
    "            x = getattr(self, 'stage_' + str(i))(x)\n",
    "        x = self.FC(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f295ed-0983-4c84-bf7b-7972a9fb57ff",
   "metadata": {},
   "source": [
    "## 将训练集随机分成训练集和验证集，实例化测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ceb17d-fd8f-4a6a-8f15-81f9fb227be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = data.random_split(TrainValidDataset(),\n",
    "                                                 [8688, 10222-8688],\n",
    "                                                 generator=torch.Generator().manual_seed(42))\n",
    "train_dataset, valid_dataset = TrainDataset(train_dataset), ValidDataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7901636a-7a90-40c7-93fa-798a4bd1b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab009743-2447-482f-be37-c4e6ce73167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_acc(net, data_iter, criterion, device=device):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度。\"\"\"\n",
    "    net.eval()  # 设置为评估模式\n",
    "    loss = []\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for input, target in data_iter:\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = net(input)\n",
    "            loss.append(float(criterion(output, target).item()))\n",
    "            metric.add(d2l.accuracy(output, target), target.numel())\n",
    "    return sum(loss) / len(loss), metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7b26c4-9593-42ae-97e3-db325c172df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return (optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cb2b7-857a-44f1-964b-8fd755604248",
   "metadata": {},
   "source": [
    "## 定义GoogLeNet训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d13c693d-8aeb-4356-bc33-d48e41ba16cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_GoogLeNet(net,\n",
    "                    batch_size,\n",
    "                    lr,\n",
    "                    num_epochs,\n",
    "                    weight_decay=5e-4):\n",
    "\n",
    "    writer = SummaryWriter(f'runs/GoogLeNet')\n",
    "    train_iter = data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=8)\n",
    "    valid_iter = data.DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                                 shuffle=False, num_workers=8)\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "    net.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=8,gamma=0.96)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        tic = time.time()\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (input, target) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(loss * input.shape[0],\n",
    "                           d2l.accuracy(output, target),\n",
    "                           input.shape[0])\n",
    "            timer.stop()\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        valid_loss, valid_acc = evaluate_loss_acc(net, valid_iter, criterion, device)\n",
    "        writer.add_scalar('train/loss', train_loss, global_step=epoch+1)\n",
    "        writer.add_scalar('train/accuracy', train_acc, global_step=epoch+1)\n",
    "        writer.add_scalar('valid/loss', valid_loss, global_step=epoch+1)\n",
    "        writer.add_scalar('valid/accuracy', valid_acc, global_step=epoch+1)\n",
    "        writer.add_scalar('learning rate', get_lr(optimizer), global_step=epoch+1)\n",
    "        scheduler.step()\n",
    "        toc = time.time()\n",
    "        print(f\"epoch {epoch+1:3d}, train loss: {train_loss:.4f}, train accuracy: {train_acc:.4f}, \\\n",
    "valid loss: {valid_loss:.4f}, valid accuracy: {valid_acc:.4f}, time: {toc-tic:.4f}\")\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'valid loss {valid_loss:.3f}, valid acc {valid_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d29c642-37ae-4d63-bbcc-2daefce91f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = GoogLeNet(batch_norm=True, dropout=0.5).to(device)\n",
    "net.print_num_params()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c62c7-6a23-4399-8bd2-1eba72a42c68",
   "metadata": {},
   "source": [
    "## 训练GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5556cf2d-eae5-49a2-9858-7d37d6b5e301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_GoogLeNet(net,\n",
    "                batch_size=256,\n",
    "                lr=1e-3,\n",
    "                num_epochs=200,\n",
    "                weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a87ed25-f102-461c-a4a2-4c8eaa4373ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'GoogLeNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf6a4357-6583-467c-bf1e-5d7f83215bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('GoogLeNet.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88bf93db-0249-46c1-8966-500b56f94800",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79960345-4413-4d27-bcb0-316235ddb132",
   "metadata": {},
   "source": [
    "## 定义测试函数，生成和`sample_submission.csv`格式相同的`DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71423bd1-536e-4859-a3c6-62c93d3d5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(net, one_hot=False):\n",
    "    net.eval()\n",
    "    column = ['id'] + label_list\n",
    "    rows, outputs = [], []\n",
    "    for input in test_dataloader:\n",
    "        input = input.to(device).squeeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = net(input)\n",
    "        outputs.append(F.softmax(output, dim=1).mean(dim=0).to(torch.float16))\n",
    "    print('Inference done! Building submission...')\n",
    "    if one_hot:\n",
    "        for i, output in enumerate(outputs):\n",
    "            pred = int(output.argmax())\n",
    "            row = [test_csv['id'][i]] + [0. for _ in range(test_csv.shape[1] - 1)]\n",
    "            row[pred+1] = 1.\n",
    "            rows.append(pd.Series(row, index=column))\n",
    "    else:\n",
    "        for i, output in enumerate(outputs):\n",
    "            row = [test_csv['id'][i]] + list(output.cpu().numpy())\n",
    "            rows.append(pd.Series(row, index=column))\n",
    "    submission = pd.DataFrame(rows)\n",
    "    return outputs, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "814d5182-846e-4ae5-8391-f190cca34719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference done! Building submission...\n"
     ]
    }
   ],
   "source": [
    "outputs, submission = Test(net, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ecb11-8ac6-48e2-a8bd-e2a8d0f776a6",
   "metadata": {},
   "source": [
    "## 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "689205fa-c4f4-453d-b8b1-50e4f09519ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_onehot.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
